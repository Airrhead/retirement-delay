<html xmlns:v="urn:schemas-microsoft-com:vml"
xmlns:o="urn:schemas-microsoft-com:office:office"
xmlns:x="urn:schemas-microsoft-com:office:excel"
xmlns="http://www.w3.org/TR/REC-html40">

<head>
<meta http-equiv=Content-Type content="text/html; charset=utf-8">
<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content="Microsoft Excel 15">
<link id=Main-File rel=Main-File href="../cultural.htm">
<link rel=File-List href=filelist.xml>
<link rel=Stylesheet href=stylesheet.css>
<style>
<!--table
	{mso-displayed-decimal-separator:"\.";
	mso-displayed-thousand-separator:"\,";}
@page
	{margin:.75in .7in .75in .7in;
	mso-header-margin:.3in;
	mso-footer-margin:.3in;}
ruby
	{ruby-align:left;}
rt
	{color:windowtext;
	font-size:9.0pt;
	font-weight:400;
	font-style:normal;
	text-decoration:none;
	font-family:等线;
	mso-generic-font-family:auto;
	mso-font-charset:134;
	mso-char-type:none;
	display:none;}
-->
</style>
<![if !supportTabStrip]><script language="JavaScript">
<!--
function fnUpdateTabs()
 {
  if (parent.window.g_iIEVer>=4) {
   if (parent.document.readyState=="complete"
    && parent.frames['frTabs'].document.readyState=="complete")
   parent.fnSetActiveSheet(0);
  else
   window.setTimeout("fnUpdateTabs();",150);
 }
}

if (window.name!="frSheet")
 window.location.replace("../cultural.htm");
else
 fnUpdateTabs();
//-->
</script>
<![endif]>
</head>

<body link="#0563C1" vlink="#954F72">

<table border=0 cellpadding=0 cellspacing=0 width=1646 style='border-collapse:
 collapse;table-layout:fixed;width:1234pt'>
 <col width=87 span=2 style='width:65pt'>
 <col class=xl65 width=649 span=2 style='mso-width-source:userset;mso-width-alt:
 20778;width:487pt'>
 <col width=87 span=2 style='width:65pt'>
 <tr height=23 style='height:17.0pt'>
  <td height=23 width=87 style='height:17.0pt;width:65pt'>INDEX</td>
  <td width=87 style='width:65pt'>arxiv_id</td>
  <td class=xl65 width=649 style='width:487pt'>title</td>
  <td class=xl65 width=649 style='width:487pt'>abstract</td>
  <td colspan=2 width=174 style='mso-ignore:colspan;width:130pt'>submitted_date</td>
 </tr>
 <tr height=283 style='mso-height-source:userset;height:212.0pt'>
  <td height=283 align=right style='height:212.0pt'>5136</td>
  <td align=right>2111.0492</td>
  <td class=xl65 width=649 style='width:487pt'>PopBlends: Strategies for
  Conceptual Blending with Large Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>Pop culture is an important
  aspect of communication. On social media people often post pop culture
  reference images that connect an event, product or other entity to a pop
  culture domain. Creating these images is a creative challenge that requires
  finding a conceptual connection between the users' topic and a pop culture
  domain. In cognitive theory, this task is called conceptual blending. We
  present a system called PopBlends that automatically suggests conceptual
  blends. The system explores three approaches that involve both traditional
  knowledge extraction methods and large language models. Our annotation study
  shows that all three methods provide connections with similar accuracy, but
  with very different characteristics. Our user study shows that people found
  twice as many blend suggestions as they did without the system, and with half
  the mental demand. We discuss the advantages of combining large language
  models with knowledge bases for supporting divergent and convergent thinking.</td>
  <td colspan=2 style='mso-ignore:colspan'>19 February, 2023</td>
 </tr>
 <tr height=204 style='height:153.0pt'>
  <td height=204 align=right style='height:153.0pt'>5112</td>
  <td align=right>2203.13722</td>
  <td class=xl65 width=649 style='width:487pt'>Probing Pre-Trained Language
  Models for Cross-Cultural Differences in Values</td>
  <td class=xl65 width=649 style='width:487pt'>Language embeds information
  about social, cultural, and political values people hold. Prior work has
  explored social and potentially harmful biases encoded in Pre-Trained
  Language models (PTLMs). However, there has been no systematic study
  investigating how values embedded in these models vary across cultures. In
  this paper, we introduce probes to study which values across cultures are
  embedded in these models, and whether they align with existing theories and
  cross-cultural value surveys. We find that PTLMs capture differences in
  values across cultures, but those only weakly align with established value
  surveys. We discuss implications of using mis-aligned models in
  cross-cultural settings, as well as ways of aligning PTLMs with value
  surveys.</td>
  <td colspan=2 style='mso-ignore:colspan'>6 April, 2023</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>5094</td>
  <td align=right>2205.12247</td>
  <td class=xl65 width=649 style='width:487pt'>GeoMLAMA: Geo-Diverse
  Commonsense Probing on Multilingual Pre-Trained Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>Recent work has shown that
  Pre-trained Language Models (PLMs) store the relational knowledge learned
  from data and utilize it for performing downstream tasks. However,
  commonsense knowledge across different regions may vary. For instance, the
  color of bridal dress is white in American weddings whereas it is red in
  Chinese weddings. In this paper, we introduce a benchmark dataset,
  Geo-Diverse Commonsense Multilingual Language Models Analysis (GeoMLAMA), for
  probing the diversity of the relational knowledge in multilingual PLMs.
  GeoMLAMA contains 3,125 prompts in English, Chinese, Hindi, Persian, and
  Swahili, with a wide coverage of concepts shared by people from American,
  Chinese, Indian, Iranian and Kenyan cultures. We benchmark 11 standard
  multilingual PLMs on GeoMLAMA. Interestingly, we find that 1) larger
  multilingual PLMs variants do not necessarily store geo-diverse concepts
  better than its smaller variant; 2) multilingual PLMs are not intrinsically
  biased towards knowledge from the Western countries (the United States); 3)
  the native language of a country may not be the best language to probe its
  knowledge and 4) a language may better probe knowledge about a non-native
  country than its native country. Code and data are released at https://github.com/WadeYin9712/GeoMLAMA.</td>
  <td colspan=2 style='mso-ignore:colspan'>29 November, 2022</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>5055</td>
  <td align=right>2209.06293</td>
  <td class=xl65 width=649 style='width:487pt'>Do Androids Laugh at Electric
  Sheep? Humor &quot;Understanding&quot; Benchmarks from The New Yorker Caption
  Contest</td>
  <td class=xl65 width=649 style='width:487pt'>Large neural networks can now
  generate jokes, but do they really &quot;understand&quot; humor? We challenge
  AI models with three tasks derived from the New Yorker Cartoon Caption
  Contest: matching a joke to a cartoon, identifying a winning caption, and explaining
  why a winning caption is funny. These tasks encapsulate progressively more
  sophisticated aspects of &quot;understanding&quot; a cartoon; key elements
  are the complex, often surprising relationships between images and captions
  and the frequent inclusion of indirect and playful allusions to human
  experience and culture. We investigate both multimodal and language-only
  models: the former are challenged with the cartoon images directly, while the
  latter are given multifaceted descriptions of the visual scene to simulate
  human-level visual understanding. We find that both types of models struggle
  at all three tasks. For example, our best multimodal models fall 30 accuracy
  points behind human performance on the matching task, and, even when provided
  ground-truth visual scene descriptors, human-authored explanations are
  preferred head-to-head over the best machine-authored ones (few-shot GPT-4)
  in more than 2/3 of cases. We release models, code, leaderboard, and corpus,
  which includes newly-gathered annotations describing the image's
  locations/entities, what's unusual in the scene, and an explanation of the
  joke.</td>
  <td>6 July, 2023</td>
  <td></td>
 </tr>
 <tr height=431 style='height:323.0pt'>
  <td height=431 align=right style='height:323.0pt'>5034</td>
  <td align=right>2210.08604</td>
  <td class=xl65 width=649 style='width:487pt'>NormSAGE: Multi-Lingual
  Multi-Cultural Norm Discovery from Conversations On-the-Fly</td>
  <td class=xl65 width=649 style='width:487pt'>Norm discovery is important for
  understanding and reasoning about the acceptable behaviors and potential
  violations in human communication and interactions. We introduce NormSage, a
  framework for addressing the novel task of conversation-grounded multi-lingual,
  multi-cultural norm discovery, based on language model prompting and
  self-verification. NormSAGE leverages the expressiveness and implicit
  knowledge of the pretrained GPT-3 language model backbone, to elicit
  knowledge about norms through directed questions representing the norm
  discovery task and conversation context. It further addresses the risk of
  language model hallucination with a self-verification mechanism ensuring that
  the norms discovered are correct and are substantially grounded to their
  source conversations. Evaluation results show that our approach discovers
  significantly more relevant and insightful norms for conversations on-the-fly
  compared to baselines (&gt;10+% in Likert scale rating). The norms discovered
  from Chinese conversation are also comparable to the norms discovered from
  English conversation in terms of insightfulness and correctness (&lt;3%
  difference). In addition, the culture-specific norms are promising quality,
  allowing for 80% accuracy in culture pair human identification. Finally, our
  grounding process in norm discovery self-verification can be extended for
  instantiating the adherence and violation of any norm for a given
  conversation on-the-fly, with explainability and transparency. NormSAGE
  achieves an AUC of 95.4% in grounding, with natural language explanation
  matching human-written quality.</td>
  <td colspan=2 style='mso-ignore:colspan'>13 January, 2024</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>4981</td>
  <td align=right>2211.0746</td>
  <td class=xl65 width=649 style='width:487pt'>An Analytics of Culture:
  Modeling Subjectivity, Scalability, Contextuality, and Temporality</td>
  <td class=xl65 width=649 style='width:487pt'>There is a bidirectional
  relationship between culture and AI; AI models are increasingly used to
  analyse culture, thereby shaping our understanding of culture. On the other
  hand, the models are trained on collections of cultural artifacts thereby
  implicitly, and not always correctly, encoding expressions of culture. This
  creates a tension that both limits the use of AI for analysing culture and
  leads to problems in AI with respect to cultural complex issues such as bias.
  One approach to overcome this tension is to more extensively take into
  account the intricacies and complexities of culture. We structure our
  discussion using four concepts that guide humanistic inquiry into culture:
  subjectivity, scalability, contextuality, and temporality. We focus on these
  concepts because they have not yet been sufficiently represented in AI
  research. We believe that possible implementations of these aspects into AI
  research leads to AI that better captures the complexities of culture. In
  what follows, we briefly describe these four concepts and their absence in AI
  research. For each concept, we define possible research challenges.</td>
  <td colspan=2 style='mso-ignore:colspan'>14 November, 2022</td>
 </tr>
 <tr height=227 style='height:170.0pt'>
  <td height=227 align=right style='height:170.0pt'>4969</td>
  <td align=right>2211.1078</td>
  <td class=xl65 width=649 style='width:487pt'>ArtELingo: A Million Emotion
  Annotations of WikiArt with Emphasis on Diversity over Language and Culture</td>
  <td class=xl65 width=649 style='width:487pt'>This paper introduces ArtELingo,
  a new benchmark and dataset, designed to encourage work on diversity across
  languages and cultures. Following ArtEmis, a collection of 80k artworks from
  WikiArt with 0.45M emotion labels and English-only captions, ArtELingo adds
  another 0.79M annotations in Arabic and Chinese, plus 4.8K in Spanish to
  evaluate &quot;cultural-transfer&quot; performance. More than 51K artworks
  have 5 annotations or more in 3 languages. This diversity makes it possible
  to study similarities and differences across languages and cultures. Further,
  we investigate captioning tasks, and find diversity improves the performance
  of baseline models. ArtELingo is publicly available at
  https://www.artelingo.org/ with standard splits and baseline models. We hope
  our work will help ease future research on multilinguality and
  culturally-aware AI.</td>
  <td colspan=2 style='mso-ignore:colspan'>19 November, 2022</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>4957</td>
  <td align=right>2211.13069</td>
  <td class=xl65 width=649 style='width:487pt'>Cultural Incongruencies in
  Artificial Intelligence</td>
  <td class=xl65 width=649 style='width:487pt'>Artificial intelligence (AI)
  systems attempt to imitate human behavior. How well they do this imitation is
  often used to assess their utility and to attribute human-like (or
  artificial) intelligence to them. However, most work on AI refers to and
  relies on human intelligence without accounting for the fact that human
  behavior is inherently shaped by the cultural contexts they are embedded in,
  the values and beliefs they hold, and the social practices they follow.
  Additionally, since AI technologies are mostly conceived and developed in
  just a handful of countries, they embed the cultural values and practices of
  these countries. Similarly, the data that is used to train the models also
  fails to equitably represent global cultural diversity. Problems therefore
  arise when these technologies interact with globally diverse societies and
  cultures, with different values and interpretive practices. In this position
  paper, we describe a set of cultural dependencies and incongruencies in the
  context of AI-based language and vision technologies, and reflect on the
  possibilities of and potential strategies towards addressing these
  incongruencies.</td>
  <td colspan=2 style='mso-ignore:colspan'>19 November, 2022</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>4950</td>
  <td align=right>2211.14402</td>
  <td class=xl65 width=649 style='width:487pt'>An Analysis of Social Biases
  Present in BERT Variants Across Multiple Languages</td>
  <td class=xl65 width=649 style='width:487pt'>Although large pre-trained
  language models have achieved great success in many NLP tasks, it has been
  shown that they reflect human biases from their pre-training corpora. This
  bias may lead to undesirable outcomes when these models are applied in real-world
  settings. In this paper, we investigate the bias present in monolingual BERT
  models across a diverse set of languages (English, Greek, and Persian). While
  recent research has mostly focused on gender-related biases, we analyze
  religious and ethnic biases as well and propose a template-based method to
  measure any kind of bias, based on sentence pseudo-likelihood, that can
  handle morphologically complex languages with gender-based adjective
  declensions. We analyze each monolingual model via this method and visualize
  cultural similarities and differences across different dimensions of bias.
  Ultimately, we conclude that current methods of probing for bias are highly
  language-dependent, necessitating cultural insights regarding the unique ways
  bias is expressed in each language and culture (e.g. through coded language,
  synecdoche, and other similar linguistic concepts). We also hypothesize that
  higher measured social biases in the non-English BERT models correlate with
  user-generated content in their training.</td>
  <td colspan=2 style='mso-ignore:colspan'>25 November, 2022</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>4939</td>
  <td align=right>2211.16699</td>
  <td class=xl65 width=649 style='width:487pt'>Interpretability and
  accessibility of machine learning in selected food processing, agriculture
  and health applications</td>
  <td class=xl65 width=649 style='width:487pt'>Artificial Intelligence (AI) and
  its data-centric branch of machine learning (ML) have greatly evolved over
  the last few decades. However, as AI is used increasingly in real world use
  cases, the importance of the interpretability of and accessibility to AI
  systems have become major research areas. The lack of interpretability of ML
  based systems is a major hindrance to widespread adoption of these powerful
  algorithms. This is due to many reasons including ethical and regulatory
  concerns, which have resulted in poorer adoption of ML in some areas. The
  recent past has seen a surge in research on interpretable ML. Generally,
  designing a ML system requires good domain understanding combined with expert
  knowledge. New techniques are emerging to improve ML accessibility through
  automated model design. This paper provides a review of the work done to
  improve interpretability and accessibility of machine learning in the context
  of global problems while also being relevant to developing countries. We
  review work under multiple levels of interpretability including scientific
  and mathematical interpretation, statistical interpretation and partial
  semantic interpretation. This review includes applications in three areas,
  namely food processing, agriculture and health.</td>
  <td colspan=2 style='mso-ignore:colspan'>29 November, 2022</td>
 </tr>
 <tr height=496 style='height:372.0pt'>
  <td height=496 align=right style='height:372.0pt'>4936</td>
  <td align=right>2211.16938</td>
  <td class=xl65 width=649 style='width:487pt'>Evaluating Digital Agriculture
  Recommendations with Causal Inference</td>
  <td class=xl65 width=649 style='width:487pt'>In contrast to the rapid
  digitalization of several industries, agriculture suffers from low adoption
  of smart farming tools. While AI-driven digital agriculture tools can offer
  high-performing predictive functionalities, they lack tangible quantitative evidence
  on their benefits to the farmers. Field experiments can derive such evidence,
  but are often costly, time consuming and hence limited in scope and scale of
  application. To this end, we propose an observational causal inference
  framework for the empirical evaluation of the impact of digital tools on
  target farm performance indicators (e.g., yield in this case). This way, we
  can increase farmers' trust via enhancing the transparency of the digital
  agriculture market and accelerate the adoption of technologies that aim to
  secure farmer income resilience and global agricultural sustainability. As a
  case study, we designed and implemented a recommendation system for the
  optimal sowing time of cotton based on numerical weather predictions, which
  was used by a farmers' cooperative during the growing season of 2021. We then
  leverage agricultural knowledge, collected yield data, and environmental
  information to develop a causal graph of the farm system. Using the back-door
  criterion, we identify the impact of sowing recommendations on the yield and
  subsequently estimate it using linear regression, matching, inverse
  propensity score weighting and meta-learners. The results reveal that a field
  sown according to our recommendations exhibited a statistically significant
  yield increase that ranged from 12% to 17%, depending on the method. The
  effect estimates were robust, as indicated by the agreement among the
  estimation methods and four successful refutation tests. We argue that this
  approach can be implemented for decision support systems of other fields,
  extending their evaluation beyond a performance assessment of internal
  functionalities.</td>
  <td colspan=2 style='mso-ignore:colspan'>30 November, 2022</td>
 </tr>
 <tr height=181 style='height:136.0pt'>
  <td height=181 align=right style='height:136.0pt'>4934</td>
  <td align=right>2212.00509</td>
  <td class=xl65 width=649 style='width:487pt'>CultureBERT: Measuring Corporate
  Culture With Transformer-Based Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>This paper introduces
  transformer-based language models to the literature measuring corporate
  culture from text documents. We compile a unique data set of employee reviews
  that were labeled by human evaluators with respect to the information the
  reviews reveal about the firms' corporate culture. Using this data set, we
  fine-tune state-of-the-art transformer-based language models to perform the
  same classification task. In out-of-sample predictions, our language models
  classify 17 to 30 percentage points more of employee reviews in line with
  human evaluators than traditional approaches of text classification. We make
  our models publicly available.</td>
  <td colspan=2 style='mso-ignore:colspan'>25 January, 2024</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>4920</td>
  <td align=right>2212.0325</td>
  <td class=xl65 width=649 style='width:487pt'>Neural Cell Video Synthesis via
  Optical-Flow Diffusion</td>
  <td class=xl65 width=649 style='width:487pt'>The biomedical imaging world is
  notorious for working with small amounts of data, frustrating
  state-of-the-art efforts in the computer vision and deep learning worlds.
  With large datasets, it is easier to make progress we have seen from the
  natural image distribution. It is the same with microscopy videos of neuron
  cells moving in a culture. This problem presents several challenges as it can
  be difficult to grow and maintain the culture for days, and it is expensive
  to acquire the materials and equipment. In this work, we explore how to
  alleviate this data scarcity problem by synthesizing the videos. We,
  therefore, take the recent work of the video diffusion model to synthesize
  videos of cells from our training dataset. We then analyze the model's strengths
  and consistent shortcomings to guide us on improving video generation to be
  as high-quality as possible. To improve on such a task, we propose modifying
  the denoising function and adding motion information (dense optical flow) so
  that the model has more context regarding how video frames transition over
  time and how each pixel changes over time.</td>
  <td colspan=2 style='mso-ignore:colspan'>6 December, 2022</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>4890</td>
  <td align=right>2212.08038</td>
  <td class=xl65 width=649 style='width:487pt'>Redefining Relationships in
  Music</td>
  <td class=xl65 width=649 style='width:487pt'>AI tools increasingly shape how
  we discover, make and experience music. While these tools can have the
  potential to empower creativity, they may fundamentally redefine
  relationships between stakeholders, to the benefit of some and the detriment
  of others. In this position paper, we argue that these tools will
  fundamentally reshape our music culture, with profound effects (for better
  and for worse) on creators, consumers and the commercial enterprises that
  often connect them. By paying careful attention to emerging Music AI
  technologies and developments in other creative domains and understanding the
  implications, people working in this space could decrease the possible
  negative impacts on the practice, consumption and meaning of music. Given
  that many of these technologies are already available, there is some urgency
  in conducting analyses of these technologies now. It is important that people
  developing and working with these tools address these issues now to help
  guide their evolution to be equitable and empower creativity. We identify
  some potential risks and opportunities associated with existing and
  forthcoming AI tools for music, though more work is needed to identify
  concrete actions which leverage the opportunities while mitigating risks.</td>
  <td colspan=2 style='mso-ignore:colspan'>16 December, 2022</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>4869</td>
  <td align=right>2212.13631</td>
  <td class=xl65 width=649 style='width:487pt'>Proceedings of AAAI 2022 Fall
  Symposium: The Role of AI in Responding to Climate Challenges</td>
  <td class=xl65 width=649 style='width:487pt'>Climate change is one of the
  most pressing challenges of our time, requiring rapid action across society.
  As artificial intelligence tools (AI) are rapidly deployed, it is therefore
  crucial to understand how they will impact climate action. On the one hand,
  AI can support applications in climate change mitigation (reducing or
  preventing greenhouse gas emissions), adaptation (preparing for the effects
  of a changing climate), and climate science. These applications have
  implications in areas ranging as widely as energy, agriculture, and finance.
  At the same time, AI is used in many ways that hinder climate action (e.g.,
  by accelerating the use of greenhouse gas-emitting fossil fuels). In
  addition, AI technologies have a carbon and energy footprint themselves. This
  symposium brought together participants from across academia, industry,
  government, and civil society to explore these intersections of AI with
  climate change, as well as how each of these sectors can contribute to
  solutions.</td>
  <td colspan=2 style='mso-ignore:colspan'>29 January, 2023</td>
 </tr>
 <tr height=496 style='height:372.0pt'>
  <td height=496 align=right style='height:372.0pt'>4857</td>
  <td align=right>2301.01894</td>
  <td class=xl65 width=649 style='width:487pt'>Urban-Semantic Computer Vision:
  A Framework for Contextual Understanding of People in Urban Spaces</td>
  <td class=xl65 width=649 style='width:487pt'>Increasing computational power
  and improving deep learning methods have made computer vision technologies
  pervasively common in urban environments. Their applications in policing,
  traffic management, and documenting public spaces are increasingly common.
  Despite the often-discussed biases in the algorithms' training and unequally
  borne benefits, almost all applications similarly reduce urban experiences to
  simplistic, reductive, and mechanistic measures. There is a lack of context,
  depth, and specificity in these practices that enables semantic knowledge or
  analysis within urban contexts, especially within the context of using and
  occupying urban space. This paper will critique existing uses of artificial
  intelligence and computer vision in urban practices to propose a new
  framework for understanding people, action, and public space. This paper
  revisits Geertz's use of thick descriptions in generating interpretive
  theories of culture and activity and uses this lens to establish a framework
  to evaluate the varied uses of computer vision technologies that weigh
  meaning. We discuss how the framework's positioning may differ (and conflict)
  between different users of the technology. This paper also discusses the
  current use and training of deep learning algorithms and how this process
  limits semantic learning and proposes three potential methodologies for
  gaining a more contextually specific, urban-semantic, description of urban
  space relevant to urbanists. This paper contributes to the critical conversations
  regarding the proliferation of artificial intelligence by challenging the
  current applications of these technologies in the urban environment by
  highlighting their failures within this context while also proposing an
  evolution of these algorithms that may ultimately make them sensitive and
  useful within this spatial and cultural milieu.</td>
  <td colspan=2 style='mso-ignore:colspan'>4 January, 2023</td>
 </tr>
 <tr height=204 style='height:153.0pt'>
  <td height=204 align=right style='height:153.0pt'>4836</td>
  <td align=right>2301.08741</td>
  <td class=xl65 width=649 style='width:487pt'>Enactive Artificial
  Intelligence: Subverting Gender Norms in Robot-Human Interaction</td>
  <td class=xl65 width=649 style='width:487pt'>This paper introduces Enactive
  Artificial Intelligence (eAI) as an intersectional gender-inclusive stance
  towards AI. AI design is an enacted human sociocultural practice that
  reflects human culture and values. Unrepresentative AI design could lead to
  social marginalisation. Section 1, drawing from radical enactivism, outlines
  embodied cultural practices. In Section 2, explores how intersectional gender
  intertwines with technoscience as a sociocultural practice. Section 3 focuses
  on subverting gender norms in the specific case of Robot-Human Interaction in
  AI. Finally, Section 4 identifies four vectors of ethics: explainability,
  fairness, transparency, and auditability for adopting an
  intersectionality-inclusive stance in developing gender-inclusive AI and
  subverting existing gender norms in robot design.</td>
  <td colspan=2 style='mso-ignore:colspan'>7 May, 2023</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>4835</td>
  <td align=right>2301.08742</td>
  <td class=xl65 width=649 style='width:487pt'>Unifying Consciousness and Time
  to Enhance Artificial Intelligence</td>
  <td class=xl65 width=649 style='width:487pt'>Consciousness is a sequential
  process of awareness which can focus on one piece of information at a time.
  This process of awareness experiences causation which underpins the notion of
  time while it interplays with matter and energy, forming reality. The study
  of Consciousness, time and reality is complex and evolving fast in many
  fields, including metaphysics and fundamental physics. Reality composes
  patterns in human Consciousness in response to the regularities in nature.
  These regularities could be physical (e.g., astronomical, environmental),
  biological, chemical, mental, social, etc. The patterns that emerged in
  Consciousness were correlated to the environment, life and social behaviours
  followed by constructed frameworks, systems and structures. The complex
  constructs evolved as cultures, customs, norms and values, which created a
  diverse society. In the evolution of responsible AI, it is important to be
  attuned to the evolved cultural, ethical and moral values through
  Consciousness. This requires the advocated design of self-learning AI aware
  of time perception and human ethics.</td>
  <td colspan=2 style='mso-ignore:colspan'>10 January, 2023</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>4816</td>
  <td align=right>2301.12073</td>
  <td class=xl65 width=649 style='width:487pt'>Towards Equitable Representation
  in Text-to-Image Synthesis Models with the Cross-Cultural Understanding
  Benchmark (CCUB) Dataset</td>
  <td class=xl65 width=649 style='width:487pt'>It has been shown that accurate
  representation in media improves the well-being of the people who consume it.
  By contrast, inaccurate representations can negatively affect viewers and
  lead to harmful perceptions of other cultures. To achieve inclusive representation
  in generated images, we propose a culturally-aware priming approach for
  text-to-image synthesis using a small but culturally curated dataset that we
  collected, known here as Cross-Cultural Understanding Benchmark (CCUB)
  Dataset, to fight the bias prevalent in giant datasets. Our proposed approach
  is comprised of two fine-tuning techniques: (1) Adding visual context via
  fine-tuning a pre-trained text-to-image synthesis model, Stable Diffusion, on
  the CCUB text-image pairs, and (2) Adding semantic context via automated
  prompt engineering using the fine-tuned large language model, GPT-3, trained
  on our CCUB culturally-aware text data. CCUB dataset is curated and our
  approach is evaluated by people who have a personal relationship with that particular
  culture. Our experiments indicate that priming using both text and image is
  effective in improving the cultural relevance and decreasing the
  offensiveness of generated images while maintaining quality.</td>
  <td colspan=2 style='mso-ignore:colspan'>26 April, 2023</td>
 </tr>
 <tr height=453 style='height:340.0pt'>
  <td height=453 align=right style='height:340.0pt'>4761</td>
  <td align=right>2302.09051</td>
  <td class=xl65 width=649 style='width:487pt'>Complex QA and language models
  hybrid architectures, Survey</td>
  <td class=xl65 width=649 style='width:487pt'>This paper reviews the
  state-of-the-art of language models architectures and strategies for
  &quot;complex&quot; question-answering (QA, CQA, CPS) with a focus on
  hybridization. Large Language Models (LLM) are good at leveraging public data
  on standard problems but once you want to tackle more specific complex
  questions or problems (e.g. How does the concept of personal freedom vary
  between different cultures ? What is the best mix of power generation methods
  to reduce climate change ?) you may need specific architecture, knowledge,
  skills, methods, sensitive data protection, explainability, human approval
  and versatile feedback... Recent projects like ChatGPT and GALACTICA have
  allowed non-specialists to grasp the great potential as well as the equally
  strong limitations of LLM in complex QA. In this paper, we start by reviewing
  required skills and evaluation techniques. We integrate findings from the
  robust community edited research papers BIG, BLOOM and HELM which open
  source, benchmark and analyze limits and challenges of LLM in terms of tasks
  complexity and strict evaluation on accuracy (e.g. fairness, robustness,
  toxicity, ...) as a baseline. We discuss some challenges associated with
  complex QA, including domain adaptation, decomposition and efficient
  multi-step QA, long form and non-factoid QA, safety and multi-sensitivity
  data protection, multimodal search, hallucinations, explainability and
  truthfulness, temporal reasoning. We analyze current solutions and promising
  research trends, using elements such as: hybrid LLM architectural patterns,
  training and prompting strategies, active human reinforcement learning
  supervised with AI, neuro-symbolic and structured knowledge grounding,
  program synthesis, iterated decomposition and others.</td>
  <td colspan=2 style='mso-ignore:colspan'>7 April, 2023</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>4756</td>
  <td align=right>2302.1048</td>
  <td class=xl65 width=649 style='width:487pt'>Climate Model Driven Seasonal
  Forecasting Approach with Deep Learning</td>
  <td class=xl65 width=649 style='width:487pt'>Understanding seasonal climatic
  conditions is critical for better management of resources such as water,
  energy and agriculture. Recently, there has been a great interest in
  utilizing the power of artificial intelligence methods in climate studies.
  This paper presents a cutting-edge deep learning model (UNet++) trained by
  state-of-the-art global CMIP6 models to forecast global temperatures a month
  ahead using the ERA5 reanalysis dataset. ERA5 dataset was also used for
  finetuning as well performance analysis in the validation dataset. Three
  different setups (CMIP6; CMIP6 + elevation; CMIP6 + elevation + ERA5
  finetuning) were used with both UNet and UNet++ algorithms resulting in six
  different models. For each model 14 different sequential and non-sequential
  temporal settings were used. The Mean Absolute Error (MAE) analysis revealed
  that UNet++ with CMIP6 with elevation and ERA5 finetuning model with
  &quot;Year 3 Month 2&quot; temporal case provided the best outcome with an
  MAE of 0.7. Regression analysis over the validation dataset between the ERA5
  data values and the corresponding AI model predictions revealed slope and
  $R^2$ values close to 1 suggesting a very good agreement. The AI model
  predicts significantly better than the mean CMIP6 ensemble between 2016 and
  2021. Both models predict the summer months more accurately than the winter
  months.</td>
  <td colspan=2 style='mso-ignore:colspan'>21 February, 2023</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>4748</td>
  <td align=right>2302.12299</td>
  <td class=xl65 width=649 style='width:487pt'>In What Languages are Generative
  Language Models the Most Formal? Analyzing Formality Distribution across
  Languages</td>
  <td class=xl65 width=649 style='width:487pt'>Multilingual generative language
  models (LMs) are increasingly fluent in a large variety of languages. Trained
  on the concatenation of corpora in multiple languages, they enable powerful
  transfer from high-resource languages to low-resource ones. However, it is
  still unknown what cultural biases are induced in the predictions of these
  models. In this work, we focus on one language property highly influenced by
  culture: formality. We analyze the formality distributions of XGLM and
  BLOOM's predictions, two popular generative multilingual language models, in
  5 languages. We classify 1,200 generations per language as formal, informal,
  or incohesive and measure the impact of the prompt formality on the
  predictions. Overall, we observe a diversity of behaviors across the models
  and languages. For instance, XGLM generates informal text in Arabic and
  Bengali when conditioned with informal prompts, much more than BLOOM. In
  addition, even though both models are highly biased toward the formal style
  when prompted neutrally, we find that the models generate a significant
  amount of informal predictions even when prompted with formal text. We
  release with this work 6,000 annotated samples, paving the way for future
  work on the formality of generative multilingual LMs.</td>
  <td colspan=2 style='mso-ignore:colspan'>23 February, 2023</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>4746</td>
  <td align=right>2302.12578</td>
  <td class=xl65 width=649 style='width:487pt'>Fairness in Language Models
  Beyond English: Gaps and Challenges</td>
  <td class=xl65 width=649 style='width:487pt'>With language models becoming
  increasingly ubiquitous, it has become essential to address their inequitable
  treatment of diverse demographic groups and factors. Most research on
  evaluating and mitigating fairness harms has been concentrated on English, while
  multilingual models and non-English languages have received comparatively
  little attention. This paper presents a survey of fairness in multilingual
  and non-English contexts, highlighting the shortcomings of current research
  and the difficulties faced by methods designed for English. We contend that
  the multitude of diverse cultures and languages across the world makes it
  infeasible to achieve comprehensive coverage in terms of constructing
  fairness datasets. Thus, the measurement and mitigation of biases must evolve
  beyond the current dataset-driven practices that are narrowly focused on
  specific dimensions and types of biases and, therefore, impossible to scale
  across languages and cultures.</td>
  <td colspan=2 style='mso-ignore:colspan'>28 February, 2023</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>4728</td>
  <td align=right>2303.00377</td>
  <td class=xl65 width=649 style='width:487pt'>Few-shots Portrait Generation
  with Style Enhancement and Identity Preservation</td>
  <td class=xl65 width=649 style='width:487pt'>Nowadays, the wide application
  of virtual digital human promotes the comprehensive prosperity and
  development of digital culture supported by digital economy. The personalized
  portrait automatically generated by AI technology needs both the natural
  artistic style and human sentiment. In this paper, we propose a novel
  StyleIdentityGAN model, which can ensure the identity and artistry of the
  generated portrait at the same time. Specifically, the style-enhanced module
  focuses on artistic style features decoupling and transferring to improve the
  artistry of generated virtual face images. Meanwhile, the identity-enhanced
  module preserves the significant features extracted from the input photo.
  Furthermore, the proposed method requires a small number of reference style
  data. Experiments demonstrate the superiority of StyleIdentityGAN over
  state-of-art methods in artistry and identity effects, with comparisons done
  qualitatively, quantitatively and through a perceptual user study. Code has
  been released on Github3.</td>
  <td colspan=2 style='mso-ignore:colspan'>1 March, 2023</td>
 </tr>
 <tr height=431 style='height:323.0pt'>
  <td height=431 align=right style='height:323.0pt'>4701</td>
  <td align=right>2303.06049</td>
  <td class=xl65 width=649 style='width:487pt'>Affordable Artificial
  Intelligence -- Augmenting Farmer Knowledge with AI</td>
  <td class=xl65 width=649 style='width:487pt'>Farms produce hundreds of
  thousands of data points on the ground daily. Farming technique which
  combines farming practices with the insights uncovered in these data points
  using AI technology is called precision farming. Precision farming technology
  augments and extends farmers' deep knowledge about their land, making
  production more sustainable and profitable. As part of the larger effort at
  Microsoft for empowering agricultural labor force to be more productive and
  sustainable, this paper presents the AI technology for predicting
  micro-climate conditions on the farm. This article is a chapter in
  publication by Food and Agriculture Organization of the United Nations and
  International Telecommunication Union Bangkok, 2021. This publication on
  artificial intelligence (AI) for agriculture is the fifth in the
  E-agriculture in Action series, launched in 2016 and jointly produced by FAO
  and ITU. It aims to raise awareness about existing AI applications in
  agriculture and to inspire stakeholders to develop and replicate the new
  ones. Improvement of capacity and tools for capturing and processing data and
  substantial advances in the field of machine learning open new horizons for
  data-driven solutions that can support decision-making, facilitate
  supervision and monitoring, improve the timeliness and effectiveness of
  safety measures (e.g. use of pesticides), and support automation of many
  resource-consuming tasks in agriculture. This publication presents the reader
  with a collection of informative applications highlighting various ways AI is
  used in agriculture and offering valuable insights on the implementation
  process, success factors, and lessons learnt.</td>
  <td colspan=2 style='mso-ignore:colspan'>3 March, 2023</td>
 </tr>
 <tr height=408 style='height:306.0pt'>
  <td height=408 align=right style='height:306.0pt'>4699</td>
  <td align=right>2303.06794</td>
  <td class=xl65 width=649 style='width:487pt'>Sensing Wellbeing in the
  Workplace, Why and For Whom? Envisioning Impacts with Organizational
  Stakeholders</td>
  <td class=xl65 width=649 style='width:487pt'>With the heightened digitization
  of the workplace, alongside the rise of remote and hybrid work prompted by
  the pandemic, there is growing corporate interest in using passive sensing
  technologies for workplace wellbeing. Existing research on these technologies
  often focus on understanding or improving interactions between an individual
  user and the technology. Workplace settings can, however, introduce a range
  of complexities that challenge the potential impact and in-practice
  desirability of wellbeing sensing technologies. Today, there is an inadequate
  empirical understanding of how everyday workers -- including those who are
  impacted by, and impact the deployment of workplace technologies -- envision
  its broader socio-ecological impacts. In this study, we conduct
  storyboard-driven interviews with 33 participants across three stakeholder
  groups: organizational governors, AI builders, and worker data subjects.
  Overall, our findings surface how workers envisioned wellbeing sensing
  technologies may lead to cascading impacts on their broader organizational
  culture, interpersonal relationships with colleagues, and individual
  day-to-day lives. Participants anticipated harms arising from ambiguity and
  misalignment around scaled notions of ``worker wellbeing,'' underlying
  technical limitations to workplace-situated sensing, and assumptions
  regarding how social structures and relationships may shape the impacts and
  use of these technologies. Based on our findings, we discuss implications for
  designing worker-centered data-driven wellbeing technologies.</td>
  <td colspan=2 style='mso-ignore:colspan'>6 June, 2023</td>
 </tr>
 <tr height=249 style='height:187.0pt'>
  <td height=249 align=right style='height:187.0pt'>4698</td>
  <td align=right>2303.06956</td>
  <td class=xl65 width=649 style='width:487pt'>Teaching and learning in the age
  of artificial intelligence</td>
  <td class=xl65 width=649 style='width:487pt'>As part of the Digital Working
  Group (GTnum) #Scol_IA &quot;Renewal of digital practices and creative uses
  of digital and AI&quot; we are pleased to present the white paper
  &quot;Teaching and learning in the era of Artificial Intelligence,
  Acculturation, integration and creative uses of AI in education&quot;. The
  white paper edited by Margarida Romero, Laurent Heiser and Alexandre Lepage
  aims to provide the various educational actors with a diversified perspective
  both on the issues of acculturation and training in AI and on the resources
  and feedback from the various research teams and organisations. of scientific
  culture in the French-speaking countries. A multidisciplinary approach makes
  it possible to consider the perspectives of researchers in computer science
  as well as those of education and training sciences, information and
  communication sciences and the expertise of teaching professionals. and
  scientific mediation.</td>
  <td colspan=2 style='mso-ignore:colspan'>14 March, 2023</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>4693</td>
  <td align=right>2303.08027</td>
  <td class=xl65 width=649 style='width:487pt'>A Hierarchical Regression Chain
  Framework for Affective Vocal Burst Recognition</td>
  <td class=xl65 width=649 style='width:487pt'>As a common way of emotion
  signaling via non-linguistic vocalizations, vocal burst (VB) plays an
  important role in daily social interaction. Understanding and modeling human
  vocal bursts are indispensable for developing robust and general artificial
  intelligence. Exploring computational approaches for understanding vocal
  bursts is attracting increasing research attention. In this work, we propose
  a hierarchical framework, based on chain regression models, for affective
  recognition from VBs, that explicitly considers multiple relationships: (i)
  between emotional states and diverse cultures; (ii) between low-dimensional
  (arousal &amp; valence) and high-dimensional (10 emotion classes) emotion
  spaces; and (iii) between various emotion classes within the high-dimensional
  space. To address the challenge of data sparsity, we also use self-supervised
  learning (SSL) representations with layer-wise and temporal aggregation
  modules. The proposed systems participated in the ACII Affective Vocal Burst
  (A-VB) Challenge 2022 and ranked first in the &quot;TWO'' and &quot;CULTURE''
  tasks. Experimental results based on the ACII Challenge 2022 dataset
  demonstrate the superior performance of the proposed system and the
  effectiveness of considering multiple relationships using hierarchical
  regression chain models.</td>
  <td colspan=2 style='mso-ignore:colspan'>14 March, 2023</td>
 </tr>
 <tr height=475 style='height:356.0pt'>
  <td height=475 align=right style='height:356.0pt'>4669</td>
  <td align=right>2303.12429</td>
  <td class=xl65 width=649 style='width:487pt'>Man vs the machine: The Struggle
  for Effective Text Anonymisation in the Age of Large Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>The collection and use of
  personal data are becoming more common in today's data-driven culture. While
  there are many advantages to this, including better decision-making and
  service delivery, it also poses significant ethical issues around
  confidentiality and privacy. Text anonymisation tries to prune and/or mask
  identifiable information from a text while keeping the remaining content
  intact to alleviate privacy concerns. Text anonymisation is especially
  important in industries like healthcare, law, as well as research, where
  sensitive and personal information is collected, processed, and exchanged
  under high legal and ethical standards. Although text anonymization is widely
  adopted in practice, it continues to face considerable challenges. The most significant
  challenge is striking a balance between removing information to protect
  individuals' privacy while maintaining the text's usability for future
  purposes. The question is whether these anonymisation methods sufficiently
  reduce the risk of re-identification, in which an individual can be
  identified based on the remaining information in the text. In this work, we
  challenge the effectiveness of these methods and how we perceive identifiers.
  We assess the efficacy of these methods against the elephant in the room, the
  use of AI over big data. While most of the research is focused on identifying
  and removing personal information, there is limited discussion on whether the
  remaining information is sufficient to deanonymise individuals and, more precisely,
  who can do it. To this end, we conduct an experiment using GPT over
  anonymised texts of famous people to determine whether such trained networks
  can deanonymise them. The latter allows us to revise these methods and
  introduce a novel methodology that employs Large Language Models to improve
  the anonymity of texts.</td>
  <td colspan=2 style='mso-ignore:colspan'>22 March, 2023</td>
 </tr>
 <tr height=385 style='height:289.0pt'>
  <td height=385 align=right style='height:289.0pt'>4653</td>
  <td align=right>2303.16352</td>
  <td class=xl65 width=649 style='width:487pt'>ChatGPT or academic scientist?
  Distinguishing authorship with over 99% accuracy using off-the-shelf machine
  learning tools</td>
  <td class=xl65 width=649 style='width:487pt'>ChatGPT has enabled access to
  AI-generated writing for the masses, and within just a few months, this
  product has disrupted the knowledge economy, initiating a culture shift in
  the way people work, learn, and write. The need to discriminate human writing
  from AI is now both critical and urgent, particularly in domains like higher
  education and academic writing, where AI had not been a significant threat or
  contributor to authorship. Addressing this need, we developed a method for
  discriminating text generated by ChatGPT from (human) academic scientists,
  relying on prevalent and accessible supervised classification methods. We
  focused on how a particular group of humans, academic scientists, write
  differently than ChatGPT, and this targeted approach led to the discovery of
  new features for discriminating (these) humans from AI; as examples,
  scientists write long paragraphs and have a penchant for equivocal language,
  frequently using words like but, however, and although. With a set of 20
  features, including the aforementioned ones and others, we built a model that
  assigned the author, as human or AI, at well over 99% accuracy, resulting in
  20 times fewer misclassified documents compared to the field-leading
  approach. This strategy for discriminating a particular set of humans writing
  from AI could be further adapted and developed by others with basic skills in
  supervised classification, enabling access to many highly accurate and
  targeted models for detecting AI usage in academic writing and beyond.</td>
  <td colspan=2 style='mso-ignore:colspan'>28 March, 2023</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>4647</td>
  <td align=right>2303.16972</td>
  <td class=xl65 width=649 style='width:487pt'>Queer In AI: A Case Study in
  Community-Led Participatory AI</td>
  <td class=xl65 width=649 style='width:487pt'>We present Queer in AI as a case
  study for community-led participatory design in AI. We examine how
  participatory design and intersectional tenets started and shaped this
  community's programs over the years. We discuss different challenges that
  emerged in the process, look at ways this organization has fallen short of
  operationalizing participatory and intersectional principles, and then assess
  the organization's impact. Queer in AI provides important lessons and
  insights for practitioners and theorists of participatory methods broadly
  through its rejection of hierarchy in favor of decentralization, success at
  building aid and programs by and for the queer community, and effort to
  change actors and institutions outside of the queer community. Finally, we
  theorize how communities like Queer in AI contribute to the participatory
  design in AI more broadly by fostering cultures of participation in AI,
  welcoming and empowering marginalized participants, critiquing poor or
  exploitative participatory practices, and bringing participation to
  institutions outside of individual research projects. Queer in AI's work
  serves as a case study of grassroots activism and participatory methods
  within AI, demonstrating the potential of community-led participatory methods
  and intersectional praxis, while also providing challenges, case studies, and
  nuanced insights to researchers developing and using participatory methods.</td>
  <td colspan=2 style='mso-ignore:colspan'>8 June, 2023</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>4636</td>
  <td align=right>2303.17927</td>
  <td class=xl65 width=649 style='width:487pt'>Cross-Cultural Transfer Learning
  for Chinese Offensive Language Detection</td>
  <td class=xl65 width=649 style='width:487pt'>Detecting offensive language is
  a challenging task. Generalizing across different cultures and languages
  becomes even more challenging: besides lexical, syntactic and semantic
  differences, pragmatic aspects such as cultural norms and sensitivities,
  which are particularly relevant in this context, vary greatly. In this paper,
  we target Chinese offensive language detection and aim to investigate the
  impact of transfer learning using offensive language detection data from
  different cultural backgrounds, specifically Korean and English. We find that
  culture-specific biases in what is considered offensive negatively impact the
  transferability of language models (LMs) and that LMs trained on diverse
  cultural data are sensitive to different features in Chinese offensive
  language detection. In a few-shot learning scenario, however, our study shows
  promising prospects for non-English offensive language detection with limited
  resources. Our findings highlight the importance of cross-cultural transfer
  learning in improving offensive language detection and promoting inclusive
  digital spaces.</td>
  <td colspan=2 style='mso-ignore:colspan'>31 March, 2023</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>4617</td>
  <td align=right>2304.02754</td>
  <td class=xl65 width=649 style='width:487pt'>Conceptual structure coheres in
  human cognition but not in large language models</td>
  <td class=xl65 width=649 style='width:487pt'>Neural network models of
  language have long been used as a tool for developing hypotheses about
  conceptual representation in the mind and brain. For many years, such use
  involved extracting vector-space representations of words and using distances
  among these to predict or understand human behavior in various semantic
  tasks. Contemporary large language models (LLMs), however, make it possible
  to interrogate the latent structure of conceptual representations using
  experimental methods nearly identical to those commonly used with human
  participants. The current work utilizes three common techniques borrowed from
  cognitive psychology to estimate and compare the structure of concepts in
  humans and a suite of LLMs. In humans, we show that conceptual structure is
  robust to differences in culture, language, and method of estimation.
  Structures estimated from LLM behavior, while individually fairly consistent
  with those estimated from human behavior, vary much more depending upon the
  particular task used to generate responses--across tasks, estimates of
  conceptual structure from the very same model cohere less with one another
  than do human structure estimates. These results highlight an important
  difference between contemporary LLMs and human cognition, with implications
  for understanding some fundamental limitations of contemporary machine
  language.</td>
  <td colspan=2 style='mso-ignore:colspan'>10 November, 2023</td>
 </tr>
 <tr height=496 style='height:372.0pt'>
  <td height=496 align=right style='height:372.0pt'>4605</td>
  <td align=right>2304.04966</td>
  <td class=xl65 width=649 style='width:487pt'>Computer Vision-Aided
  Intelligent Monitoring of Coffee: Towards Sustainable Coffee Production</td>
  <td class=xl65 width=649 style='width:487pt'>Coffee which is prepared from
  the grinded roasted seeds of harvested coffee cherries, is one of the most
  consumed beverage and traded commodity, globally. To manually monitor the
  coffee field regularly, and inform about plant and soil health, as well as estimate
  yield and harvesting time, is labor-intensive, time-consuming and
  error-prone. Some recent studies have developed sensors for estimating coffee
  yield at the time of harvest, however a more inclusive and applicable
  technology to remotely monitor multiple parameters of the field and estimate
  coffee yield and quality even at pre-harvest stage, was missing. Following
  precision agriculture approach, we employed machine learning algorithm YOLO,
  for image processing of coffee plant. In this study, the latest version of
  the state-of-the-art algorithm YOLOv7 was trained with 324 annotated images
  followed by its evaluation with 82 unannotated images as test data. Next, as
  an innovative approach for annotating the training data, we trained K-means
  models which led to machine-generated color classes of coffee fruit and could
  thus characterize the informed objects in the image. Finally, we attempted to
  develop an AI-based handy mobile application which would not only efficiently
  predict harvest time, estimate coffee yield and quality, but also inform
  about plant health. Resultantly, the developed model efficiently analyzed the
  test data with a mean average precision of 0.89. Strikingly, our innovative
  semi-supervised method with an mean average precision of 0.77 for multi-class
  mode surpassed the supervised method with mean average precision of only
  0.60, leading to faster and more accurate annotation. The mobile application
  we designed based on the developed code, was named CoffeApp, which possesses
  multiple features of analyzing fruit from the image taken by phone camera
  with in field and can thus track fruit ripening in real time.</td>
  <td colspan=2 style='mso-ignore:colspan'>11 April, 2023</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>4589</td>
  <td align=right>2304.06953</td>
  <td class=xl65 width=649 style='width:487pt'>Cultural-aware Machine Learning
  based Analysis of COVID-19 Vaccine Hesitancy</td>
  <td class=xl65 width=649 style='width:487pt'>Understanding the COVID-19
  vaccine hesitancy, such as who and why, is very crucial since a large-scale
  vaccine adoption remains as one of the most efficient methods of controlling
  the pandemic. Such an understanding also provides insights into designing successful
  vaccination campaigns for future pandemics. Unfortunately, there are many
  factors involving in deciding whether to take the vaccine, especially from
  the cultural point of view. To obtain these goals, we design a novel
  culture-aware machine learning (ML) model, based on our new data collection,
  for predicting vaccination willingness. We further analyze the most important
  features which contribute to the ML model's predictions using advanced AI
  explainers such as the Probabilistic Graphical Model (PGM) and Shapley
  Additive Explanations (SHAP). These analyses reveal the key factors that most
  likely impact the vaccine adoption decisions. Our findings show that Hispanic
  and African American are most likely impacted by cultural characteristics such
  as religions and ethnic affiliation, whereas the vaccine trust and approval
  influence the Asian communities the most. Our results also show that cultural
  characteristics, rumors, and political affiliation are associated with
  increased vaccine rejection.</td>
  <td colspan=2 style='mso-ignore:colspan'>14 April, 2023</td>
 </tr>
 <tr height=227 style='height:170.0pt'>
  <td height=227 align=right style='height:170.0pt'>4585</td>
  <td align=right>2304.07778</td>
  <td class=xl65 width=649 style='width:487pt'>SikuGPT: A Generative
  Pre-trained Model for Intelligent Information Processing of Ancient Texts
  from the Perspective of Digital Humanities</td>
  <td class=xl65 width=649 style='width:487pt'>The rapid advance in artificial
  intelligence technology has facilitated the prosperity of digital humanities
  research. Against such backdrop, research methods need to be transformed in
  the intelligent processing of ancient texts, which is a crucial component of
  digital humanities research, so as to adapt to new development trends in the
  wave of AIGC. In this study, we propose a GPT model called SikuGPT based on
  the corpus of Siku Quanshu. The model's performance in tasks such as
  intralingual translation and text classification exceeds that of other
  GPT-type models aimed at processing ancient texts. SikuGPT's ability to
  process traditional Chinese ancient texts can help promote the organization
  of ancient information and knowledge services, as well as the international
  dissemination of Chinese ancient culture.</td>
  <td colspan=2 style='mso-ignore:colspan'>16 April, 2023</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>4583</td>
  <td align=right>2304.0788</td>
  <td class=xl65 width=649 style='width:487pt'>Sabiá: Portuguese Large Language
  Models</td>
  <td class=xl65 width=649 style='width:487pt'>As the capabilities of language
  models continue to advance, it is conceivable that
  &quot;one-size-fits-all&quot; model will remain as the main paradigm. For
  instance, given the vast number of languages worldwide, many of which are
  low-resource, the prevalent practice is to pretrain a single model on
  multiple languages. In this paper, we add to the growing body of evidence
  that challenges this practice, demonstrating that monolingual pretraining on
  the target language significantly improves models already extensively trained
  on diverse corpora. More specifically, we further pretrain GPT-J and LLaMA
  models on Portuguese texts using 3% or less of their original pretraining
  budget. Few-shot evaluations on Poeta, a suite of 14 Portuguese datasets,
  reveal that our models outperform English-centric and multilingual
  counterparts by a significant margin. Our best model, Sabiá-65B, performs on
  par with GPT-3.5-turbo. By evaluating on datasets originally conceived in the
  target language as well as translated ones, we study the contributions of
  language-specific pretraining in terms of 1) capturing linguistic nuances and
  structures inherent to the target language, and 2) enriching the model's
  knowledge about a domain or culture. Our results indicate that the majority
  of the benefits stem from the domain-specific knowledge acquired through
  monolingual pretraining.</td>
  <td colspan=2 style='mso-ignore:colspan'>9 November, 2023</td>
 </tr>
 <tr height=496 style='height:372.0pt'>
  <td height=496 align=right style='height:372.0pt'>4577</td>
  <td align=right>2304.0924</td>
  <td class=xl65 width=649 style='width:487pt'>The Metaverse: Survey, Trends,
  Novel Pipeline Ecosystem &amp; Future Directions</td>
  <td class=xl65 width=649 style='width:487pt'>The Metaverse offers a second
  world beyond reality, where boundaries are non-existent, and possibilities
  are endless through engagement and immersive experiences using the virtual
  reality (VR) technology. Many disciplines can benefit from the advancement of
  the Metaverse when accurately developed, including the fields of technology,
  gaming, education, art, and culture. Nevertheless, developing the Metaverse
  environment to its full potential is an ambiguous task that needs proper
  guidance and directions. Existing surveys on the Metaverse focus only on a
  specific aspect and discipline of the Metaverse and lack a holistic view of
  the entire process. To this end, a more holistic, multi-disciplinary,
  in-depth, and academic and industry-oriented review is required to provide a
  thorough study of the Metaverse development pipeline. To address these
  issues, we present in this survey a novel multi-layered pipeline ecosystem
  composed of (1) the Metaverse computing, networking, communications and
  hardware infrastructure, (2) environment digitization, and (3) user
  interactions. For every layer, we discuss the components that detail the
  steps of its development. Also, for each of these components, we examine the
  impact of a set of enabling technologies and empowering domains (e.g.,
  Artificial Intelligence, Security &amp; Privacy, Blockchain, Business,
  Ethics, and Social) on its advancement. In addition, we explain the
  importance of these technologies to support decentralization,
  interoperability, user experiences, interactions, and monetization. Our
  presented study highlights the existing challenges for each component,
  followed by research directions and potential solutions. To the best of our
  knowledge, this survey is the most comprehensive and allows users, scholars,
  and entrepreneurs to get an in-depth understanding of the Metaverse ecosystem
  to find their opportunities and potentials for contribution.</td>
  <td colspan=2 style='mso-ignore:colspan'>18 April, 2023</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>4569</td>
  <td align=right>2304.10182</td>
  <td class=xl65 width=649 style='width:487pt'>Using Text-to-Image Generation
  for Architectural Design Ideation</td>
  <td class=xl65 width=649 style='width:487pt'>The recent progress of
  text-to-image generation has been recognized in architectural design. Our
  study is the first to investigate the potential of text-to-image generators
  in supporting creativity during the early stages of the architectural design
  process. We conducted a laboratory study with 17 architecture students, who
  developed a concept for a culture center using three popular text-to-image
  generators: Midjourney, Stable Diffusion, and DALL-E. Through standardized
  questionnaires and group interviews, we found that image generation could be
  a meaningful part of the design process when design constraints are carefully
  considered. Generative tools support serendipitous discovery of ideas and an
  imaginative mindset, enriching the design process. We identified several
  challenges of image generators and provided considerations for software
  development and educators to support creativity and emphasize designers'
  imaginative mindset. By understanding the limitations and potential of
  text-to-image generators, architects and designers can leverage this
  technology in their design process and education, facilitating innovation and
  effective communication of concepts.</td>
  <td colspan=2 style='mso-ignore:colspan'>20 April, 2023</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>4466</td>
  <td align=right>2305.10834</td>
  <td class=xl65 width=649 style='width:487pt'>AIwriting: Relations Between
  Image Generation and Digital Writing</td>
  <td class=xl65 width=649 style='width:487pt'>During 2022, both
  transformer-based AI text generation sys-tems such as GPT-3 and AI
  text-to-image generation systems such as DALL-E 2 and Stable Diffusion made
  exponential leaps forward and are unquestionably altering the fields of
  digital art and electronic literature. In this panel a group of electronic
  literature authors and theorists consider new oppor-tunities for human
  creativity presented by these systems and present new works have produced
  during the past year that specifically address these systems as environments
  for literary expressions that are translated through iterative interlocutive
  processes into visual representations. The premise that binds these
  presentations is that these systems and the works gener-ated must be
  considered from a literary perspective, as they originate in human writing.
  In works ranging from a visual memoir of the personal experience of a health
  crisis, to interac-tive web comics, to architectures based on abstract poetic
  language, to political satire, four artists explore the capabili-ties of
  these writing environments for new genres of literary artist practice, while
  a digital culture theorist considers the origins and effects of the
  particular training datasets of human language and images on which these new hybrid
  forms are based.</td>
  <td colspan=2 style='mso-ignore:colspan'>18 May, 2023</td>
 </tr>
 <tr height=249 style='height:187.0pt'>
  <td height=249 align=right style='height:187.0pt'>4459</td>
  <td align=right>2305.11844</td>
  <td class=xl65 width=649 style='width:487pt'>AI's Regimes of Representation:
  A Community-centered Study of Text-to-Image Models in South Asia</td>
  <td class=xl65 width=649 style='width:487pt'>This paper presents a
  community-centered study of cultural limitations of text-to-image (T2I)
  models in the South Asian context. We theorize these failures using
  scholarship on dominant media regimes of representations and locate them
  within participants' reporting of their existing social marginalizations. We
  thus show how generative AI can reproduce an outsiders gaze for viewing South
  Asian cultures, shaped by global and regional power inequities. By centering
  communities as experts and soliciting their perspectives on T2I limitations,
  our study adds rich nuance into existing evaluative frameworks and deepens
  our understanding of the culturally-specific ways AI technologies can fail in
  non-Western and Global South settings. We distill lessons for responsible
  development of T2I models, recommending concrete pathways forward that can
  allow for recognition of structural inequalities.</td>
  <td colspan=2 style='mso-ignore:colspan'>19 May, 2023</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>4454</td>
  <td align=right>2305.12182</td>
  <td class=xl65 width=649 style='width:487pt'>Glot500: Scaling Multilingual
  Corpora and Language Models to 500 Languages</td>
  <td class=xl65 width=649 style='width:487pt'>The NLP community has mainly
  focused on scaling Large Language Models (LLMs) vertically, i.e., making them
  better for about 100 languages. We instead scale LLMs horizontally: we
  create, through continued pretraining, Glot500-m, an LLM that covers 511 predominantly
  low-resource languages. An important part of this effort is to collect and
  clean Glot500-c, a corpus that covers these 511 languages and allows us to
  train Glot500-m. We evaluate Glot500-m on five diverse tasks across these
  languages. We observe large improvements for both high-resource and
  low-resource languages compared to an XLM-R baseline. Our analysis shows that
  no single factor explains the quality of multilingual LLM representations.
  Rather, a combination of factors determines quality including corpus size,
  script, &quot;help&quot; from related languages and the total capacity of the
  model. Our work addresses an important goal of NLP research: we should not
  limit NLP to a small fraction of the world's languages and instead strive to
  support as many languages as possible to bring the benefits of NLP technology
  to all languages and cultures. Code, data and models are available at
  https://github.com/cisnlp/Glot500.</td>
  <td colspan=2 style='mso-ignore:colspan'>26 May, 2023</td>
 </tr>
 <tr height=249 style='height:187.0pt'>
  <td height=249 align=right style='height:187.0pt'>4424</td>
  <td align=right>2305.14328</td>
  <td class=xl65 width=649 style='width:487pt'>Benchmarking Machine Translation
  with Cultural Awareness</td>
  <td class=xl65 width=649 style='width:487pt'>Translating culture-related
  content is vital for effective cross-cultural communication. However, many
  culture-specific items (CSIs) often lack viable translations across
  languages, making it challenging to collect high-quality, diverse parallel
  corpora with CSI annotations. This difficulty hinders the analysis of
  cultural awareness of machine translation (MT) systems, including traditional
  neural MT and the emerging MT paradigm using large language models (LLM). To
  address this gap, we introduce a novel parallel corpus, enriched with CSI
  annotations in 6 language pairs for investigating Culturally-Aware Machine
  Translation--CAMT. Furthermore, we design two evaluation metrics to assess
  CSI translations, focusing on their pragmatic translation quality. Our
  findings show the superior ability of LLMs over neural MTs in leveraging
  external cultural knowledge for translating CSIs, especially those lacking
  translations in the target culture.</td>
  <td colspan=2 style='mso-ignore:colspan'>19 October, 2024</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>4423</td>
  <td align=right>2305.14456</td>
  <td class=xl65 width=649 style='width:487pt'>Having Beer after Prayer?
  Measuring Cultural Bias in Large Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>As the reach of large language
  models (LMs) expands globally, their ability to cater to diverse cultural
  contexts becomes crucial. Despite advancements in multilingual capabilities,
  models are not designed with appropriate cultural nuances. In this paper, we
  show that multilingual and Arabic monolingual LMs exhibit bias towards
  entities associated with Western culture. We introduce CAMeL, a novel
  resource of 628 naturally-occurring prompts and 20,368 entities spanning
  eight types that contrast Arab and Western cultures. CAMeL provides a
  foundation for measuring cultural biases in LMs through both extrinsic and
  intrinsic evaluations. Using CAMeL, we examine the cross-cultural performance
  in Arabic of 16 different LMs on tasks such as story generation, NER, and
  sentiment analysis, where we find concerning cases of stereotyping and
  cultural unfairness. We further test their text-infilling performance,
  revealing the incapability of appropriate adaptation to Arab cultural
  contexts. Finally, we analyze 6 Arabic pre-training corpora and find that
  commonly used sources such as Wikipedia may not be best suited to build
  culturally aware LMs, if used as they are without adjustment. We will make
  CAMeL publicly available at: https://github.com/tareknaous/camel</td>
  <td colspan=2 style='mso-ignore:colspan'>20 March, 2024</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>4422</td>
  <td align=right>2305.14492</td>
  <td class=xl65 width=649 style='width:487pt'>Sociocultural Norm Similarities
  and Differences via Situational Alignment and Explainable Textual Entailment</td>
  <td class=xl65 width=649 style='width:487pt'>Designing systems that can
  reason across cultures requires that they are grounded in the norms of the
  contexts in which they operate. However, current research on developing
  computational models of social norms has primarily focused on American
  society. Here, we propose a novel approach to discover and compare
  descriptive social norms across Chinese and American cultures. We demonstrate
  our approach by leveraging discussions on a Chinese Q&amp;A platform (Zhihu)
  and the existing SocialChemistry dataset as proxies for contrasting cultural
  axes, align social situations cross-culturally, and extract social norms from
  texts using in-context learning. Embedding Chain-of-Thought prompting in a
  human-AI collaborative framework, we build a high-quality dataset of 3,069
  social norms aligned with social situations across Chinese and American
  cultures alongside corresponding free-text explanations. To test the ability
  of models to reason about social norms across cultures, we introduce the task
  of explainable social norm entailment, showing that existing models under 3B
  parameters have significant room for improvement in both automatic and human
  evaluation. Further analysis of cross-cultural norm differences based on our
  dataset shows empirical alignment with the social orientations framework,
  revealing several situational and descriptive nuances in norms across these
  cultures.</td>
  <td colspan=2 style='mso-ignore:colspan'>22 October, 2023</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>4414</td>
  <td align=right>2305.15242</td>
  <td class=xl65 width=649 style='width:487pt'>Machine Unlearning: its nature,
  scope, and importance for a &quot;delete culture&quot;</td>
  <td class=xl65 width=649 style='width:487pt'>The article explores the
  cultural shift from recording to deleting information in the digital age and
  its implications on privacy, intellectual property (IP), and Large Language
  Models like ChatGPT. It begins by defining a delete culture where information,
  in principle legal, is made unavailable or inaccessible because unacceptable
  or undesirable, especially but not only due to its potential to infringe on
  privacy or IP. Then it focuses on two strategies in this context: deleting,
  to make information unavailable; and blocking, to make it inaccessible. The
  article argues that both strategies have significant implications,
  particularly for machine learning (ML) models where information is not easily
  made unavailable. However, the emerging research area of Machine Unlearning
  (MU) is highlighted as a potential solution. MU, still in its infancy, seeks
  to remove specific data points from ML models, effectively making them
  'forget' completely specific information. If successful, MU could provide a
  feasible means to manage the overabundance of information and ensure a better
  protection of privacy and IP. However, potential ethical risks, such as
  misuse, overuse, and underuse of MU, should be systematically studied to
  devise appropriate policies.</td>
  <td colspan=2 style='mso-ignore:colspan'>24 May, 2023</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>4410</td>
  <td align=right>2305.16171</td>
  <td class=xl65 width=649 style='width:487pt'>Multi-lingual and Multi-cultural
  Figurative Language Understanding</td>
  <td class=xl65 width=649 style='width:487pt'>Figurative language permeates
  human communication, but at the same time is relatively understudied in NLP.
  Datasets have been created in English to accelerate progress towards
  measuring and improving figurative language processing in language models
  (LMs). However, the use of figurative language is an expression of our
  cultural and societal experiences, making it difficult for these phrases to
  be universally applicable. In this work, we create a figurative language
  inference dataset, \datasetname, for seven diverse languages associated with
  a variety of cultures: Hindi, Indonesian, Javanese, Kannada, Sundanese,
  Swahili and Yoruba. Our dataset reveals that each language relies on cultural
  and regional concepts for figurative expressions, with the highest overlap
  between languages originating from the same region. We assess multilingual
  LMs' abilities to interpret figurative language in zero-shot and few-shot
  settings. All languages exhibit a significant deficiency compared to English,
  with variations in performance reflecting the availability of pre-training
  and fine-tuning data, emphasizing the need for LMs to be exposed to a broader
  range of linguistic and cultural variation during training.</td>
  <td colspan=2 style='mso-ignore:colspan'>25 May, 2023</td>
 </tr>
 <tr height=227 style='height:170.0pt'>
  <td height=227 align=right style='height:170.0pt'>4399</td>
  <td align=right>2305.17701</td>
  <td class=xl65 width=649 style='width:487pt'>KoSBi: A Dataset for Mitigating
  Social Bias Risks Towards Safer Large Language Model Application</td>
  <td class=xl65 width=649 style='width:487pt'>Large language models (LLMs)
  learn not only natural text generation abilities but also social biases
  against different demographic groups from real-world data. This poses a
  critical risk when deploying LLM-based applications. Existing research and
  resources are not readily applicable in South Korea due to the differences in
  language and culture, both of which significantly affect the biases and
  targeted demographic groups. This limitation requires localized social bias
  datasets to ensure the safe and effective deployment of LLMs. To this end, we
  present KO SB I, a new social bias dataset of 34k pairs of contexts and
  sentences in Korean covering 72 demographic groups in 15 categories. We find
  that through filtering-based moderation, social biases in generated content
  can be reduced by 16.47%p on average for HyperCLOVA (30B and 82B), and GPT-3.</td>
  <td colspan=2 style='mso-ignore:colspan'>29 May, 2023</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>4382</td>
  <td align=right>2305.19274</td>
  <td class=xl65 width=649 style='width:487pt'>Memory as a Mass-based Graph:
  Towards a Conceptual Framework for the Simulation Model of Human Memory in AI</td>
  <td class=xl65 width=649 style='width:487pt'>There are two approaches for
  simulating memory as well as learning in artificial intelligence; the
  functionalistic approach and the cognitive approach. The necessary condition
  to put the second approach into account is to provide a model of brain
  activity that contains a quite good congruence with observational facts such
  as mistakes and forgotten experiences. Given that human memory has a solid
  core that includes the components of our identity, our family and our
  hometown, the major and determinative events of our lives, and the countless
  repeated and accepted facts of our culture, the more we go to the peripheral
  spots the data becomes flimsier and more easily exposed to oblivion. It was
  essential to propose a model in which the topographical differences are quite
  distinguishable. In our proposed model, we have translated this topographical
  situation into quantities, which are attributed to the nodes. The result is
  an edge-weighted graph with mass-based values on the nodes which demonstrates
  the importance of each atomic proposition, as a truth, for an intelligent
  being. Furthermore, it dynamically develops and modifies, and in successive
  phases, it changes the mass of the nodes and weight of the edges depending on
  gathered inputs from the environment.</td>
  <td colspan=2 style='mso-ignore:colspan'>18 May, 2023</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>4352</td>
  <td align=right>2306.01857</td>
  <td class=xl65 width=649 style='width:487pt'>Knowledge of cultural moral
  norms in large language models</td>
  <td class=xl65 width=649 style='width:487pt'>Moral norms vary across
  cultures. A recent line of work suggests that English large language models
  contain human-like moral biases, but these studies typically do not examine
  moral variation in a diverse cultural setting. We investigate the extent to
  which monolingual English language models contain knowledge about moral norms
  in different countries. We consider two levels of analysis: 1) whether
  language models capture fine-grained moral variation across countries over a
  variety of topics such as ``homosexuality'' and ``divorce''; 2) whether
  language models capture cultural diversity and shared tendencies in which
  topics people around the globe tend to diverge or agree on in their moral
  judgment. We perform our analyses with two public datasets from the World
  Values Survey (across 55 countries) and PEW global surveys (across 40
  countries) on morality. We find that pre-trained English language models
  predict empirical moral norms across countries worse than the English moral
  norms reported previously. However, fine-tuning language models on the survey
  data improves inference across countries at the expense of a less accurate
  estimate of the English moral norms. We discuss the relevance and challenges
  of incorporating cultural knowledge into the automated inference of moral
  norms.</td>
  <td colspan=2 style='mso-ignore:colspan'>2 June, 2023</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>4330</td>
  <td align=right>2306.03646</td>
  <td class=xl65 width=649 style='width:487pt'>Dance Generation by Sound
  Symbolic Words</td>
  <td class=xl65 width=649 style='width:487pt'>This study introduces a novel
  approach to generate dance motions using onomatopoeia as input, with the aim
  of enhancing creativity and diversity in dance generation. Unlike text and
  music, onomatopoeia conveys rhythm and meaning through abstract word expressions
  without constraints on expression and without need for specialized knowledge.
  We adapt the AI Choreographer framework and employ the Sakamoto system, a
  feature extraction method for onomatopoeia focusing on phonemes and
  syllables. Additionally, we present a new dataset of 40 onomatopoeia-dance
  motion pairs collected through a user survey. Our results demonstrate that
  the proposed method enables more intuitive dance generation and can create
  dance motions using sound-symbolic words from a variety of languages,
  including those without onomatopoeia. This highlights the potential for
  diverse dance creation across different languages and cultures, accessible to
  a wider audience. Qualitative samples from our model can be found at:
  https://sites.google.com/view/onomatopoeia-dance/home/.</td>
  <td colspan=2 style='mso-ignore:colspan'>6 June, 2023</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>4323</td>
  <td align=right>2306.04141</td>
  <td class=xl65 width=649 style='width:487pt'>Art and the science of
  generative AI: A deeper dive</td>
  <td class=xl65 width=649 style='width:487pt'>A new class of tools,
  colloquially called generative AI, can produce high-quality artistic media
  for visual arts, concept art, music, fiction, literature, video, and
  animation. The generative capabilities of these tools are likely to
  fundamentally alter the creative processes by which creators formulate ideas
  and put them into production. As creativity is reimagined, so too may be many
  sectors of society. Understanding the impact of generative AI - and making
  policy decisions around it - requires new interdisciplinary scientific
  inquiry into culture, economics, law, algorithms, and the interaction of
  technology and creativity. We argue that generative AI is not the harbinger
  of art's demise, but rather is a new medium with its own distinct
  affordances. In this vein, we consider the impacts of this new medium on
  creators across four themes: aesthetics and culture, legal questions of
  ownership and credit, the future of creative work, and impacts on the
  contemporary media ecosystem. Across these themes, we highlight key research
  questions and directions to inform policy and beneficial uses of the
  technology.</td>
  <td colspan=2 style='mso-ignore:colspan'>7 June, 2023</td>
 </tr>
 <tr height=431 style='height:323.0pt'>
  <td height=431 align=right style='height:323.0pt'>4319</td>
  <td align=right>2306.04566</td>
  <td class=xl65 width=649 style='width:487pt'>Recent applications of machine
  learning, remote sensing, and iot approaches in yield prediction: a critical
  review</td>
  <td class=xl65 width=649 style='width:487pt'>The integration of remote
  sensing and machine learning in agriculture is transforming the industry by
  providing insights and predictions through data analysis. This combination
  leads to improved yield prediction and water management, resulting in increased
  efficiency, better yields, and more sustainable agricultural practices.
  Achieving the United Nations' Sustainable Development Goals, especially
  &quot;zero hunger,&quot; requires the investigation of crop yield and
  precipitation gaps, which can be accomplished through, the usage of
  artificial intelligence (AI), machine learning (ML), remote sensing (RS), and
  the internet of things (IoT). By integrating these technologies, a robust
  agricultural mobile or web application can be developed, providing farmers
  and decision-makers with valuable information and tools for improving crop
  management and increasing efficiency. Several studies have investigated these
  new technologies and their potential for diverse tasks such as crop
  monitoring, yield prediction, irrigation management, etc. Through a critical
  review, this paper reviews relevant articles that have used RS, ML, cloud
  computing, and IoT in crop yield prediction. It reviews the current
  state-of-the-art in this field by critically evaluating different
  machine-learning approaches proposed in the literature for crop yield
  prediction and water management. It provides insights into how these methods
  can improve decision-making in agricultural production systems. This work
  will serve as a compendium for those interested in yield prediction in terms
  of primary literature but, most importantly, what approaches can be used for
  real-time and robust prediction.</td>
  <td colspan=2 style='mso-ignore:colspan'>7 June, 2023</td>
 </tr>
 <tr height=385 style='height:289.0pt'>
  <td height=385 align=right style='height:289.0pt'>4315</td>
  <td align=right>2306.05076</td>
  <td class=xl65 width=649 style='width:487pt'>DLAMA: A Framework for Curating
  Culturally Diverse Facts for Probing the Knowledge of Pretrained Language
  Models</td>
  <td class=xl65 width=649 style='width:487pt'>A few benchmarking datasets have
  been released to evaluate the factual knowledge of pretrained language
  models. These benchmarks (e.g., LAMA, and ParaRel) are mainly developed in
  English and later are translated to form new multilingual versions (e.g., mLAMA,
  and mParaRel). Results on these multilingual benchmarks suggest that using
  English prompts to recall the facts from multilingual models usually yields
  significantly better and more consistent performance than using non-English
  prompts. Our analysis shows that mLAMA is biased toward facts from Western
  countries, which might affect the fairness of probing models. We propose a
  new framework for curating factual triples from Wikidata that are culturally
  diverse. A new benchmark DLAMA-v1 is built of factual triples from three
  pairs of contrasting cultures having a total of 78,259 triples from 20
  relation predicates. The three pairs comprise facts representing the (Arab
  and Western), (Asian and Western), and (South American and Western) countries
  respectively. Having a more balanced benchmark (DLAMA-v1) supports that mBERT
  performs better on Western facts than non-Western ones, while monolingual
  Arabic, English, and Korean models tend to perform better on their culturally
  proximate facts. Moreover, both monolingual and multilingual models tend to
  make a prediction that is culturally or geographically relevant to the
  correct label, even if the prediction is wrong.</td>
  <td colspan=2 style='mso-ignore:colspan'>8 June, 2023</td>
 </tr>
 <tr height=385 style='height:289.0pt'>
  <td height=385 align=right style='height:289.0pt'>4313</td>
  <td align=right>2306.05122</td>
  <td class=xl65 width=649 style='width:487pt'>Can AI Moderate Online
  Communities?</td>
  <td class=xl65 width=649 style='width:487pt'>The task of cultivating healthy
  communication in online communities becomes increasingly urgent, as gaming
  and social media experiences become progressively more immersive and
  life-like. We approach the challenge of moderating online communities by
  training student models using a large language model (LLM). We use zero-shot
  learning models to distill and expand datasets followed by a few-shot
  learning and a fine-tuning approach, leveraging open-access generative
  pre-trained transformer models (GPT) from OpenAI. Our preliminary findings
  suggest, that when properly trained, LLMs can excel in identifying actor
  intentions, moderating toxic comments, and rewarding positive contributions.
  The student models perform above-expectation in non-contextual assignments
  such as identifying classically toxic behavior and perform sufficiently on
  contextual assignments such as identifying positive contributions to online
  discourse. Further, using open-access models like OpenAI's GPT we experience
  a step-change in the development process for what has historically been a
  complex modeling task. We contribute to the information system (IS) discourse
  with a rapid development framework on the application of generative AI in
  content online moderation and management of culture in decentralized,
  pseudonymous communities by providing a sample model suite of
  industrial-ready generative AI models based on open-access LLMs.</td>
  <td colspan=2 style='mso-ignore:colspan'>8 June, 2023</td>
 </tr>
 <tr height=496 style='height:372.0pt'>
  <td height=496 align=right style='height:372.0pt'>4306</td>
  <td align=right>2306.05817</td>
  <td class=xl65 width=649 style='width:487pt'>How Can Recommender Systems
  Benefit from Large Language Models: A Survey</td>
  <td class=xl65 width=649 style='width:487pt'>With the rapid development of
  online services, recommender systems (RS) have become increasingly
  indispensable for mitigating information overload. Despite remarkable
  progress, conventional recommendation models (CRM) still have some
  limitations, e.g., lacking open-world knowledge, and difficulties in
  comprehending users' underlying preferences and motivations. Meanwhile, large
  language models (LLM) have shown impressive general intelligence and
  human-like capabilities, which mainly stem from their extensive open-world
  knowledge, reasoning ability, as well as their comprehension of human culture
  and society. Consequently, the emergence of LLM is inspiring the design of
  recommender systems and pointing out a promising research direction, i.e.,
  whether we can incorporate LLM and benefit from their knowledge and
  capabilities to compensate for the limitations of CRM. In this paper, we
  conduct a comprehensive survey on this research direction from the
  perspective of the whole pipeline in real-world recommender systems.
  Specifically, we summarize existing works from two orthogonal aspects: where
  and how to adapt LLM to RS. For the WHERE question, we discuss the roles that
  LLM could play in different stages of the recommendation pipeline, i.e.,
  feature engineering, feature encoder, scoring/ranking function, user
  interaction, and pipeline controller. For the HOW question, we investigate
  the training and inference strategies, resulting in two fine-grained taxonomy
  criteria, i.e., whether to tune LLM or not, and whether to involve
  conventional recommendation models for inference. Then, we highlight key
  challenges in adapting LLM to RS from three aspects, i.e., efficiency,
  effectiveness, and ethics. Finally, we summarize the survey and discuss the
  future prospects. We actively maintain a GitHub repository for papers and
  other related resources: https://github.com/CHIANGEL/Awesome-LLM-for-RecSys/.</td>
  <td>9 July, 2024</td>
  <td></td>
 </tr>
 <tr height=385 style='height:289.0pt'>
  <td height=385 align=right style='height:289.0pt'>4238</td>
  <td align=right>2306.16244</td>
  <td class=xl65 width=649 style='width:487pt'>CBBQ: A Chinese Bias Benchmark
  Dataset Curated with Human-AI Collaboration for Large Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>Holistically measuring societal
  biases of large language models is crucial for detecting and reducing ethical
  risks in highly capable AI models. In this work, we present a Chinese Bias
  Benchmark dataset that consists of over 100K questions jointly constructed by
  human experts and generative language models, covering stereotypes and
  societal biases in 14 social dimensions related to Chinese culture and
  values. The curation process contains 4 essential steps: bias identification
  via extensive literature review, ambiguous context generation, AI-assisted
  disambiguous context generation, snd manual review \&amp; recomposition. The
  testing instances in the dataset are automatically derived from 3K+
  high-quality templates manually authored with stringent quality control. The
  dataset exhibits wide coverage and high diversity. Extensive experiments
  demonstrate the effectiveness of the dataset in detecting model bias, with
  all 10 publicly available Chinese large language models exhibiting strong
  bias in certain categories. Additionally, we observe from our experiments
  that fine-tuned models could, to a certain extent, heed instructions and
  avoid generating outputs that are morally harmful in some types, in the way
  of &quot;moral self-correction&quot;. Our dataset and results are publicly
  available at
  \href{https://github.com/YFHuangxxxx/CBBQ}{https://github.com/YFHuangxxxx/CBBQ},
  offering debiasing research opportunities to a widened community.</td>
  <td colspan=2 style='mso-ignore:colspan'>28 June, 2023</td>
 </tr>
 <tr height=204 style='height:153.0pt'>
  <td height=204 align=right style='height:153.0pt'>4223</td>
  <td align=right>2307.0137</td>
  <td class=xl65 width=649 style='width:487pt'>Multilingual Language Models are
  not Multicultural: A Case Study in Emotion</td>
  <td class=xl65 width=649 style='width:487pt'>Emotions are experienced and
  expressed differently across the world. In order to use Large Language Models
  (LMs) for multilingual tasks that require emotional sensitivity, LMs must
  reflect this cultural variation in emotion. In this study, we investigate
  whether the widely-used multilingual LMs in 2023 reflect differences in
  emotional expressions across cultures and languages. We find that embeddings
  obtained from LMs (e.g., XLM-RoBERTa) are Anglocentric, and generative LMs
  (e.g., ChatGPT) reflect Western norms, even when responding to prompts in
  other languages. Our results show that multilingual LMs do not successfully
  learn the culturally appropriate nuances of emotion and we highlight possible
  research directions towards correcting this.</td>
  <td>9 July, 2023</td>
  <td></td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>4217</td>
  <td align=right>2307.02971</td>
  <td class=xl65 width=649 style='width:487pt'>On the Cultural Gap in
  Text-to-Image Generation</td>
  <td class=xl65 width=649 style='width:487pt'>One challenge in text-to-image
  (T2I) generation is the inadvertent reflection of culture gaps present in the
  training data, which signifies the disparity in generated image quality when
  the cultural elements of the input text are rarely collected in the training
  set. Although various T2I models have shown impressive but arbitrary
  examples, there is no benchmark to systematically evaluate a T2I model's
  ability to generate cross-cultural images. To bridge the gap, we propose a
  Challenging Cross-Cultural (C3) benchmark with comprehensive evaluation
  criteria, which can assess how well-suited a model is to a target culture. By
  analyzing the flawed images generated by the Stable Diffusion model on the C3
  benchmark, we find that the model often fails to generate certain cultural
  objects. Accordingly, we propose a novel multi-modal metric that considers
  object-text alignment to filter the fine-tuning data in the target culture,
  which is used to fine-tune a T2I model to improve cross-cultural generation.
  Experimental results show that our multi-modal metric provides stronger data
  selection performance on the C3 benchmark than existing metrics, in which the
  object-text alignment is crucial. We release the benchmark, data, code, and
  generated images to facilitate future research on culturally diverse T2I
  generation (https://github.com/longyuewangdcu/C3-Bench).</td>
  <td>6 July, 2023</td>
  <td></td>
 </tr>
 <tr height=431 style='height:323.0pt'>
  <td height=431 align=right style='height:323.0pt'>4199</td>
  <td align=right>2307.05354</td>
  <td class=xl65 width=649 style='width:487pt'>GujiBERT and GujiGPT:
  Construction of Intelligent Information Processing Foundation Language Models
  for Ancient Texts</td>
  <td class=xl65 width=649 style='width:487pt'>In the context of the rapid
  development of large language models, we have meticulously trained and
  introduced the GujiBERT and GujiGPT language models, which are foundational
  models specifically designed for intelligent information processing of
  ancient texts. These models have been trained on an extensive dataset that
  encompasses both simplified and traditional Chinese characters, allowing them
  to effectively handle various natural language processing tasks related to
  ancient books, including but not limited to automatic sentence segmentation,
  punctuation, word segmentation, part-of-speech tagging, entity recognition,
  and automatic translation. Notably, these models have exhibited exceptional
  performance across a range of validation tasks using publicly available
  datasets. Our research findings highlight the efficacy of employing
  self-supervised methods to further train the models using classical text
  corpora, thus enhancing their capability to tackle downstream tasks.
  Moreover, it is worth emphasizing that the choice of font, the scale of the
  corpus, and the initial model selection all exert significant influence over
  the ultimate experimental outcomes. To cater to the diverse text processing
  preferences of researchers in digital humanities and linguistics, we have
  developed three distinct categories comprising a total of nine model
  variations. We believe that by sharing these foundational language models
  specialized in the domain of ancient texts, we can facilitate the intelligent
  processing and scholarly exploration of ancient literary works and,
  consequently, contribute to the global dissemination of China's rich and
  esteemed traditional culture in this new era.</td>
  <td colspan=2 style='mso-ignore:colspan'>11 July, 2023</td>
 </tr>
 <tr height=385 style='height:289.0pt'>
  <td height=385 align=right style='height:289.0pt'>4192</td>
  <td align=right>2307.06218</td>
  <td class=xl65 width=649 style='width:487pt'>Ashaar: Automatic Analysis and
  Generation of Arabic Poetry Using Deep Learning Approaches</td>
  <td class=xl65 width=649 style='width:487pt'>Poetry holds immense
  significance within the cultural and traditional fabric of any nation. It
  serves as a vehicle for poets to articulate their emotions, preserve customs,
  and convey the essence of their culture. Arabic poetry is no exception,
  having played a cherished role in the heritage of the Arabic community
  throughout history and maintaining its relevance in the present era.
  Typically, comprehending Arabic poetry necessitates the expertise of a
  linguist who can analyze its content and assess its quality. This paper
  presents the introduction of a framework called \textit{Ashaar}
  https://github.com/ARBML/Ashaar, which encompasses a collection of datasets
  and pre-trained models designed specifically for the analysis and generation
  of Arabic poetry. The pipeline established within our proposed approach
  encompasses various aspects of poetry, such as meter, theme, and era
  classification. It also incorporates automatic poetry diacritization,
  enabling more intricate analyses like automated extraction of the
  \textit{Arudi} style. Additionally, we explore the feasibility of generating
  conditional poetry through the pre-training of a character-based GPT model.
  Furthermore, as part of this endeavor, we provide four datasets: one for
  poetry generation, another for diacritization, and two for Arudi-style
  prediction. These datasets aim to facilitate research and development in the
  field of Arabic poetry by enabling researchers and enthusiasts to delve into
  the nuances of this rich literary tradition.</td>
  <td colspan=2 style='mso-ignore:colspan'>12 July, 2023</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>4191</td>
  <td align=right>2307.06518</td>
  <td class=xl65 width=649 style='width:487pt'>Machine Learning practices and
  infrastructures</td>
  <td class=xl65 width=649 style='width:487pt'>Machine Learning (ML) systems,
  particularly when deployed in high-stakes domains, are deeply consequential.
  They can exacerbate existing inequities, create new modes of discrimination,
  and reify outdated social constructs. Accordingly, the social context (i.e.
  organisations, teams, cultures) in which ML systems are developed is a site
  of active research for the field of AI ethics, and intervention for
  policymakers. This paper focuses on one aspect of social context that is
  often overlooked: interactions between practitioners and the tools they rely
  on, and the role these interactions play in shaping ML practices and the
  development of ML systems. In particular, through an empirical study of
  questions asked on the Stack Exchange forums, the use of interactive
  computing platforms (e.g. Jupyter Notebook and Google Colab) in ML practices
  is explored. I find that interactive computing platforms are used in a host
  of learning and coordination practices, which constitutes an infrastructural
  relationship between interactive computing platforms and ML practitioners. I
  describe how ML practices are co-evolving alongside the development of
  interactive computing platforms, and highlight how this risks making
  invisible aspects of the ML life cycle that AI ethics researchers' have
  demonstrated to be particularly salient for the societal impact of deployed
  ML systems.</td>
  <td colspan=2 style='mso-ignore:colspan'>25 July, 2023</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>4185</td>
  <td align=right>2307.07521</td>
  <td class=xl65 width=649 style='width:487pt'>Artistic Strategies to Guide
  Neural Networks</td>
  <td class=xl65 width=649 style='width:487pt'>Artificial Intelligence is
  present in the generation and distribution of culture. How do artists exploit
  neural networks? What impact do these algorithms have on artistic practice?
  Through a practice-based research methodology, this paper explores the potentials
  and limits of current AI technology, more precisely deep neural networks, in
  the context of image, text, form and translation of semiotic spaces. In a
  relatively short time, the generation of high-resolution images and 3D
  objects has been achieved. There are models, like CLIP and text2mesh, that do
  not need the same kind of media input as the output; we call them translation
  models. Such a twist contributes toward creativity arousal, which manifests
  itself in art practice and feeds back to the developers' pipeline. Yet again,
  we see how artworks act as catalysts for technology development. Those
  creative scenarios and processes are enabled not solely by AI models, but by
  the hard work behind implementing these new technologies. AI does not create
  a 'push-a-button' masterpiece but requires a deep understanding of the
  technology behind it, and a creative and critical mindset. Thus, AI opens new
  avenues for inspiration and offers novel tool sets, and yet again the
  question of authorship is asked.</td>
  <td>6 July, 2023</td>
  <td></td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>4182</td>
  <td align=right>2307.07871</td>
  <td class=xl65 width=649 style='width:487pt'>The SocialAI School: Insights
  from Developmental Psychology Towards Artificial Socio-Cultural Agents</td>
  <td class=xl65 width=649 style='width:487pt'>Developmental psychologists have
  long-established the importance of socio-cognitive abilities in human
  intelligence. These abilities enable us to enter, participate and benefit
  from human culture. AI research on social interactive agents mostly concerns the
  emergence of culture in a multi-agent setting (often without a strong
  grounding in developmental psychology). We argue that AI research should be
  informed by psychology and study socio-cognitive abilities enabling to enter
  a culture too. We discuss the theories of Michael Tomasello and Jerome Bruner
  to introduce some of their concepts to AI and outline key concepts and
  socio-cognitive abilities. We present The SocialAI school - a tool including
  a customizable parameterized uite of procedurally generated environments,
  which simplifies conducting experiments regarding those concepts. We show
  examples of such experiments with RL agents and Large Language Models. The
  main motivation of this work is to engage the AI community around the problem
  of social intelligence informed by developmental psychology, and to provide a
  tool to simplify first steps in this direction. Refer to the project website
  for code and additional information:
  https://sites.google.com/view/socialai-school.</td>
  <td colspan=2 style='mso-ignore:colspan'>23 November, 2023</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>4159</td>
  <td align=right>2307.10514</td>
  <td class=xl65 width=649 style='width:487pt'>Building Socio-culturally
  Inclusive Stereotype Resources with Community Engagement</td>
  <td class=xl65 width=649 style='width:487pt'>With rapid development and
  deployment of generative language models in global settings, there is an
  urgent need to also scale our measurements of harm, not just in the number
  and types of harms covered, but also how well they account for local cultural
  contexts, including marginalized identities and the social biases experienced
  by them. Current evaluation paradigms are limited in their abilities to
  address this, as they are not representative of diverse, locally situated but
  global, socio-cultural perspectives. It is imperative that our evaluation
  resources are enhanced and calibrated by including people and experiences
  from different cultures and societies worldwide, in order to prevent gross
  underestimations or skews in measurements of harm. In this work, we
  demonstrate a socio-culturally aware expansion of evaluation resources in the
  Indian societal context, specifically for the harm of stereotyping. We devise
  a community engaged effort to build a resource which contains stereotypes for
  axes of disparity that are uniquely present in India. The resultant resource
  increases the number of stereotypes known for and in the Indian context by
  over 1000 stereotypes across many unique identities. We also demonstrate the
  utility and effectiveness of such expanded resources for evaluations of
  language models. CONTENT WARNING: This paper contains examples of stereotypes
  that may be offensive.</td>
  <td colspan=2 style='mso-ignore:colspan'>19 July, 2023</td>
 </tr>
 <tr height=408 style='height:306.0pt'>
  <td height=408 align=right style='height:306.0pt'>4141</td>
  <td align=right>2307.13714</td>
  <td class=xl65 width=649 style='width:487pt'>Diversity and Language
  Technology: How Techno-Linguistic Bias Can Cause Epistemic Injustice</td>
  <td class=xl65 width=649 style='width:487pt'>It is well known that AI-based
  language technology -- large language models, machine translation systems,
  multilingual dictionaries, and corpora -- is currently limited to 2 to 3
  percent of the world's most widely spoken and/or financially and politically
  best supported languages. In response, recent research efforts have sought to
  extend the reach of AI technology to ``underserved languages.'' In this
  paper, we show that many of these attempts produce flawed solutions that
  adhere to a hard-wired representational preference for certain languages,
  which we call techno-linguistic bias. Techno-linguistic bias is distinct from
  the well-established phenomenon of linguistic bias as it does not concern the
  languages represented but rather the design of the technologies. As we show
  through the paper, techno-linguistic bias can result in systems that can only
  express concepts that are part of the language and culture of dominant
  powers, unable to correctly represent concepts from other communities. We
  argue that at the root of this problem lies a systematic tendency of
  technology developer communities to apply a simplistic understanding of
  diversity which does not do justice to the more profound differences that
  languages, and ultimately the communities that speak them, embody. Drawing on
  the concept of epistemic injustice, we point to the broader sociopolitical
  consequences of the bias we identify and show how it can lead not only to a
  disregard for valuable aspects of diversity but also to an under-representation
  of the needs and diverse worldviews of marginalized language communities.</td>
  <td colspan=2 style='mso-ignore:colspan'>25 July, 2023</td>
 </tr>
 <tr height=227 style='height:170.0pt'>
  <td height=227 align=right style='height:170.0pt'>4139</td>
  <td align=right>2307.13882</td>
  <td class=xl65 width=649 style='width:487pt'>Human Culture: A History
  Irrelevant and Predictable Experience</td>
  <td class=xl65 width=649 style='width:487pt'>Human culture research has
  witnessed an opportunity of revolution thanks to the big data and social
  network revolution. Websites such as Douban.com, Goodreads.com, Pandora and
  IMDB become the new gold mine for cultural researchers. In 2021 and 2022, the
  author of this paper invented 2 data-free recommender systems for AI
  cold-start problem. The algorithms can recommend cultural and commercial
  products to users without reference to users' past preferences. The social
  implications of the new inventions are human cultural tastes can be predicted
  very precisely without any information related to human individuals. In this
  paper, we analyze the AI technologies and its cultural implications together
  with other AI algorithms. We show that human culture is (mostly) a history
  irrelevant and predictable experience.</td>
  <td colspan=2 style='mso-ignore:colspan'>25 July, 2023</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>4125</td>
  <td align=right>2307.16778</td>
  <td class=xl65 width=649 style='width:487pt'>KoBBQ: Korean Bias Benchmark for
  Question Answering</td>
  <td class=xl65 width=649 style='width:487pt'>The Bias Benchmark for Question
  Answering (BBQ) is designed to evaluate social biases of language models
  (LMs), but it is not simple to adapt this benchmark to cultural contexts
  other than the US because social biases depend heavily on the cultural context.
  In this paper, we present KoBBQ, a Korean bias benchmark dataset, and we
  propose a general framework that addresses considerations for cultural
  adaptation of a dataset. Our framework includes partitioning the BBQ dataset
  into three classes--Simply-Transferred (can be used directly after cultural
  translation), Target-Modified (requires localization in target groups), and
  Sample-Removed (does not fit Korean culture)-- and adding four new categories
  of bias specific to Korean culture. We conduct a large-scale survey to
  collect and validate the social biases and the targets of the biases that
  reflect the stereotypes in Korean culture. The resulting KoBBQ dataset
  comprises 268 templates and 76,048 samples across 12 categories of social
  bias. We use KoBBQ to measure the accuracy and bias scores of several
  state-of-the-art multilingual LMs. The results clearly show differences in
  the bias of LMs as measured by KoBBQ and a machine-translated version of BBQ,
  demonstrating the need for and utility of a well-constructed,
  culturally-aware social bias benchmark.</td>
  <td colspan=2 style='mso-ignore:colspan'>25 January, 2024</td>
 </tr>
 <tr height=475 style='height:356.0pt'>
  <td height=475 align=right style='height:356.0pt'>4102</td>
  <td align=right>2308.03151</td>
  <td class=xl65 width=649 style='width:487pt'>Food-500 Cap: A Fine-Grained
  Food Caption Benchmark for Evaluating Vision-Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>Vision-language models (VLMs)
  have shown impressive performance in substantial downstream multi-modal
  tasks. However, only comparing the fine-tuned performance on downstream tasks
  leads to the poor interpretability of VLMs, which is adverse to their future
  improvement. Several prior works have identified this issue and used various
  probing methods under a zero-shot setting to detect VLMs' limitations, but
  they all examine VLMs using general datasets instead of specialized ones. In
  practical applications, VLMs are usually applied to specific scenarios, such
  as e-commerce and news fields, so the generalization of VLMs in specific
  domains should be given more attention. In this paper, we comprehensively
  investigate the capabilities of popular VLMs in a specific field, the food
  domain. To this end, we build a food caption dataset, Food-500 Cap, which
  contains 24,700 food images with 494 categories. Each image is accompanied by
  a detailed caption, including fine-grained attributes of food, such as the
  ingredient, shape, and color. We also provide a culinary culture taxonomy
  that classifies each food category based on its geographic origin in order to
  better analyze the performance differences of VLM in different regions.
  Experiments on our proposed datasets demonstrate that popular VLMs
  underperform in the food domain compared with their performance in the
  general domain. Furthermore, our research reveals severe bias in VLMs'
  ability to handle food items from different geographic regions. We adopt
  diverse probing methods and evaluate nine VLMs belonging to different
  architectures to verify the aforementioned observations. We hope that our
  study will bring researchers' attention to VLM's limitations when applying
  them to the domain of food or culinary cultures, and spur further
  investigations to address this issue.</td>
  <td colspan=2 style='mso-ignore:colspan'>6 August, 2023</td>
 </tr>
 <tr height=385 style='height:289.0pt'>
  <td height=385 align=right style='height:289.0pt'>4049</td>
  <td align=right>2308.11107</td>
  <td class=xl65 width=649 style='width:487pt'>Classification of the lunar
  surface pattern by AI architectures: Does AI see a rabbit in the Moon?</td>
  <td class=xl65 width=649 style='width:487pt'>In Asian countries, there is a
  tradition that a rabbit, known as the Moon rabbit, lives on the Moon.
  Typically, two reasons are mentioned for the origin of this tradition. The
  first reason is that the color pattern of the lunar surface resembles the
  shape of a rabbit. The second reason is that both the Moon and rabbits are
  symbols of fertility, as the Moon appears and disappears (i.e., waxing and
  waning) cyclically and rabbits are known for their high fertility.
  Considering the latter reason, is the color pattern of the lunar surface not
  similar to a rabbit? Here, the similarity between rabbit and the lunar
  surface pattern was evaluated using seven AI architectures. In the test
  conducted with Contrastive Language-Image Pre-Training (CLIP), which can classify
  images based on given words, it was assumed that people frequently observe
  the Moon in the early evening. Under this condition, the lunar surface
  pattern was found to be more similar to a rabbit than a face in low-latitude
  regions, while it could also be classified as a face as the latitude
  increases. This result is consistent with that the oldest literatures about
  the Moon rabbit were written in India and that a tradition of seeing a human
  face in the Moon exists in Europe. In a 1000-class test using seven AI
  architectures, ConvNeXt and CLIP sometimes classified the lunar surface
  pattern as a rabbit with relatively high probabilities. Cultures are
  generated by our attitude to the environment. Both dynamic and static
  similarities may be essential to induce our imagination.</td>
  <td colspan=2 style='mso-ignore:colspan'>9 December, 2024</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>4035</td>
  <td align=right>2308.13961</td>
  <td class=xl65 width=649 style='width:487pt'>Translate Meanings, Not Just
  Words: IdiomKB's Role in Optimizing Idiomatic Translation with Language
  Models</td>
  <td class=xl65 width=649 style='width:487pt'>To translate well, machine
  translation (MT) systems and general-purposed language models (LMs) need a
  deep understanding of both source and target languages and cultures.
  Therefore, idioms, with their non-compositional nature, pose particular
  challenges for Transformer-based systems, as literal translations often miss
  the intended meaning. Traditional methods, which replace idioms using
  existing knowledge bases (KBs), often lack scale and context awareness.
  Addressing these challenges, our approach prioritizes context awareness and
  scalability, allowing for offline storage of idioms in a manageable KB size.
  This ensures efficient serving with smaller models and provides a more
  comprehensive understanding of idiomatic expressions. We introduce a multilingual
  idiom KB (IdiomKB) developed using large LMs to address this. This KB
  facilitates better translation by smaller models, such as BLOOMZ (7.1B),
  Alpaca (7B), and InstructGPT (6.7B), by retrieving idioms' figurative
  meanings. We present a novel, GPT-4-powered metric for human-aligned
  evaluation, demonstrating that IdiomKB considerably boosts model performance.
  Human evaluations further validate our KB's quality.</td>
  <td colspan=2 style='mso-ignore:colspan'>24 December, 2023</td>
 </tr>
 <tr height=227 style='height:170.0pt'>
  <td height=227 align=right style='height:170.0pt'>4020</td>
  <td align=right>2308.15668</td>
  <td class=xl65 width=649 style='width:487pt'>Intersectional Inquiry, on the
  Ground and in the Algorithm</td>
  <td class=xl65 width=649 style='width:487pt'>This article makes two key
  contributions to methodological debates in automation research. First, we
  argue for and demonstrate how methods in this field must account for
  intersections of social difference, such as race, class, ethnicity, culture,
  and disability, in more nuanced ways. Second, we consider the complexities of
  bringing together computational and qualitative methods in an intersectional
  methodological approach while also arguing that in their respective subjects
  (machines and human subjects) and conceptual scope they enable a specific
  dialogue on intersectionality and automation to be articulated. We draw on
  field reflections from a project that combines an analysis of intersectional
  bias in language models with findings from a community workshop on the
  frustrations and aspirations produced through engagement with everyday
  AI-driven technologies in the context of care.</td>
  <td colspan=2 style='mso-ignore:colspan'>29 August, 2023</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>3995</td>
  <td align=right>2309.02136</td>
  <td class=xl65 width=649 style='width:487pt'>Exploring the Intersection of
  Complex Aesthetics and Generative AI for Promoting Cultural Creativity in
  Rural China after the Post-Pandemic Era</td>
  <td class=xl65 width=649 style='width:487pt'>This paper explores using
  generative AI and aesthetics to promote cultural creativity in rural China
  amidst COVID-19's impact. Through literature reviews, case studies, surveys,
  and text analysis, it examines art and technology applications in rural contexts
  and identifies key challenges. The study finds artworks often fail to
  resonate locally, while reliance on external artists limits sustainability.
  Hence, nurturing grassroots &quot;artist villagers&quot; through AI is
  proposed. Our approach involves training machine learning on subjective
  aesthetics to generate culturally relevant content. Interactive AI media can
  also boost tourism while preserving heritage. This pioneering research puts
  forth original perspectives on the intersection of AI and aesthetics to
  invigorate rural culture. It advocates holistic integration of technology and
  emphasizes AI's potential as a creative enabler versus replacement.
  Ultimately, it lays the groundwork for further exploration of leveraging AI
  innovations to empower rural communities. This timely study contributes to
  growing interest in emerging technologies to address critical issues facing
  rural China.</td>
  <td colspan=2 style='mso-ignore:colspan'>5 September, 2023</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>3989</td>
  <td align=right>2309.03126</td>
  <td class=xl65 width=649 style='width:487pt'>Everyone Deserves A Reward:
  Learning Customized Human Preferences</td>
  <td class=xl65 width=649 style='width:487pt'>Reward models (RMs) are
  essential for aligning large language models (LLMs) with human preferences to
  improve interaction quality. However, the real world is pluralistic, which
  leads to diversified human preferences with respect to different religions, politics,
  cultures, etc. Moreover, each individual can have their unique preferences on
  various topics. Neglecting the diversity of human preferences, current human
  feedback aligning methods only consider a general reward model, which is
  below satisfaction for customized or personalized application scenarios. To
  explore customized preference learning, we collect a domain-specific
  preference (DSP) dataset, which includes preferred responses for each given
  query from four practical domains. Besides, from the perspective of data
  efficiency, we propose a three-stage customized RM learning scheme, then
  empirically verify its effectiveness on both general preference datasets and
  our DSP set. Furthermore, we test multiple training and data strategies on
  the three learning stages. We find several ways to better preserve the
  general preferring ability while training the customized RMs, especially
  general preference enrichment, and customized preference imitation learning.
  The DSP dataset and code are available at https://github.com/Linear95/DSP.</td>
  <td colspan=2 style='mso-ignore:colspan'>15 September, 2023</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>3943</td>
  <td align=right>2309.08591</td>
  <td class=xl65 width=649 style='width:487pt'>Are Multilingual LLMs
  Culturally-Diverse Reasoners? An Investigation into Multicultural Proverbs
  and Sayings</td>
  <td class=xl65 width=649 style='width:487pt'>Large language models (LLMs) are
  highly adept at question answering and reasoning tasks, but when reasoning in
  a situational context, human expectations vary depending on the relevant
  cultural common ground. As languages are associated with diverse cultures,
  LLMs should also be culturally-diverse reasoners. In this paper, we study the
  ability of a wide range of state-of-the-art multilingual LLMs (mLLMs) to
  reason with proverbs and sayings in a conversational context. Our experiments
  reveal that: (1) mLLMs &quot;know&quot; limited proverbs and memorizing
  proverbs does not mean understanding them within a conversational context;
  (2) mLLMs struggle to reason with figurative proverbs and sayings, and when
  asked to select the wrong answer (instead of asking it to select the correct
  answer); and (3) there is a &quot;culture gap&quot; in mLLMs when reasoning
  about proverbs and sayings translated from other languages. We construct and
  release our evaluation dataset MAPS (MulticultrAl Proverbs and Sayings) for
  proverb understanding with conversational context for six different
  languages.</td>
  <td colspan=2 style='mso-ignore:colspan'>30 March, 2024</td>
 </tr>
 <tr height=249 style='height:187.0pt'>
  <td height=249 align=right style='height:187.0pt'>3937</td>
  <td align=right>2309.0912</td>
  <td class=xl65 width=649 style='width:487pt'>Public Perceptions of Gender
  Bias in Large Language Models: Cases of ChatGPT and Ernie</td>
  <td class=xl65 width=649 style='width:487pt'>Large language models are
  quickly gaining momentum, yet are found to demonstrate gender bias in their
  responses. In this paper, we conducted a content analysis of social media
  discussions to gauge public perceptions of gender bias in LLMs which are
  trained in different cultural contexts, i.e., ChatGPT, a US-based LLM, or
  Ernie, a China-based LLM. People shared both observations of gender bias in
  their personal use and scientific findings about gender bias in LLMs. A
  difference between the two LLMs was seen -- ChatGPT was more often found to
  carry implicit gender bias, e.g., associating men and women with different
  profession titles, while explicit gender bias was found in Ernie's responses,
  e.g., overly promoting women's pursuit of marriage over career. Based on the
  findings, we reflect on the impact of culture on gender bias and propose
  governance recommendations to regulate gender bias in LLMs.</td>
  <td colspan=2 style='mso-ignore:colspan'>16 September, 2023</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>3913</td>
  <td align=right>2309.12053</td>
  <td class=xl65 width=649 style='width:487pt'>AceGPT, Localizing Large
  Language Models in Arabic</td>
  <td class=xl65 width=649 style='width:487pt'>This paper is devoted to the
  development of a localized Large Language Model (LLM) specifically for
  Arabic, a language imbued with unique cultural characteristics inadequately
  addressed by current mainstream models. Significant concerns emerge when
  addressing cultural sensitivity and local values. To address this, the paper
  proposes a comprehensive solution that includes further pre-training with
  Arabic texts, Supervised Fine-Tuning (SFT) utilizing native Arabic
  instructions, and GPT-4 responses in Arabic, alongside Reinforcement Learning
  with AI Feedback (RLAIF) employing a reward model attuned to local culture
  and values. The goal is to cultivate culturally cognizant and value-aligned
  Arabic LLMs capable of accommodating the diverse, application-specific needs
  of Arabic-speaking communities. Comprehensive evaluations reveal that the
  resulting model, dubbed `AceGPT', sets the state-of-the-art standard for open
  Arabic LLMs across various benchmarks. Codes, data, and models are in
  https://github.com/FreedomIntelligence/AceGPT.</td>
  <td colspan=2 style='mso-ignore:colspan'>2 April, 2024</td>
 </tr>
 <tr height=408 style='height:306.0pt'>
  <td height=408 align=right style='height:306.0pt'>3903</td>
  <td align=right>2309.12342</td>
  <td class=xl65 width=649 style='width:487pt'>Cultural Alignment in Large
  Language Models: An Explanatory Analysis Based on Hofstede's Cultural
  Dimensions</td>
  <td class=xl65 width=649 style='width:487pt'>The deployment of large language
  models (LLMs) raises concerns regarding their cultural misalignment and
  potential ramifications on individuals and societies with diverse cultural
  backgrounds. While the discourse has focused mainly on political and social
  biases, our research proposes a Cultural Alignment Test (Hoftede's CAT) to
  quantify cultural alignment using Hofstede's cultural dimension framework,
  which offers an explanatory cross-cultural comparison through the latent
  variable analysis. We apply our approach to quantitatively evaluate LLMs,
  namely Llama 2, GPT-3.5, and GPT-4, against the cultural dimensions of
  regions like the United States, China, and Arab countries, using different
  prompting styles and exploring the effects of language-specific fine-tuning
  on the models' behavioural tendencies and cultural values. Our results
  quantify the cultural alignment of LLMs and reveal the difference between
  LLMs in explanatory cultural dimensions. Our study demonstrates that while
  all LLMs struggle to grasp cultural values, GPT-4 shows a unique capability
  to adapt to cultural nuances, particularly in Chinese settings. However, it
  faces challenges with American and Arab cultures. The research also
  highlights that fine-tuning LLama 2 models with different languages changes
  their responses to cultural questions, emphasizing the need for culturally
  diverse development in AI for worldwide acceptance and ethical use. For more
  details or to contribute to this research, visit our GitHub page
  https://github.com/reemim/Hofstedes_CAT/</td>
  <td colspan=2 style='mso-ignore:colspan'>8 May, 2024</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>3891</td>
  <td align=right>2309.14228</td>
  <td class=xl65 width=649 style='width:487pt'>ID.8: Co-Creating Visual Stories
  with Generative AI</td>
  <td class=xl65 width=649 style='width:487pt'>Storytelling is an integral part
  of human culture and significantly impacts cognitive and socio-emotional
  development and connection. Despite the importance of interactive visual
  storytelling, the process of creating such content requires specialized skills
  and is labor-intensive. This paper introduces ID.8, an open-source system
  designed for the co-creation of visual stories with generative AI. We focus
  on enabling an inclusive storytelling experience by simplifying the content
  creation process and allowing for customization. Our user evaluation confirms
  a generally positive user experience in domains such as enjoyment and
  exploration, while highlighting areas for improvement, particularly in
  immersiveness, alignment, and partnership between the user and the AI system.
  Overall, our findings indicate promising possibilities for empowering people
  to create visual stories with generative AI. This work contributes a novel
  content authoring system, ID.8, and insights into the challenges and
  potential of using generative AI for multimedia content creation.</td>
  <td colspan=2 style='mso-ignore:colspan'>3 June, 2024</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>3841</td>
  <td align=right>2310.01929</td>
  <td class=xl65 width=649 style='width:487pt'>Navigating Cultural Chasms:
  Exploring and Unlocking the Cultural POV of Text-To-Image Models</td>
  <td class=xl65 width=649 style='width:487pt'>Text-To-Image (TTI) models, such
  as DALL-E and StableDiffusion, have demonstrated remarkable prompt-based
  image generation capabilities. Multilingual encoders may have a substantial
  impact on the cultural agency of these models, as language is a conduit of
  culture. In this study, we explore the cultural perception embedded in TTI
  models by characterizing culture across three hierarchical tiers: cultural
  dimensions, cultural domains, and cultural concepts. Based on this ontology,
  we derive prompt templates to unlock the cultural knowledge in TTI models,
  and propose a comprehensive suite of evaluation techniques, including
  intrinsic evaluations using the CLIP space, extrinsic evaluations with a
  Visual-Question-Answer (VQA) model and human assessments, to evaluate the
  cultural content of TTI-generated images. To bolster our research, we
  introduce the CulText2I dataset, derived from six diverse TTI models and
  spanning ten languages. Our experiments provide insights regarding Do, What,
  Which and How research questions about the nature of cultural encoding in TTI
  models, paving the way for cross-cultural applications of these models.</td>
  <td colspan=2 style='mso-ignore:colspan'>13 August, 2024</td>
 </tr>
 <tr height=227 style='height:170.0pt'>
  <td height=227 align=right style='height:170.0pt'>3837</td>
  <td align=right>2310.02457</td>
  <td class=xl65 width=649 style='width:487pt'>The Empty Signifier Problem:
  Towards Clearer Paradigms for Operationalising &quot;Alignment&quot; in Large
  Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>In this paper, we address the
  concept of &quot;alignment&quot; in large language models (LLMs) through the
  lens of post-structuralist socio-political theory, specifically examining its
  parallels to empty signifiers. To establish a shared vocabulary around how
  abstract concepts of alignment are operationalised in empirical datasets, we
  propose a framework that demarcates: 1) which dimensions of model behaviour
  are considered important, then 2) how meanings and definitions are ascribed
  to these dimensions, and by whom. We situate existing empirical literature
  and provide guidance on deciding which paradigm to follow. Through this
  framework, we aim to foster a culture of transparency and critical
  evaluation, aiding the community in navigating the complexities of aligning
  LLMs with human populations.</td>
  <td colspan=2 style='mso-ignore:colspan'>15 November, 2023</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>3831</td>
  <td align=right>2310.03368</td>
  <td class=xl65 width=649 style='width:487pt'>Evaluating Hallucinations in
  Chinese Large Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>In this paper, we establish a
  benchmark named HalluQA (Chinese Hallucination Question-Answering) to measure
  the hallucination phenomenon in Chinese large language models. HalluQA
  contains 450 meticulously designed adversarial questions, spanning multiple
  domains, and takes into account Chinese historical culture, customs, and
  social phenomena. During the construction of HalluQA, we consider two types
  of hallucinations: imitative falsehoods and factual errors, and we construct
  adversarial samples based on GLM-130B and ChatGPT. For evaluation, we design
  an automated evaluation method using GPT-4 to judge whether a model output is
  hallucinated. We conduct extensive experiments on 24 large language models,
  including ERNIE-Bot, Baichuan2, ChatGLM, Qwen, SparkDesk and etc. Out of the
  24 models, 18 achieved non-hallucination rates lower than 50%. This indicates
  that HalluQA is highly challenging. We analyze the primary types of
  hallucinations in different types of models and their causes. Additionally,
  we discuss which types of hallucinations should be prioritized for different
  types of models.</td>
  <td colspan=2 style='mso-ignore:colspan'>25 October, 2023</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>3816</td>
  <td align=right>2310.04928</td>
  <td class=xl65 width=649 style='width:487pt'>Large Language Models Only Pass
  Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU</td>
  <td class=xl65 width=649 style='width:487pt'>Although large language models
  (LLMs) are often pre-trained on large-scale multilingual texts, their
  reasoning abilities and real-world knowledge are mainly evaluated based on
  English datasets. Assessing LLM capabilities beyond English is increasingly
  vital but hindered due to the lack of suitable datasets. In this work, we
  introduce IndoMMLU, the first multi-task language understanding benchmark for
  Indonesian culture and languages, which consists of questions from primary
  school to university entrance exams in Indonesia. By employing professional
  teachers, we obtain 14,981 questions across 64 tasks and education levels,
  with 46% of the questions focusing on assessing proficiency in the Indonesian
  language and knowledge of nine local languages and cultures in Indonesia. Our
  empirical evaluations show that GPT-3.5 only manages to pass the Indonesian
  primary school level, with limited knowledge of local Indonesian languages
  and culture. Other smaller models such as BLOOMZ and Falcon perform at even
  lower levels.</td>
  <td colspan=2 style='mso-ignore:colspan'>21 October, 2023</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>3744</td>
  <td align=right>2310.12481</td>
  <td class=xl65 width=649 style='width:487pt'>Not All Countries Celebrate
  Thanksgiving: On the Cultural Dominance in Large Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>This paper identifies a cultural
  dominance issue within large language models (LLMs) due to the predominant
  use of English data in model training (e.g., ChatGPT). LLMs often provide
  inappropriate English-culture-related answers that are not relevant to the
  expected culture when users ask in non-English languages. To systematically
  evaluate the cultural dominance issue, we build a benchmark of concrete
  (e.g., holidays and songs) and abstract (e.g., values and opinions) cultural
  objects. Empirical results show that the representative GPT models suffer
  from the culture dominance problem, where GPT-4 is the most affected while
  text-davinci-003 suffers the least from this problem. Our study emphasizes
  the need to critically examine cultural dominance and ethical consideration
  in their development and deployment. We show that two straightforward methods
  in model development (i.e., pretraining on more diverse data) and deployment
  (e.g., culture-aware prompting) can significantly mitigate the cultural
  dominance issue in LLMs.</td>
  <td colspan=2 style='mso-ignore:colspan'>16 February, 2024</td>
 </tr>
 <tr height=227 style='height:170.0pt'>
  <td height=227 align=right style='height:170.0pt'>3717</td>
  <td align=right>2310.14563</td>
  <td class=xl65 width=649 style='width:487pt'>NormDial: A Comparable Bilingual
  Synthetic Dialog Dataset for Modeling Social Norm Adherence and Violation</td>
  <td class=xl65 width=649 style='width:487pt'>Social norms fundamentally shape
  interpersonal communication. We present NormDial, a high-quality dyadic
  dialogue dataset with turn-by-turn annotations of social norm adherences and
  violations for Chinese and American cultures. Introducing the task of social
  norm observance detection, our dataset is synthetically generated in both
  Chinese and English using a human-in-the-loop pipeline by prompting large
  language models with a small collection of expert-annotated social norms. We
  show that our generated dialogues are of high quality through human
  evaluation and further evaluate the performance of existing large language
  models on this task. Our findings point towards new directions for
  understanding the nuances of social norms as they manifest in conversational
  contexts that span across languages and cultures.</td>
  <td colspan=2 style='mso-ignore:colspan'>24 October, 2023</td>
 </tr>
 <tr height=227 style='height:170.0pt'>
  <td height=227 align=right style='height:170.0pt'>3706</td>
  <td align=right>2310.15383</td>
  <td class=xl65 width=649 style='width:487pt'>GD-COMET: A Geo-Diverse
  Commonsense Inference Model</td>
  <td class=xl65 width=649 style='width:487pt'>With the increasing integration
  of AI into everyday life, it's becoming crucial to design AI systems that
  serve users from diverse backgrounds by making them culturally aware. In this
  paper, we present GD-COMET, a geo-diverse version of the COMET commonsense
  inference model. GD-COMET goes beyond Western commonsense knowledge and is
  capable of generating inferences pertaining to a broad range of cultures. We
  demonstrate the effectiveness of GD-COMET through a comprehensive human
  evaluation across 5 diverse cultures, as well as extrinsic evaluation on a
  geo-diverse task. The evaluation shows that GD-COMET captures and generates
  culturally nuanced commonsense knowledge, demonstrating its potential to
  benefit NLP applications across the board and contribute to making NLP more
  inclusive.</td>
  <td colspan=2 style='mso-ignore:colspan'>23 October, 2023</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>3693</td>
  <td align=right>2310.16523</td>
  <td class=xl65 width=649 style='width:487pt'>Improving Diversity of
  Demographic Representation in Large Language Models via Collective-Critiques
  and Self-Voting</td>
  <td class=xl65 width=649 style='width:487pt'>A crucial challenge for
  generative large language models (LLMs) is diversity: when a user's prompt is
  under-specified, models may follow implicit assumptions while generating a
  response, which may result in homogenization of the responses, as well as certain
  demographic groups being under-represented or even erased from the generated
  responses. In this paper, we formalize diversity of representation in
  generative LLMs. We present evaluation datasets and propose metrics to
  measure diversity in generated responses along people and culture axes. We
  find that LLMs understand the notion of diversity, and that they can reason
  and critique their own responses for that goal. This finding motivated a new
  prompting technique called collective-critique and self-voting (CCSV) to
  self-improve people diversity of LLMs by tapping into its diversity reasoning
  capabilities, without relying on handcrafted examples or prompt tuning.
  Extensive empirical experiments with both human and automated evaluations
  show that our proposed approach is effective at improving people and culture
  diversity, and outperforms all baseline methods by a large margin.</td>
  <td colspan=2 style='mso-ignore:colspan'>25 October, 2023</td>
 </tr>
 <tr height=385 style='height:289.0pt'>
  <td height=385 align=right style='height:289.0pt'>3685</td>
  <td align=right>2310.17353</td>
  <td class=xl65 width=649 style='width:487pt'>Cultural Adaptation of Recipes</td>
  <td class=xl65 width=649 style='width:487pt'>Building upon the considerable
  advances in Large Language Models (LLMs), we are now equipped to address more
  sophisticated tasks demanding a nuanced understanding of cross-cultural
  contexts. A key example is recipe adaptation, which goes beyond simple translation
  to include a grasp of ingredients, culinary techniques, and dietary
  preferences specific to a given culture. We introduce a new task involving
  the translation and cultural adaptation of recipes between Chinese and
  English-speaking cuisines. To support this investigation, we present
  CulturalRecipes, a unique dataset comprised of automatically paired recipes
  written in Mandarin Chinese and English. This dataset is further enriched
  with a human-written and curated test set. In this intricate task of
  cross-cultural recipe adaptation, we evaluate the performance of various
  methods, including GPT-4 and other LLMs, traditional machine translation, and
  information retrieval techniques. Our comprehensive analysis includes both
  automatic and human evaluation metrics. While GPT-4 exhibits impressive
  abilities in adapting Chinese recipes into English, it still lags behind
  human expertise when translating English recipes into Chinese. This
  underscores the multifaceted nature of cultural adaptations. We anticipate
  that these insights will significantly contribute to future research on
  culturally-aware language models and their practical application in
  culturally diverse contexts.</td>
  <td colspan=2 style='mso-ignore:colspan'>26 October, 2023</td>
 </tr>
 <tr height=227 style='height:170.0pt'>
  <td height=227 align=right style='height:170.0pt'>3665</td>
  <td align=right>2310.19425</td>
  <td class=xl65 width=649 style='width:487pt'>Artificial intelligence and the
  limits of the humanities</td>
  <td class=xl65 width=649 style='width:487pt'>The complexity of cultures in
  the modern world is now beyond human comprehension. Cognitive sciences cast
  doubts on the traditional explanations based on mental models. The core
  subjects in humanities may lose their importance. Humanities have to adapt to
  the digital age. New, interdisciplinary branches of humanities emerge.
  Instant access to information will be replaced by instant access to
  knowledge. Understanding the cognitive limitations of humans and the
  opportunities opened by the development of artificial intelligence and
  interdisciplinary research necessary to address global challenges is the key
  to the revitalization of humanities. Artificial intelligence will radically
  change humanities, from art to political sciences and philosophy, making these
  disciplines attractive to students and enabling them to go beyond current
  limitations.</td>
  <td colspan=2 style='mso-ignore:colspan'>30 October, 2023</td>
 </tr>
 <tr height=475 style='height:356.0pt'>
  <td height=475 align=right style='height:356.0pt'>3611</td>
  <td align=right>2311.05418</td>
  <td class=xl65 width=649 style='width:487pt'>Generalization in medical AI: a
  perspective on developing scalable models</td>
  <td class=xl65 width=649 style='width:487pt'>Over the past few years,
  research has witnessed the advancement of deep learning models trained on
  large datasets, some even encompassing millions of examples. While these
  impressive performance on their hidden test sets, they often underperform
  when assessed on external datasets. Recognizing the critical role of
  generalization in medical AI development, many prestigious journals now
  require reporting results both on the local hidden test set as well as on
  external datasets before considering a study for publication. Effectively,
  the field of medical AI has transitioned from the traditional usage of a
  single dataset that is split into train and test to a more comprehensive
  framework using multiple datasets, some of which are used for model
  development (source domain) and others for testing (target domains). However,
  this new experimental setting does not necessarily resolve the challenge of
  generalization. This is because of the variability encountered in intended
  use and specificities across hospital cultures making the idea of universally
  generalizable systems a myth. On the other hand, the systematic, and a
  fortiori recurrent re-calibration, of models at the individual hospital
  level, although ideal, may be overoptimistic given the legal, regulatory and
  technical challenges that are involved. Re-calibration using transfer
  learning may not even be possible in some instances where reference labels of
  target domains are not available. In this perspective we establish a
  hierarchical three-level scale system reflecting the generalization level of
  a medical AI algorithm. This scale better reflects the diversity of
  real-world medical scenarios per which target domain data for re-calibration
  of models may or not be available and if it is, may or not have reference
  labels systematically available.</td>
  <td colspan=2 style='mso-ignore:colspan'>9 November, 2023</td>
 </tr>
 <tr height=453 style='height:340.0pt'>
  <td height=453 align=right style='height:340.0pt'>3608</td>
  <td align=right>2311.06207</td>
  <td class=xl65 width=649 style='width:487pt'>Vox Populi, Vox ChatGPT: Large
  Language Models, Education and Democracy</td>
  <td class=xl65 width=649 style='width:487pt'>In the era of generative AI and
  specifically large language models (LLMs), exemplified by ChatGPT, the
  intersection of artificial intelligence and human reasoning has become a
  focal point of global attention. Unlike conventional search engines, LLMs go
  beyond mere information retrieval, entering into the realm of discourse
  culture. Its outputs mimic well-considered, independent opinions or
  statements of facts, presenting a pretense of wisdom. This paper explores the
  potential transformative impact of LLMs on democratic societies. It delves
  into the concerns regarding the difficulty in distinguishing
  ChatGPT-generated texts from human output. The discussion emphasizes the
  essence of authorship, rooted in the unique human capacity for reason - a
  quality indispensable for democratic discourse and successful collaboration
  within free societies. Highlighting the potential threats to democracy, this
  paper presents three arguments: the Substitution argument, the Authenticity
  argument, and the Facts argument. These arguments highlight the potential
  risks that are associated with an overreliance on LLMs. The central thesis
  posits that widespread deployment of LLMs may adversely affect the fabric of
  a democracy if not comprehended and addressed proactively and properly. In
  proposing a solution, we advocate for an emphasis on education as a means to
  mitigate risks. We suggest cultivating thinking skills in children, fostering
  coherent thought formulation, and distinguishing between machine-generated
  output and genuine, i.e. human, reasoning. The focus should be on responsible
  development and usage of LLMs, with the goal of augmenting human capacities
  in thinking, deliberating and decision-making rather than substituting them.</td>
  <td colspan=2 style='mso-ignore:colspan'>10 November, 2023</td>
 </tr>
 <tr height=249 style='height:187.0pt'>
  <td height=249 align=right style='height:187.0pt'>3576</td>
  <td align=right>2311.09271</td>
  <td class=xl65 width=649 style='width:487pt'>An Empathetic User-Centric
  Chatbot for Emotional Support</td>
  <td class=xl65 width=649 style='width:487pt'>This paper explores the
  intersection of Otome Culture and artificial intelligence, particularly
  focusing on how Otome-oriented games fulfill the emotional needs of young
  women. These games, which are deeply rooted in a subcultural understanding of
  love, provide players with feelings of satisfaction, companionship, and
  protection through carefully crafted narrative structures and character
  development. With the proliferation of Large Language Models (LLMs), there is
  an opportunity to transcend traditional static game narratives and create
  dynamic, emotionally responsive interactions. We present a case study of
  Tears of Themis, where we have integrated LLM technology to enhance the
  interactive experience. Our approach involves augmenting existing game narratives
  with a Question and Answer (QA) system, enriched through data augmentation
  and emotional enhancement techniques, resulting in a chatbot that offers
  realistic and supportive companionship.</td>
  <td colspan=2 style='mso-ignore:colspan'>15 November, 2023</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>3553</td>
  <td align=right>2311.10766</td>
  <td class=xl65 width=649 style='width:487pt'>Value FULCRA: Mapping Large
  Language Models to the Multidimensional Spectrum of Basic Human Values</td>
  <td class=xl65 width=649 style='width:487pt'>The rapid advancement of Large
  Language Models (LLMs) has attracted much attention to value alignment for
  their responsible development. However, how to define values in this context
  remains a largely unexplored question. Existing work mainly follows the
  Helpful, Honest, Harmless principle and specifies values as risk criteria
  formulated in the AI community, e.g., fairness and privacy protection,
  suffering from poor clarity, adaptability and transparency. Inspired by basic
  values in humanity and social science across cultures, this work proposes a
  novel basic value alignment paradigm and introduces a value space spanned by
  basic value dimensions. All LLMs' behaviors can be mapped into the space by
  identifying the underlying values, possessing the potential to address the
  three challenges. To foster future research, we apply the representative
  Schwartz's Theory of Basic Values as an initialized example and construct
  FULCRA, a dataset consisting of 5k (LLM output, value vector) pairs. Our
  extensive analysis of FULCRA reveals the underlying relation between basic
  values and LLMs' behaviors, demonstrating that our approach not only covers
  existing mainstream risks but also anticipates possibly unidentified ones.
  Additionally, we present an initial implementation of the basic value
  evaluation and alignment, paving the way for future research in this line.</td>
  <td colspan=2 style='mso-ignore:colspan'>15 November, 2023</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>3527</td>
  <td align=right>2311.14096</td>
  <td class=xl65 width=649 style='width:487pt'>Cultural Bias and Cultural
  Alignment of Large Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>Culture fundamentally shapes
  people's reasoning, behavior, and communication. As people increasingly use
  generative artificial intelligence (AI) to expedite and automate personal and
  professional tasks, cultural values embedded in AI models may bias people's
  authentic expression and contribute to the dominance of certain cultures. We
  conduct a disaggregated evaluation of cultural bias for five widely used
  large language models (OpenAI's GPT-4o/4-turbo/4/3.5-turbo/3) by comparing
  the models' responses to nationally representative survey data. All models
  exhibit cultural values resembling English-speaking and Protestant European
  countries. We test cultural prompting as a control strategy to increase
  cultural alignment for each country/territory. For recent models (GPT-4,
  4-turbo, 4o), this improves the cultural alignment of the models' output for
  71-81% of countries and territories. We suggest using cultural prompting and
  ongoing evaluation to reduce cultural bias in the output of generative AI.</td>
  <td colspan=2 style='mso-ignore:colspan'>26 June, 2024</td>
 </tr>
 <tr height=385 style='height:289.0pt'>
  <td height=385 align=right style='height:289.0pt'>3515</td>
  <td align=right>2311.15138</td>
  <td class=xl65 width=649 style='width:487pt'>Can SAM recognize crops?
  Quantifying the zero-shot performance of a semantic segmentation foundation
  model on generating crop-type maps using satellite imagery for precision
  agriculture</td>
  <td class=xl65 width=649 style='width:487pt'>Climate change is increasingly
  disrupting worldwide agriculture, making global food production less
  reliable. To tackle the growing challenges in feeding the planet,
  cutting-edge management strategies, such as precision agriculture, empower
  farmers and decision-makers with rich and actionable information to increase
  the efficiency and sustainability of their farming practices. Crop-type maps
  are key information for decision-support tools but are challenging and costly
  to generate. We investigate the capabilities of Meta AI's Segment Anything
  Model (SAM) for crop-map prediction task, acknowledging its recent successes
  at zero-shot image segmentation. However, SAM being limited to up-to 3
  channel inputs and its zero-shot usage being class-agnostic in nature pose
  unique challenges in using it directly for crop-type mapping. We propose
  using clustering consensus metrics to assess SAM's zero-shot performance in
  segmenting satellite imagery and producing crop-type maps. Although direct
  crop-type mapping is challenging using SAM in zero-shot setting, experiments
  reveal SAM's potential for swiftly and accurately outlining fields in
  satellite images, serving as a foundation for subsequent crop classification.
  This paper attempts to highlight a use-case of state-of-the-art image
  segmentation models like SAM for crop-type mapping and related specific needs
  of the agriculture industry, offering a potential avenue for automatic,
  efficient, and cost-effective data products for precision agriculture
  practices.</td>
  <td colspan=2 style='mso-ignore:colspan'>4 December, 2023</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>3500</td>
  <td align=right>2311.16421</td>
  <td class=xl65 width=649 style='width:487pt'>CDEval: A Benchmark for
  Measuring the Cultural Dimensions of Large Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>As the scaling of Large Language
  Models (LLMs) has dramatically enhanced their capabilities, there has been a
  growing focus on the alignment problem to ensure their responsible and
  ethical use. While existing alignment efforts predominantly concentrate on
  universal values such as the HHH principle, the aspect of culture, which is
  inherently pluralistic and diverse, has not received adequate attention. This
  work introduces a new benchmark, CDEval, aimed at evaluating the cultural
  dimensions of LLMs. CDEval is constructed by incorporating both GPT-4's
  automated generation and human verification, covering six cultural dimensions
  across seven domains. Our comprehensive experiments provide intriguing
  insights into the culture of mainstream LLMs, highlighting both consistencies
  and variations across different dimensions and domains. The findings
  underscore the importance of integrating cultural considerations in LLM
  development, particularly for applications in diverse cultural settings.
  Through CDEval, we aim to broaden the horizon of LLM alignment research by
  including cultural dimensions, thus providing a more holistic framework for
  the future development and evaluation of LLMs. This benchmark serves as a
  valuable resource for cultural studies in LLMs, paving the way for more
  culturally aware and sensitive models.</td>
  <td colspan=2 style='mso-ignore:colspan'>20 June, 2024</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>3496</td>
  <td align=right>2311.17086</td>
  <td class=xl65 width=649 style='width:487pt'>PEA-Diffusion:
  Parameter-Efficient Adapter with Knowledge Distillation in non-English
  Text-to-Image Generation</td>
  <td class=xl65 width=649 style='width:487pt'>Text-to-image diffusion models
  are well-known for their ability to generate realistic images based on
  textual prompts. However, the existing works have predominantly focused on
  English, lacking support for non-English text-to-image models. The most commonly
  used translation methods cannot solve the generation problem related to
  language culture, while training from scratch on a specific language dataset
  is prohibitively expensive. In this paper, we are inspired to propose a
  simple plug-and-play language transfer method based on knowledge
  distillation. All we need to do is train a lightweight MLP-like
  parameter-efficient adapter (PEA) with only 6M parameters under teacher
  knowledge distillation along with a small parallel data corpus. We are
  surprised to find that freezing the parameters of UNet can still achieve
  remarkable performance on the language-specific prompt evaluation set,
  demonstrating that PEA can stimulate the potential generation ability of the
  original UNet. Additionally, it closely approaches the performance of the
  English text-to-image model on a general prompt evaluation set. Furthermore,
  our adapter can be used as a plugin to achieve significant results in
  downstream tasks in cross-lingual text-to-image generation. Code will be available
  at: https://github.com/OPPO-Mente-Lab/PEA-Diffusion</td>
  <td colspan=2 style='mso-ignore:colspan'>23 July, 2024</td>
 </tr>
 <tr height=496 style='height:372.0pt'>
  <td height=496 align=right style='height:372.0pt'>3486</td>
  <td align=right>2311.17978</td>
  <td class=xl65 width=649 style='width:487pt'>AutArch: An AI-assisted workflow
  for object detection and automated recording in archaeological catalogues</td>
  <td class=xl65 width=649 style='width:487pt'>The context of this paper is the
  creation of large uniform archaeological datasets from heterogeneous
  published resources, such as find catalogues - with the help of AI and Big
  Data. The paper is concerned with the challenge of consistent assemblages of archaeological
  data. We cannot simply combine existing records, as they differ in terms of
  quality and recording standards. Thus, records have to be recreated from
  published archaeological illustrations. This is only a viable path with the
  help of automation. The contribution of this paper is a new workflow for
  collecting data from archaeological find catalogues available as legacy
  resources, such as archaeological drawings and photographs in large unsorted
  PDF files; the workflow relies on custom software (AutArch) supporting image
  processing, object detection, and interactive means of validating and
  adjusting automatically retrieved data. We integrate artificial intelligence
  (AI) in terms of neural networks for object detection and classification into
  the workflow, thereby speeding up, automating, and standardising data
  collection. Objects commonly found in archaeological catalogues - such as
  graves, skeletons, ceramics, ornaments, stone tools and maps - are detected.
  Those objects are spatially related and analysed to extract real-life
  attributes, such as the size and orientation of graves based on the north
  arrow and the scale. We also automate recording of geometric whole-outlines
  through contour detection, as an alternative to landmark-based geometric
  morphometrics. Detected objects, contours, and other automatically retrieved
  data can be manually validated and adjusted. We use third millennium BC
  Europe (encompassing cultures such as 'Corded Ware' and 'Bell Beaker', and
  their burial practices) as a 'testing ground' and for evaluation purposes;
  this includes a user study for the workflow and the AutArch software.</td>
  <td colspan=2 style='mso-ignore:colspan'>15 February, 2024</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>3474</td>
  <td align=right>2312.0003</td>
  <td class=xl65 width=649 style='width:487pt'>Artificial Intelligence in
  Sustainable Vertical Farming</td>
  <td class=xl65 width=649 style='width:487pt'>As global challenges of
  population growth, climate change, and resource scarcity intensify, the
  agricultural landscape is at a critical juncture. Sustainable vertical
  farming emerges as a transformative solution to address these challenges by
  maximizing crop yields in controlled environments. This paradigm shift
  necessitates the integration of cutting-edge technologies, with Artificial
  Intelligence (AI) at the forefront. The paper provides a comprehensive
  exploration of the role of AI in sustainable vertical farming, investigating
  its potential, challenges, and opportunities. The review synthesizes the
  current state of AI applications, encompassing machine learning, computer
  vision, the Internet of Things (IoT), and robotics, in optimizing resource
  usage, automating tasks, and enhancing decision-making. It identifies gaps in
  research, emphasizing the need for optimized AI models, interdisciplinary
  collaboration, and the development of explainable AI in agriculture. The
  implications extend beyond efficiency gains, considering economic viability,
  reduced environmental impact, and increased food security. The paper
  concludes by offering insights for stakeholders and suggesting avenues for
  future research, aiming to guide the integration of AI technologies in
  sustainable vertical farming for a resilient and sustainable future in
  agriculture.</td>
  <td colspan=2 style='mso-ignore:colspan'>17 November, 2023</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>3412</td>
  <td align=right>2312.06094</td>
  <td class=xl65 width=649 style='width:487pt'>MATK: The Meme Analytical Tool
  Kit</td>
  <td class=xl65 width=649 style='width:487pt'>The rise of social media
  platforms has brought about a new digital culture called memes. Memes, which
  combine visuals and text, can strongly influence public opinions on social
  and cultural issues. As a result, people have become interested in categorizing
  memes, leading to the development of various datasets and multimodal models
  that show promising results in this field. However, there is currently a lack
  of a single library that allows for the reproduction, evaluation, and
  comparison of these models using fair benchmarks and settings. To fill this
  gap, we introduce the Meme Analytical Tool Kit (MATK), an open-source toolkit
  specifically designed to support existing memes datasets and cutting-edge
  multimodal models. MATK aims to assist researchers and engineers in training
  and reproducing these multimodal models for meme classification tasks, while
  also providing analysis techniques to gain insights into their strengths and
  weaknesses. To access MATK, please visit \url{https://github.com/Social-AI-Studio/MATK}.</td>
  <td colspan=2 style='mso-ignore:colspan'>10 December, 2023</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>3389</td>
  <td align=right>2312.08467</td>
  <td class=xl65 width=649 style='width:487pt'>Culturally Responsive Artificial
  Intelligence -- Problems, Challenges and Solutions</td>
  <td class=xl65 width=649 style='width:487pt'>In the contemporary
  interconnected world, the concept of cultural responsibility occupies
  paramount importance. As the lines between nations become less distinct, it
  is incumbent upon individuals, communities, and institutions to assume the
  responsibility of safeguarding and valuing the landscape of diverse cultures
  that constitute our global society. This paper explores the socio-cultural
  and ethical challenges stemming from the implementation of AI algorithms and
  highlights the necessity for their culturally responsive development. It also
  offers recommendations on essential elements required to enhance AI systems'
  adaptability to meet the demands of contemporary multicultural societies. The
  paper highlights the need for further multidisciplinary research to create AI
  models that effectively address these challenges. It also advocates the
  significance of AI enculturation and underlines the importance of regulatory
  measures to promote cultural responsibility in AI systems.</td>
  <td colspan=2 style='mso-ignore:colspan'>13 December, 2023</td>
 </tr>
 <tr height=496 style='height:372.0pt'>
  <td height=496 align=right style='height:372.0pt'>3372</td>
  <td align=right>2312.10075</td>
  <td class=xl65 width=649 style='width:487pt'>Assessing LLMs for Moral Value
  Pluralism</td>
  <td class=xl65 width=649 style='width:487pt'>The fields of AI current lacks
  methods to quantitatively assess and potentially alter the moral values
  inherent in the output of large language models (LLMs). However, decades of
  social science research has developed and refined widely-accepted moral value
  surveys, such as the World Values Survey (WVS), eliciting value judgments
  from direct questions in various geographies. We have turned those questions
  into value statements and use NLP to compute to how well popular LLMs are
  aligned with moral values for various demographics and cultures. While the
  WVS is accepted as an explicit assessment of values, we lack methods for
  assessing implicit moral and cultural values in media, e.g., encountered in
  social media, political rhetoric, narratives, and generated by AI systems
  such as LLMs that are increasingly present in our daily lives. As we consume
  online content and utilize LLM outputs, we might ask, which moral values are
  being implicitly promoted or undercut, or -- in the case of LLMs -- if they
  are intending to represent a cultural identity, are they doing so
  consistently? In this paper we utilize a Recognizing Value Resonance (RVR)
  NLP model to identify WVS values that resonate and conflict with a given
  passage of output text. We apply RVR to the text generated by LLMs to
  characterize implicit moral values, allowing us to quantify the
  moral/cultural distance between LLMs and various demographics that have been
  surveyed using the WVS. In line with other work we find that LLMs exhibit
  several Western-centric value biases; they overestimate how conservative
  people in non-Western countries are, they are less accurate in representing
  gender for non-Western countries, and portray older populations as having
  more traditional values. Our results highlight value misalignment and age
  groups, and a need for social science informed technological solutions
  addressing value plurality in LLMs.</td>
  <td colspan=2 style='mso-ignore:colspan'>8 December, 2023</td>
 </tr>
 <tr height=181 style='height:136.0pt'>
  <td height=181 align=right style='height:136.0pt'>3357</td>
  <td align=right>2312.11011</td>
  <td class=xl65 width=649 style='width:487pt'>VinaLLaMA: LLaMA-based
  Vietnamese Foundation Model</td>
  <td class=xl65 width=649 style='width:487pt'>In this technical report, we
  present VinaLLaMA, an open-weight, state-of-the-art (SOTA) Large Language
  Model for the Vietnamese language, built upon LLaMA-2 with an additional 800
  billion trained tokens. VinaLLaMA not only demonstrates fluency in Vietnamese
  but also exhibits a profound understanding of Vietnamese culture, making it a
  truly indigenous model. VinaLLaMA-7B-chat, trained on 1 million high-quality
  synthetic samples, achieves SOTA results on key benchmarks, including VLSP,
  VMLU, and Vicuna Benchmark Vietnamese, marking a significant advancement in
  the Vietnamese AI landscape and offering a versatile resource for various
  applications.</td>
  <td colspan=2 style='mso-ignore:colspan'>18 December, 2023</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>3353</td>
  <td align=right>2312.11753</td>
  <td class=xl65 width=649 style='width:487pt'>Recording and Describing Poker
  Hands</td>
  <td class=xl65 width=649 style='width:487pt'>This paper introduces the Poker
  Hand History (PHH) file format, designed to standardize the recording of
  poker hands across different game variants. Despite poker's widespread
  popularity in the mainstream culture as a mind sport and its prominence in
  the field of artificial intelligence (AI) research as a benchmark for
  imperfect information AI agents, it lacks a consistent format that humans can
  use to document poker hands across different variants that can also easily be
  parsed by machines. To address this gap in the literature, we propose the PHH
  format which provides a concise human-readable machine-friendly
  representation of hand history that comprehensively captures various details
  of the hand, ranging from initial game parameters and actions to contextual
  parameters including but not limited to the venue, players, and time control
  information. In the supplementary, we provide 10,088 hands covering 11
  different variants in the PHH format. The full specification is available on
  https://github.com/uoftcprg/phh-std</td>
  <td colspan=2 style='mso-ignore:colspan'>29 August, 2024</td>
 </tr>
 <tr height=408 style='height:306.0pt'>
  <td height=408 align=right style='height:306.0pt'>3305</td>
  <td align=right>2312.17479</td>
  <td class=xl65 width=649 style='width:487pt'>Culturally-Attuned Moral
  Machines: Implicit Learning of Human Value Systems by AI through Inverse
  Reinforcement Learning</td>
  <td class=xl65 width=649 style='width:487pt'>Constructing a universal moral
  code for artificial intelligence (AI) is difficult or even impossible, given
  that different human cultures have different definitions of morality and
  different societal norms. We therefore argue that the value system of an AI
  should be culturally attuned: just as a child raised in a particular culture
  learns the specific values and norms of that culture, we propose that an AI
  agent operating in a particular human community should acquire that
  community's moral, ethical, and cultural codes. How AI systems might acquire
  such codes from human observation and interaction has remained an open
  question. Here, we propose using inverse reinforcement learning (IRL) as a
  method for AI agents to acquire a culturally-attuned value system implicitly.
  We test our approach using an experimental paradigm in which AI agents use
  IRL to learn different reward functions, which govern the agents' moral
  values, by observing the behavior of different cultural groups in an online
  virtual world requiring real-time decision making. We show that an AI agent
  learning from the average behavior of a particular cultural group can acquire
  altruistic characteristics reflective of that group's behavior, and this
  learned value system can generalize to new scenarios requiring altruistic
  judgments. Our results provide, to our knowledge, the first demonstration
  that AI agents could potentially be endowed with the ability to continually
  learn their values and norms from observing and interacting with humans,
  thereby becoming attuned to the culture they are operating in.</td>
  <td colspan=2 style='mso-ignore:colspan'>29 December, 2023</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>3297</td>
  <td align=right>2401.00504</td>
  <td class=xl65 width=649 style='width:487pt'>HSC-GPT: A Large Language Model
  for Human Settlements Construction</td>
  <td class=xl65 width=649 style='width:487pt'>The field of human settlement
  construction encompasses a range of spatial designs and management tasks,
  including urban planning and landscape architecture design. These tasks
  involve a plethora of instructions and descriptions presented in natural
  language, which are essential for understanding design requirements and
  producing effective design solutions. Recent research has sought to integrate
  natural language processing (NLP) and generative artificial intelligence (AI)
  into human settlement construction tasks. Due to the efficient processing and
  analysis capabilities of AI with data, significant successes have been
  achieved in design within this domain. However, this task still faces several
  fundamental challenges. The semantic information involved includes complex
  spatial details, diverse data source formats, high sensitivity to regional
  culture, and demanding requirements for innovation and rigor in work
  scenarios. These factors lead to limitations when applying general generative
  AI in this field, further exacerbated by a lack of high-quality data for
  model training. To address these challenges, this paper first proposes
  HSC-GPT, a large-scale language model framework specifically designed for
  tasks in human settlement construction, considering the unique
  characteristics of this domain.</td>
  <td colspan=2 style='mso-ignore:colspan'>31 December, 2023</td>
 </tr>
 <tr height=431 style='height:323.0pt'>
  <td height=431 align=right style='height:323.0pt'>3294</td>
  <td align=right>2401.00814</td>
  <td class=xl65 width=649 style='width:487pt'>Agricultural 4.0 Leveraging on
  Technological Solutions: Study for Smart Farming Sector</td>
  <td class=xl65 width=649 style='width:487pt'>By 2050, it is predicted that
  there will be 9 billion people on the planet, which will call for more
  production, lower costs, and the preservation of natural resources. It is
  anticipated that atypical occurrences and climate change will pose severe
  risks to agricultural output. It follows that a 70% or more significant rise
  in food output is anticipated. Smart farming, often known as agriculture 4.0,
  is a tech-driven revolution in agriculture with the goal of raising industry
  production and efficiency. Four primary trends are responsible for it: food
  waste, climate change, population shifts, and resource scarcity. The
  agriculture industry is changing as a result of the adoption of emerging
  technologies. Using cutting-edge technology like IoT, AI, and other sensors,
  smart farming transforms traditional production methods and international
  agricultural policies. The objective is to establish a value chain that is
  optimized to facilitate enhanced monitoring and decreased labor expenses. The
  agricultural sector has seen tremendous transformation as a result of the
  fourth industrial revolution, which has combined traditional farming methods
  with cutting-edge technology to increase productivity, sustainability, and
  efficiency. To effectively utilize the potential of technology gadgets in the
  agriculture sector, collaboration between governments, private sector
  entities, and other stakeholders is necessary. This paper covers Agriculture
  4.0, looks at its possible benefits and drawbacks of the implementation
  methodologies, compatibility, reliability, and investigates the several
  digital tools that are being utilized to change the agriculture industry and
  how to mitigate the challenges.</td>
  <td colspan=2 style='mso-ignore:colspan'>1 January, 2024</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>3284</td>
  <td align=right>2401.016</td>
  <td class=xl65 width=649 style='width:487pt'>PLLaMa: An Open-source Large
  Language Model for Plant Science</td>
  <td class=xl65 width=649 style='width:487pt'>Large Language Models (LLMs)
  have exhibited remarkable capabilities in understanding and interacting with
  natural language across various sectors. However, their effectiveness is
  limited in specialized areas requiring high accuracy, such as plant science,
  due to a lack of specific expertise in these fields. This paper introduces
  PLLaMa, an open-source language model that evolved from LLaMa-2. It's
  enhanced with a comprehensive database, comprising more than 1.5 million
  scholarly articles in plant science. This development significantly enriches
  PLLaMa with extensive knowledge and proficiency in plant and agricultural
  sciences. Our initial tests, involving specific datasets related to plants
  and agriculture, show that PLLaMa substantially improves its understanding of
  plant science-related topics. Moreover, we have formed an international panel
  of professionals, including plant scientists, agricultural engineers, and
  plant breeders. This team plays a crucial role in verifying the accuracy of
  PLLaMa's responses to various academic inquiries, ensuring its effective and
  reliable application in the field. To support further research and
  development, we have made the model's checkpoints and source codes accessible
  to the scientific community. These resources are available for download at
  \url{https://github.com/Xianjun-Yang/PLLaMa}.</td>
  <td colspan=2 style='mso-ignore:colspan'>3 January, 2024</td>
 </tr>
 <tr height=204 style='height:153.0pt'>
  <td height=204 align=right style='height:153.0pt'>3277</td>
  <td align=right>2401.02424</td>
  <td class=xl65 width=649 style='width:487pt'>Mapping of Land Use and Land
  Cover (LULC) using EuroSAT and Transfer Learning</td>
  <td class=xl65 width=649 style='width:487pt'>As the global population
  continues to expand, the demand for natural resources increases.
  Unfortunately, human activities account for 23% of greenhouse gas emissions.
  On a positive note, remote sensing technologies have emerged as a valuable
  tool in managing our environment. These technologies allow us to monitor land
  use, plan urban areas, and drive advancements in areas such as agriculture,
  climate change mitigation, disaster recovery, and environmental monitoring.
  Recent advances in AI, computer vision, and earth observation data have
  enabled unprecedented accuracy in land use mapping. By using transfer
  learning and fine-tuning with RGB bands, we achieved an impressive 99.19%
  accuracy in land use analysis. Such findings can be used to inform conservation
  and urban planning policies.</td>
  <td colspan=2 style='mso-ignore:colspan'>6 November, 2023</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>3269</td>
  <td align=right>2401.03481</td>
  <td class=xl65 width=649 style='width:487pt'>A Large Language Model Supported
  Synthesis of Contemporary Academic Integrity Research Trends</td>
  <td class=xl65 width=649 style='width:487pt'>This paper reports on
  qualitative content analysis undertaken using ChatGPT, a Large Language Model
  (LLM), to identify primary research themes in current academic integrity
  research as well as the methodologies used to explore these areas. The
  analysis by the LLM identified 7 research themes and 13 key areas for
  exploration. The outcomes from the analysis suggest that much contemporary
  research in the academic integrity field is guided by technology. Technology
  is often explored as potential way of preventing academic misconduct, but
  this could also be a limiting factor when aiming to promote a culture of
  academic integrity. The findings underscore that LLM led research may be
  option in the academic integrity field, but that there is also a need for continued
  traditional research. The findings also indicate that researchers and
  educational providers should continue to develop policy and operational
  frameworks for academic integrity. This will help to ensure that academic
  standards are maintained across the wide range of settings that are present
  in modern education.</td>
  <td colspan=2 style='mso-ignore:colspan'>7 January, 2024</td>
 </tr>
 <tr height=385 style='height:289.0pt'>
  <td height=385 align=right style='height:289.0pt'>3258</td>
  <td align=right>2401.0421</td>
  <td class=xl65 width=649 style='width:487pt'>FunnyNet-W: Multimodal Learning
  of Funny Moments in Videos in the Wild</td>
  <td class=xl65 width=649 style='width:487pt'>Automatically understanding
  funny moments (i.e., the moments that make people laugh) when watching comedy
  is challenging, as they relate to various features, such as body language,
  dialogues and culture. In this paper, we propose FunnyNet-W, a model that
  relies on cross- and self-attention for visual, audio and text data to
  predict funny moments in videos. Unlike most methods that rely on ground
  truth data in the form of subtitles, in this work we exploit modalities that
  come naturally with videos: (a) video frames as they contain visual
  information indispensable for scene understanding, (b) audio as it contains
  higher-level cues associated with funny moments, such as intonation, pitch
  and pauses and (c) text automatically extracted with a speech-to-text model
  as it can provide rich information when processed by a Large Language Model.
  To acquire labels for training, we propose an unsupervised approach that
  spots and labels funny audio moments. We provide experiments on five
  datasets: the sitcoms TBBT, MHD, MUStARD, Friends, and the TED talk UR-Funny.
  Extensive experiments and analysis show that FunnyNet-W successfully exploits
  visual, auditory and textual cues to identify funny moments, while our
  findings reveal FunnyNet-W's ability to predict funny moments in the wild.
  FunnyNet-W sets the new state of the art for funny moment detection with
  multimodal cues on all datasets with and without using ground truth
  information.</td>
  <td colspan=2 style='mso-ignore:colspan'>8 January, 2024</td>
 </tr>
 <tr height=227 style='height:170.0pt'>
  <td height=227 align=right style='height:170.0pt'>3244</td>
  <td align=right>2401.06171</td>
  <td class=xl65 width=649 style='width:487pt'>Harnessing Artificial
  Intelligence for Sustainable Agricultural Development in Africa:
  Opportunities, Challenges, and Impact</td>
  <td class=xl65 width=649 style='width:487pt'>This paper explores the
  transformative potential of artificial intelligence (AI) in the context of
  sustainable agricultural development across diverse regions in Africa.
  Delving into opportunities, challenges, and impact, the study navigates
  through the dynamic landscape of AI applications in agriculture.
  Opportunities such as precision farming, crop monitoring, and
  climate-resilient practices are examined, alongside challenges related to
  technological infrastructure, data accessibility, and skill gaps. The article
  analyzes the impact of AI on smallholder farmers, supply chains, and
  inclusive growth. Ethical considerations and policy implications are also
  discussed, offering insights into responsible AI integration. By providing a
  nuanced understanding, this paper contributes to the ongoing discourse on
  leveraging AI for fostering sustainability in African agriculture.</td>
  <td colspan=2 style='mso-ignore:colspan'>3 January, 2024</td>
 </tr>
 <tr height=249 style='height:187.0pt'>
  <td height=249 align=right style='height:187.0pt'>3225</td>
  <td align=right>2401.08212</td>
  <td class=xl65 width=649 style='width:487pt'>Human vs. LMMs: Exploring the
  Discrepancy in Emoji Interpretation and Usage in Digital Communication</td>
  <td class=xl65 width=649 style='width:487pt'>Leveraging Large Multimodal
  Models (LMMs) to simulate human behaviors when processing multimodal
  information, especially in the context of social media, has garnered immense
  interest due to its broad potential and far-reaching implications. Emojis, as
  one of the most unique aspects of digital communication, are pivotal in
  enriching and often clarifying the emotional and tonal dimensions. Yet, there
  is a notable gap in understanding how these advanced models, such as GPT-4V,
  interpret and employ emojis in the nuanced context of online interaction.
  This study intends to bridge this gap by examining the behavior of GPT-4V in
  replicating human-like use of emojis. The findings reveal a discernible
  discrepancy between human and GPT-4V behaviors, likely due to the subjective
  nature of human interpretation and the limitations of GPT-4V's
  English-centric training, suggesting cultural biases and inadequate
  representation of non-English cultures.</td>
  <td colspan=2 style='mso-ignore:colspan'>15 April, 2024</td>
 </tr>
 <tr height=431 style='height:323.0pt'>
  <td height=431 align=right style='height:323.0pt'>3219</td>
  <td align=right>2401.08691</td>
  <td class=xl65 width=649 style='width:487pt'>Towards Responsible AI in
  Banking: Addressing Bias for Fair Decision-Making</td>
  <td class=xl65 width=649 style='width:487pt'>In an era characterized by the
  pervasive integration of artificial intelligence into decision-making
  processes across diverse industries, the demand for trust has never been more
  pronounced. This thesis embarks on a comprehensive exploration of bias and fairness,
  with a particular emphasis on their ramifications within the banking sector,
  where AI-driven decisions bear substantial societal consequences. In this
  context, the seamless integration of fairness, explainability, and human
  oversight is of utmost importance, culminating in the establishment of what
  is commonly referred to as &quot;Responsible AI&quot;. This emphasizes the
  critical nature of addressing biases within the development of a corporate
  culture that aligns seamlessly with both AI regulations and universal human
  rights standards, particularly in the realm of automated decision-making
  systems. Nowadays, embedding ethical principles into the development,
  training, and deployment of AI models is crucial for compliance with
  forthcoming European regulations and for promoting societal good. This thesis
  is structured around three fundamental pillars: understanding bias,
  mitigating bias, and accounting for bias. These contributions are validated
  through their practical application in real-world scenarios, in collaboration
  with Intesa Sanpaolo. This collaborative effort not only contributes to our
  understanding of fairness but also provides practical tools for the
  responsible implementation of AI-based decision-making systems. In line with
  open-source principles, we have released Bias On Demand and FairView as
  accessible Python packages, further promoting progress in the field of AI
  fairness.</td>
  <td colspan=2 style='mso-ignore:colspan'>13 January, 2024</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>3213</td>
  <td align=right>2401.09457</td>
  <td class=xl65 width=649 style='width:487pt'>Empowering Africa: An In-depth
  Exploration of the Adoption of Artificial Intelligence Across the Continent</td>
  <td class=xl65 width=649 style='width:487pt'>This paper explores the dynamic
  landscape of Artificial Intelligence (AI) adoption in Africa, analysing its
  varied applications in addressing socio-economic challenges and fostering
  development. Examining the African AI ecosystem, the study considers regional
  nuances, cultural factors, and infrastructural constraints shaping the
  deployment of AI solutions. Case studies in healthcare, agriculture, finance,
  and education highlight AI's transformative potential for efficiency,
  accessibility, and inclusivity. The paper emphasizes indigenous AI
  innovations and international collaborations contributing to a distinct
  African AI ecosystem. Ethical considerations, including data privacy and
  algorithmic bias, are addressed alongside policy frameworks supporting responsible
  AI implementation. The role of governmental bodies, regulations, and private
  sector partnerships is explored in creating a conducive AI development
  environment. Challenges such as digital literacy gaps and job displacement
  are discussed, with proposed strategies for mitigation. In conclusion, the
  paper provides a nuanced understanding of AI in Africa, contributing to
  sustainable development discussions and advocating for an inclusive and
  ethical AI ecosystem on the continent.</td>
  <td colspan=2 style='mso-ignore:colspan'>28 December, 2023</td>
 </tr>
 <tr height=496 style='height:372.0pt'>
  <td height=496 align=right style='height:372.0pt'>3191</td>
  <td align=right>2401.1272</td>
  <td class=xl65 width=649 style='width:487pt'>A Comprehensive View of the
  Biases of Toxicity and Sentiment Analysis Methods Towards Utterances with
  African American English Expressions</td>
  <td class=xl65 width=649 style='width:487pt'>Language is a dynamic aspect of
  our culture that changes when expressed in different
  technologies/communities. Online social networks have enabled the diffusion
  and evolution of different dialects, including African American English
  (AAE). However, this increased usage is not without barriers. One particular
  barrier is how sentiment (Vader, TextBlob, and Flair) and toxicity (Google's
  Perspective and the open-source Detoxify) methods present biases towards
  utterances with AAE expressions. Consider Google's Perspective to understand
  bias. Here, an utterance such as ``All n*ggers deserve to die respectfully.
  The police murder us.'' it reaches a higher toxicity than ``African-Americans
  deserve to die respectfully. The police murder us.''. This score difference
  likely arises because the tool cannot understand the re-appropriation of the
  term ``n*gger''. One explanation for this bias is that AI models are trained
  on limited datasets, and using such a term in training data is more likely to
  appear in a toxic utterance. While this may be plausible, the tool will make
  mistakes regardless. Here, we study bias on two Web-based (YouTube and
  Twitter) datasets and two spoken English datasets. Our analysis shows how
  most models present biases towards AAE in most settings. We isolate the
  impact of AAE expression usage via linguistic control features from the
  Linguistic Inquiry and Word Count (LIWC) software, grammatical control
  features extracted via Part-of-Speech (PoS) tagging from Natural Language
  Processing (NLP) models, and the semantic of utterances by comparing sentence
  embeddings from recent language models. We present consistent results on how
  a heavy usage of AAE expressions may cause the speaker to be considered
  substantially more toxic, even when speaking about nearly the same subject.
  Our study complements similar analyses focusing on small datasets and/or one
  method only.</td>
  <td colspan=2 style='mso-ignore:colspan'>23 January, 2024</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>3178</td>
  <td align=right>2401.13481</td>
  <td class=xl65 width=649 style='width:487pt'>How AI Ideas Affect the
  Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large,
  Dynamic Experiment</td>
  <td class=xl65 width=649 style='width:487pt'>Exposure to large language model
  output is rapidly increasing. How will seeing AI-generated ideas affect human
  ideas? We conducted an experiment (800+ participants, 40+ countries) where
  participants viewed creative ideas that were from ChatGPT or prior experimental
  participants and then brainstormed their own idea. We varied the number of
  AI-generated examples (none, low, or high exposure) and if the examples were
  labeled as 'AI' (disclosure). Our dynamic experiment design -- ideas from
  prior participants in an experimental condition are used as stimuli for
  future participants in the same experimental condition -- speaks to the
  interdependent process of cultural creation: creative ideas are built upon
  prior ideas. Hence, we capture the compounding effects of having LLMs 'in the
  culture loop'. We find that high AI exposure (but not low AI exposure) did
  not affect the creativity of individual ideas but did increase the average
  amount and rate of change of collective idea diversity. AI made ideas different,
  not better. There were no main effects of disclosure. We also found that
  self-reported creative people were less influenced by knowing an idea was
  from AI and that participants may knowingly adopt AI ideas when the task is
  difficult. Our findings suggest that introducing AI ideas may increase
  collective diversity but not individual creativity.</td>
  <td>4 July, 2024</td>
  <td></td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>3177</td>
  <td align=right>2401.13554</td>
  <td class=xl65 width=649 style='width:487pt'>PanAf20K: A Large Video Dataset
  for Wild Ape Detection and Behaviour Recognition</td>
  <td class=xl65 width=649 style='width:487pt'>We present the PanAf20K dataset,
  the largest and most diverse open-access annotated video dataset of great
  apes in their natural environment. It comprises more than 7 million frames
  across ~20,000 camera trap videos of chimpanzees and gorillas collected at 14
  field sites in tropical Africa as part of the Pan African Programme: The
  Cultured Chimpanzee. The footage is accompanied by a rich set of annotations
  and benchmarks making it suitable for training and testing a variety of
  challenging and ecologically important computer vision tasks including ape
  detection and behaviour recognition. Furthering AI analysis of camera trap
  information is critical given the International Union for Conservation of
  Nature now lists all species in the great ape family as either Endangered or
  Critically Endangered. We hope the dataset can form a solid basis for
  engagement of the AI community to improve performance, efficiency, and result
  interpretation in order to support assessments of great ape presence,
  abundance, distribution, and behaviour and thereby aid conservation efforts.</td>
  <td colspan=2 style='mso-ignore:colspan'>31 January, 2024</td>
 </tr>
 <tr height=249 style='height:187.0pt'>
  <td height=249 align=right style='height:187.0pt'>3169</td>
  <td align=right>2401.14425</td>
  <td class=xl65 width=649 style='width:487pt'>No Longer Trending on
  Artstation: Prompt Analysis of Generative AI Art</td>
  <td class=xl65 width=649 style='width:487pt'>Image generation using
  generative AI is rapidly becoming a major new source of visual media, with
  billions of AI generated images created using diffusion models such as Stable
  Diffusion and Midjourney over the last few years. In this paper we collect
  and analyse over 3 million prompts and the images they generate. Using
  natural language processing, topic analysis and visualisation methods we aim
  to understand collectively how people are using text prompts, the impact of
  these systems on artists, and more broadly on the visual cultures they
  promote. Our study shows that prompting focuses largely on surface
  aesthetics, reinforcing cultural norms, popular conventional representations
  and imagery. We also find that many users focus on popular topics (such as
  making colouring books, fantasy art, or Christmas cards), suggesting that the
  dominant use for the systems analysed is recreational rather than artistic.</td>
  <td colspan=2 style='mso-ignore:colspan'>24 January, 2024</td>
 </tr>
 <tr height=408 style='height:306.0pt'>
  <td height=408 align=right style='height:306.0pt'>3141</td>
  <td align=right>2401.16727</td>
  <td class=xl65 width=649 style='width:487pt'>Recent Advances in Hate Speech
  Moderation: Multimodality and the Role of Large Models</td>
  <td class=xl65 width=649 style='width:487pt'>In the evolving landscape of
  online communication, moderating hate speech (HS) presents an intricate
  challenge, compounded by the multimodal nature of digital content. This
  comprehensive survey delves into the recent strides in HS moderation,
  spotlighting the burgeoning role of large language models (LLMs) and large
  multimodal models (LMMs). Our exploration begins with a thorough analysis of
  current literature, revealing the nuanced interplay between textual, visual,
  and auditory elements in propagating HS. We uncover a notable trend towards
  integrating these modalities, primarily due to the complexity and subtlety
  with which HS is disseminated. A significant emphasis is placed on the
  advances facilitated by LLMs and LMMs, which have begun to redefine the
  boundaries of detection and moderation capabilities. We identify existing
  gaps in research, particularly in the context of underrepresented languages
  and cultures, and the need for solutions to handle low-resource settings. The
  survey concludes with a forward-looking perspective, outlining potential
  avenues for future research, including the exploration of novel AI
  methodologies, the ethical governance of AI in moderation, and the
  development of more nuanced, context-aware systems. This comprehensive
  overview aims to catalyze further research and foster a collaborative effort
  towards more sophisticated, responsible, and human-centric approaches to HS
  moderation in the digital era. WARNING: This paper contains offensive
  examples.</td>
  <td colspan=2 style='mso-ignore:colspan'>30 October, 2024</td>
 </tr>
 <tr height=385 style='height:289.0pt'>
  <td height=385 align=right style='height:289.0pt'>3137</td>
  <td align=right>2401.17428</td>
  <td class=xl65 width=649 style='width:487pt'>Metaverse Perspectives from
  Japan: A Participatory Speculative Design Case Study</td>
  <td class=xl65 width=649 style='width:487pt'>Currently, the development of
  the metaverse lies in the hands of industry. Citizens have little influence
  on this process. Instead, to do justice to the pluralism of (digital)
  societies, we should strive for an open discourse including many different
  perspectives on the metaverse and its core technologies such as AI. We
  utilize a participatory speculative design (PSD) approach to explore Japanese
  citizens' perspectives on future metaverse societies, as well as social and
  ethical implications. Our contributions are twofold. Firstly, we demonstrate
  the effectiveness of PSD in engaging citizens in critical discourse on
  emerging technologies like the metaverse, offering our workshop framework as
  a methodological contribution. Secondly, we identify key themes from
  participants' perspectives, providing insights for culturally sensitive
  design and development of virtual environments. Our analysis shows that
  participants imagine the metaverse to have the potential to solve a variety
  of societal issues; for example, breaking down barriers of physical
  environments for communication, social interaction, crisis preparation, and
  political participation, or tackling identity-related issues. Regarding
  future metaverse societies, participants' imaginations raise critical
  questions about human-AI relations, technical solutionism, politics and
  technology, globalization and local cultures, and immersive technologies. We
  discuss implications and contribute to expanding conversations on metaverse
  developments.</td>
  <td colspan=2 style='mso-ignore:colspan'>30 January, 2024</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>3134</td>
  <td align=right>2401.17882</td>
  <td class=xl65 width=649 style='width:487pt'>I Think, Therefore I am:
  Benchmarking Awareness of Large Language Models Using AwareBench</td>
  <td class=xl65 width=649 style='width:487pt'>Do large language models (LLMs)
  exhibit any forms of awareness similar to humans? In this paper, we introduce
  AwareBench, a benchmark designed to evaluate awareness in LLMs. Drawing from
  theories in psychology and philosophy, we define awareness in LLMs as the
  ability to understand themselves as AI models and to exhibit social
  intelligence. Subsequently, we categorize awareness in LLMs into five
  dimensions, including capability, mission, emotion, culture, and perspective.
  Based on this taxonomy, we create a dataset called AwareEval, which contains
  binary, multiple-choice, and open-ended questions to assess LLMs'
  understandings of specific awareness dimensions. Our experiments, conducted
  on 13 LLMs, reveal that the majority of them struggle to fully recognize
  their capabilities and missions while demonstrating decent social
  intelligence. We conclude by connecting awareness of LLMs with AI alignment
  and safety, emphasizing its significance to the trustworthy and ethical
  development of LLMs. Our dataset and code are available at
  https://github.com/HowieHwong/Awareness-in-LLM.</td>
  <td colspan=2 style='mso-ignore:colspan'>16 February, 2024</td>
 </tr>
 <tr height=385 style='height:289.0pt'>
  <td height=385 align=right style='height:289.0pt'>3093</td>
  <td align=right>2402.0268</td>
  <td class=xl65 width=649 style='width:487pt'>Large Language Models are
  Geographically Biased</td>
  <td class=xl65 width=649 style='width:487pt'>Large Language Models (LLMs)
  inherently carry the biases contained in their training corpora, which can
  lead to the perpetuation of societal harm. As the impact of these foundation
  models grows, understanding and evaluating their biases becomes crucial to
  achieving fairness and accuracy. We propose to study what LLMs know about the
  world we live in through the lens of geography. This approach is particularly
  powerful as there is ground truth for the numerous aspects of human life that
  are meaningfully projected onto geographic space such as culture, race,
  language, politics, and religion. We show various problematic geographic
  biases, which we define as systemic errors in geospatial predictions.
  Initially, we demonstrate that LLMs are capable of making accurate zero-shot
  geospatial predictions in the form of ratings that show strong monotonic
  correlation with ground truth (Spearman's $ρ$ of up to 0.89). We then show
  that LLMs exhibit common biases across a range of objective and subjective
  topics. In particular, LLMs are clearly biased against locations with lower
  socioeconomic conditions (e.g. most of Africa) on a variety of sensitive
  subjective topics such as attractiveness, morality, and intelligence
  (Spearman's $ρ$ of up to 0.70). Finally, we introduce a bias score to
  quantify this and find that there is significant variation in the magnitude
  of bias across existing LLMs. Code is available on the project website:
  https://rohinmanvi.github.io/GeoLLM</td>
  <td colspan=2 style='mso-ignore:colspan'>5 October, 2024</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>3091</td>
  <td align=right>2402.03177</td>
  <td class=xl65 width=649 style='width:487pt'>CIDAR: Culturally Relevant
  Instruction Dataset For Arabic</td>
  <td class=xl65 width=649 style='width:487pt'>Instruction tuning has emerged
  as a prominent methodology for teaching Large Language Models (LLMs) to
  follow instructions. However, current instruction datasets predominantly
  cater to English or are derived from English-dominated LLMs, resulting in
  inherent biases toward Western culture. This bias significantly impacts the
  linguistic structures of non-English languages such as Arabic, which has a
  distinct grammar reflective of the diverse cultures across the Arab region.
  This paper addresses this limitation by introducing CIDAR:
  https://hf.co/datasets/arbml/CIDAR, the first open Arabic instruction-tuning
  dataset culturally-aligned by human reviewers. CIDAR contains 10,000
  instruction and output pairs that represent the Arab region. We discuss the
  cultural relevance of CIDAR via the analysis and comparison to other models
  fine-tuned on other datasets. Our experiments show that CIDAR can help enrich
  research efforts in aligning LLMs with the Arabic culture. All the code is
  available at https://github.com/ARBML/CIDAR.</td>
  <td colspan=2 style='mso-ignore:colspan'>5 February, 2024</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>3065</td>
  <td align=right>2402.05374</td>
  <td class=xl65 width=649 style='width:487pt'>CIC: A Framework for
  Culturally-Aware Image Captioning</td>
  <td class=xl65 width=649 style='width:487pt'>Image Captioning generates
  descriptive sentences from images using Vision-Language Pre-trained models
  (VLPs) such as BLIP, which has improved greatly. However, current methods
  lack the generation of detailed descriptive captions for the cultural
  elements depicted in the images, such as the traditional clothing worn by
  people from Asian cultural groups. In this paper, we propose a new framework,
  Culturally-aware Image Captioning (CIC), that generates captions and
  describes cultural elements extracted from cultural visual elements in images
  representing cultures. Inspired by methods combining visual modality and
  Large Language Models (LLMs) through appropriate prompts, our framework (1)
  generates questions based on cultural categories from images, (2) extracts
  cultural visual elements from Visual Question Answering (VQA) using generated
  questions, and (3) generates culturally-aware captions using LLMs with the
  prompts. Our human evaluation conducted on 45 participants from 4 different
  cultural groups with a high understanding of the corresponding culture shows
  that our proposed framework generates more culturally descriptive captions
  when compared to the image captioning baseline based on VLPs. Resources can
  be found at https://shane3606.github.io/cic..</td>
  <td colspan=2 style='mso-ignore:colspan'>9 December, 2024</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>3058</td>
  <td align=right>2402.06015</td>
  <td class=xl65 width=649 style='width:487pt'>Exploring Visual Culture
  Awareness in GPT-4V: A Comprehensive Probing</td>
  <td class=xl65 width=649 style='width:487pt'>Pretrained large Vision-Language
  models have drawn considerable interest in recent years due to their
  remarkable performance. Despite considerable efforts to assess these models
  from diverse perspectives, the extent of visual cultural awareness in the state-of-the-art
  GPT-4V model remains unexplored. To tackle this gap, we extensively probed
  GPT-4V using the MaRVL benchmark dataset, aiming to investigate its
  capabilities and limitations in visual understanding with a focus on cultural
  aspects. Specifically, we introduced three visual related tasks, i.e. caption
  classification, pairwise captioning, and culture tag selection, to
  systematically delve into fine-grained visual cultural evaluation.
  Experimental results indicate that GPT-4V excels at identifying cultural
  concepts but still exhibits weaker performance in low-resource languages,
  such as Tamil and Swahili. Notably, through human evaluation, GPT-4V proves
  to be more culturally relevant in image captioning tasks than the original
  MaRVL human annotations, suggesting a promising solution for future visual
  cultural benchmark construction.</td>
  <td colspan=2 style='mso-ignore:colspan'>15 February, 2024</td>
 </tr>
 <tr height=431 style='height:323.0pt'>
  <td height=431 align=right style='height:323.0pt'>3039</td>
  <td align=right>2402.08242</td>
  <td class=xl65 width=649 style='width:487pt'>Towards Equitable Agile Research
  and Development of AI and Robotics</td>
  <td class=xl65 width=649 style='width:487pt'>Machine Learning (ML) and
  'Artificial Intelligence' ('AI') methods tend to replicate and amplify
  existing biases and prejudices, as do Robots with AI. For example, robots
  with facial recognition have failed to identify Black Women as human, while
  others have categorized people, such as Black Men, as criminals based on
  appearance alone. A 'culture of modularity' means harms are perceived as 'out
  of scope', or someone else's responsibility, throughout employment positions
  in the 'AI supply chain'. Incidents are routine enough (incidentdatabase.ai
  lists over 2000 examples) to indicate that few organizations are capable of
  completely respecting peoples' rights; meeting claimed equity, diversity, and
  inclusion (EDI or DEI) goals; or recognizing and then addressing such
  failures in their organizations and artifacts. We propose a framework for
  adapting widely practiced Research and Development (R&amp;D) project
  management methodologies to build organizational equity capabilities and
  better integrate known evidence-based best practices. We describe how project
  teams can organize and operationalize the most promising practices, skill
  sets, organizational cultures, and methods to detect and address rights-based
  fairness, equity, accountability, and ethical problems as early as possible
  when they are often less harmful and easier to mitigate; then monitor for
  unforeseen incidents to adaptively and constructively address them. Our
  primary example adapts an Agile development process based on Scrum, one of
  the most widely adopted approaches to organizing R&amp;D teams. We also
  discuss limitations of our proposed framework and future research directions.</td>
  <td colspan=2 style='mso-ignore:colspan'>13 February, 2024</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>3028</td>
  <td align=right>2402.09369</td>
  <td class=xl65 width=649 style='width:487pt'>Massively Multi-Cultural
  Knowledge Acquisition &amp; LM Benchmarking</td>
  <td class=xl65 width=649 style='width:487pt'>Pretrained large language models
  have revolutionized many applications but still face challenges related to
  cultural bias and a lack of cultural commonsense knowledge crucial for
  guiding cross-culture communication and interactions. Recognizing the shortcomings
  of existing methods in capturing the diverse and rich cultures across the
  world, this paper introduces a novel approach for massively multicultural
  knowledge acquisition. Specifically, our method strategically navigates from
  densely informative Wikipedia documents on cultural topics to an extensive
  network of linked pages. Leveraging this valuable source of data collection,
  we construct the CultureAtlas dataset, which covers a wide range of
  sub-country level geographical regions and ethnolinguistic groups, with data
  cleaning and preprocessing to ensure textual assertion sentence
  self-containment, as well as fine-grained cultural profile information
  extraction. Our dataset not only facilitates the evaluation of language model
  performance in culturally diverse contexts but also serves as a foundational
  tool for the development of culturally sensitive and aware language models.
  Our work marks an important step towards deeper understanding and bridging
  the gaps of cultural disparities in AI, to promote a more inclusive and
  balanced representation of global cultures in the digital domain.</td>
  <td colspan=2 style='mso-ignore:colspan'>14 February, 2024</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>3007</td>
  <td align=right>2402.10689</td>
  <td class=xl65 width=649 style='width:487pt'>Cultural Commonsense Knowledge
  for Intercultural Dialogues</td>
  <td class=xl65 width=649 style='width:487pt'>Despite recent progress, large
  language models (LLMs) still face the challenge of appropriately reacting to
  the intricacies of social and cultural conventions. This paper presents
  MANGO, a methodology for distilling high-accuracy, high-recall assertions of
  cultural knowledge. We judiciously and iteratively prompt LLMs for this
  purpose from two entry points, concepts and cultures. Outputs are
  consolidated via clustering and generative summarization. Running the MANGO
  method with GPT-3.5 as underlying LLM yields 167K high-accuracy assertions
  for 30K concepts and 11K cultures, surpassing prior resources by a large
  margin in quality and size. In an extrinsic evaluation for intercultural
  dialogues, we explore augmenting dialogue systems with cultural knowledge
  assertions. Notably, despite LLMs inherently possessing cultural knowledge,
  we find that adding knowledge from MANGO improves the overall quality,
  specificity, and cultural sensitivity of dialogue responses, as judged by
  human annotators. Data and code are available for download.</td>
  <td colspan=2 style='mso-ignore:colspan'>23 July, 2024</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>3001</td>
  <td align=right>2402.10946</td>
  <td class=xl65 width=649 style='width:487pt'>CultureLLM: Incorporating
  Cultural Differences into Large Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>Large language models (LLMs) are
  reported to be partial to certain cultures owing to the training data
  dominance from the English corpora. Since multilingual cultural data are
  often expensive to collect, existing efforts handle this by prompt
  engineering or culture-specific pre-training. However, they might overlook
  the knowledge deficiency of low-resource culture and require extensive
  computing resources. In this paper, we propose CultureLLM, a cost-effective
  solution to incorporate cultural differences into LLMs. CultureLLM adopts
  World Value Survey (WVS) as seed data and generates semantically equivalent
  training data via the proposed semantic data augmentation. Using only 50 seed
  samples from WVS with augmented data, we fine-tune culture-specific LLMs and
  one unified model (CultureLLM-One) for 9 cultures covering rich and
  low-resource languages. Extensive experiments on 60 culture-related datasets
  demonstrate that CultureLLM significantly outperforms various counterparts
  such as GPT-3.5 (by 8.1%) and Gemini Pro (by 9.5%) with comparable
  performance to GPT-4 or even better. Our human study shows that the generated
  samples are semantically equivalent to the original samples, providing an
  effective solution for LLMs augmentation. Code is released at
  https://github.com/Scarelette/CultureLLM.</td>
  <td colspan=2 style='mso-ignore:colspan'>3 December, 2024</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>2991</td>
  <td align=right>2402.11333</td>
  <td class=xl65 width=649 style='width:487pt'>Social Norms in Cinema: A
  Cross-Cultural Analysis of Shame, Pride and Prejudice</td>
  <td class=xl65 width=649 style='width:487pt'>Shame and pride are social
  emotions expressed across cultures to motivate and regulate people's
  thoughts, feelings, and behaviors. In this paper, we introduce the first
  cross-cultural dataset of over 10k shame/pride-related expressions, with
  underlying social expectations from ~5.4K Bollywood and Hollywood movies. We
  examine how and why shame and pride are expressed across cultures using a
  blend of psychology-informed language analysis combined with large language
  models. We find significant cross-cultural differences in shame and pride
  expression aligning with known cultural tendencies of the USA and India --
  e.g., in Hollywood, shame-expressions predominantly discuss self whereas
  Bollywood discusses shame toward others. Pride in Hollywood is individualistic
  with more self-referential singular pronouns such as I and my whereas in
  Bollywood, pride is collective with higher use of self-referential plural
  pronouns such as we and our. Lastly, women are more sanctioned across
  cultures and for violating similar social expectations e.g. promiscuity.</td>
  <td colspan=2 style='mso-ignore:colspan'>15 October, 2024</td>
 </tr>
 <tr height=136 style='height:102.0pt'>
  <td height=136 align=right style='height:102.0pt'>2983</td>
  <td align=right>2402.1171</td>
  <td class=xl65 width=649 style='width:487pt'>A Note on Bias to Complete</td>
  <td class=xl65 width=649 style='width:487pt'>Minimizing social bias
  strengthens societal bonds, promoting shared understanding and better
  decision-making. We revisit the definition of bias by discovering new bias
  types (e.g., societal status) in dynamic environments and describe them
  relative to context, such as culture, region, time, and personal background.
  Our framework includes eight hypotheses about bias and a minimizing bias
  strategy for each assumption as well as five methods as proposed solutions in
  LLM. The realization of the framework is yet to be completed.</td>
  <td colspan=2 style='mso-ignore:colspan'>18 February, 2024</td>
 </tr>
 <tr height=408 style='height:306.0pt'>
  <td height=408 align=right style='height:306.0pt'>2969</td>
  <td align=right>2402.13231</td>
  <td class=xl65 width=649 style='width:487pt'>Investigating Cultural Alignment
  of Large Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>The intricate relationship
  between language and culture has long been a subject of exploration within
  the realm of linguistic anthropology. Large Language Models (LLMs), promoted
  as repositories of collective human knowledge, raise a pivotal question: do
  these models genuinely encapsulate the diverse knowledge adopted by different
  cultures? Our study reveals that these models demonstrate greater cultural
  alignment along two dimensions -- firstly, when prompted with the dominant
  language of a specific culture, and secondly, when pretrained with a refined
  mixture of languages employed by that culture. We quantify cultural alignment
  by simulating sociological surveys, comparing model responses to those of
  actual survey participants as references. Specifically, we replicate a survey
  conducted in various regions of Egypt and the United States through prompting
  LLMs with different pretraining data mixtures in both Arabic and English with
  the personas of the real respondents and the survey questions. Further
  analysis reveals that misalignment becomes more pronounced for
  underrepresented personas and for culturally sensitive topics, such as those
  probing social values. Finally, we introduce Anthropological Prompting, a
  novel method leveraging anthropological reasoning to enhance cultural
  alignment. Our study emphasizes the necessity for a more balanced
  multilingual pretraining dataset to better represent the diversity of human
  experience and the plurality of different cultures with many implications on
  the topic of cross-lingual transfer.</td>
  <td>6 July, 2024</td>
  <td></td>
 </tr>
 <tr height=431 style='height:323.0pt'>
  <td height=431 align=right style='height:323.0pt'>2958</td>
  <td align=right>2402.13605</td>
  <td class=xl65 width=649 style='width:487pt'>KorNAT: LLM Alignment Benchmark
  for Korean Social Values and Common Knowledge</td>
  <td class=xl65 width=649 style='width:487pt'>For Large Language Models (LLMs)
  to be effectively deployed in a specific country, they must possess an
  understanding of the nation's culture and basic knowledge. To this end, we
  introduce National Alignment, which measures an alignment between an LLM and
  a targeted country from two aspects: social value alignment and common
  knowledge alignment. Social value alignment evaluates how well the model
  understands nation-specific social values, while common knowledge alignment
  examines how well the model captures basic knowledge related to the nation.
  We constructed KorNAT, the first benchmark that measures national alignment
  with South Korea. For the social value dataset, we obtained ground truth
  labels from a large-scale survey involving 6,174 unique Korean participants.
  For the common knowledge dataset, we constructed samples based on Korean
  textbooks and GED reference materials. KorNAT contains 4K and 6K
  multiple-choice questions for social value and common knowledge,
  respectively. Our dataset creation process is meticulously designed and based
  on statistical sampling theory and was refined through multiple rounds of
  human review. The experiment results of seven LLMs reveal that only a few
  models met our reference score, indicating a potential for further
  enhancement. KorNAT has received government approval after passing an
  assessment conducted by a government-affiliated organization dedicated to
  evaluating dataset quality. Samples and detailed evaluation protocols of our
  dataset can be found in https://huggingface.co/datasets/jiyounglee0523/KorNAT
  .</td>
  <td colspan=2 style='mso-ignore:colspan'>5 June, 2024</td>
 </tr>
 <tr height=227 style='height:170.0pt'>
  <td height=227 align=right style='height:170.0pt'>2953</td>
  <td align=right>2402.13914</td>
  <td class=xl65 width=649 style='width:487pt'>Position: Explain to Question
  not to Justify</td>
  <td class=xl65 width=649 style='width:487pt'>Explainable Artificial
  Intelligence (XAI) is a young but very promising field of research.
  Unfortunately, the progress in this field is currently slowed down by
  divergent and incompatible goals. We separate various threads tangled within
  the area of XAI into two complementary cultures of human/value-oriented
  explanations (BLUE XAI) and model/validation-oriented explanations (RED XAI).
  This position paper argues that the area of RED XAI is currently
  under-explored, i.e., more methods for explainability are desperately needed
  to question models (e.g., extract knowledge from well-performing models as
  well as spotting and fixing bugs in faulty models), and the area of RED XAI
  hides great opportunities and potential for important research necessary to
  ensure the safety of AI systems. We conclude this paper by presenting
  promising challenges in this area.</td>
  <td colspan=2 style='mso-ignore:colspan'>28 June, 2024</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>2945</td>
  <td align=right>2402.14589</td>
  <td class=xl65 width=649 style='width:487pt'>Avoiding an AI-imposed Taylor's
  Version of all music history</td>
  <td class=xl65 width=649 style='width:487pt'>As future musical AIs adhere
  closely to human music, they may form their own attachments to particular
  human artists in their databases, and these biases may in the worst case lead
  to potential existential threats to all musical history. AI super fans may
  act to corrupt the historical record and extant recordings in favour of their
  own preferences, and preservation of the diversity of world music culture may
  become even more of a pressing issue than the imposition of 12 tone equal
  temperament or other Western homogenisations. We discuss the technical
  capability of AI cover software and produce Taylor's Versions of famous
  tracks from Western pop history as provocative examples; the quality of these
  productions does not affect the overall argument (which might even see a
  future AI try to impose the sound of paperclips onto all existing audio
  files, let alone Taylor Swift). We discuss some potential defenses against
  the danger of future musical monopolies, whilst analysing the feasibility of
  a maximal 'Taylor Swiftication' of the complete musical record.</td>
  <td colspan=2 style='mso-ignore:colspan'>5 February, 2024</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>2871</td>
  <td align=right>2403.01031</td>
  <td class=xl65 width=649 style='width:487pt'>Peacock: A Family of Arabic
  Multimodal Large Language Models and Benchmarks</td>
  <td class=xl65 width=649 style='width:487pt'>Multimodal large language models
  (MLLMs) have proven effective in a wide range of tasks requiring complex
  reasoning and linguistic comprehension. However, due to a lack of
  high-quality multimodal resources in languages other than English, success of
  MLLMs remains relatively limited to English-based settings. This poses
  significant challenges in developing comparable models for other languages,
  including even those with large speaker populations such as Arabic. To
  alleviate this challenge, we introduce a comprehensive family of Arabic
  MLLMs, dubbed \textit{Peacock}, with strong vision and language capabilities.
  Through comprehensive qualitative and quantitative analysis, we demonstrate
  the solid performance of our models on various visual reasoning tasks and
  further show their emerging dialectal potential. Additionally, we introduce
  ~\textit{Henna}, a new benchmark specifically designed for assessing MLLMs on
  aspects related to Arabic culture, setting the first stone for
  culturally-aware Arabic MLLMs.The GitHub repository for the \textit{Peacock}
  project is available at \url{https://github.com/UBC-NLP/peacock}.</td>
  <td colspan=2 style='mso-ignore:colspan'>24 May, 2024</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>2835</td>
  <td align=right>2403.04964</td>
  <td class=xl65 width=649 style='width:487pt'>Tell me the truth: A system to
  measure the trustworthiness of Large Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>Large Language Models (LLM) have
  taken the front seat in most of the news since November 2022, when ChatGPT
  was introduced. After more than one year, one of the major reasons companies
  are resistant to adopting them is the limited confidence they have in the
  trustworthiness of those systems. In a study by (Baymard, 2023), ChatGPT-4
  showed an 80.1% false-positive error rate in identifying usability issues on
  websites. A Jan. '24 study by JAMA Pediatrics found that ChatGPT has an
  accuracy rate of 17% percent when diagnosing pediatric medical cases (Barile
  et al., 2024). But then, what is &quot;trust&quot;? Trust is a relative,
  subject condition that can change based on culture, domain, individuals. And
  then, given a domain, how can the trustworthiness of a system be measured? In
  this paper, I present a systematic approach to measure trustworthiness based
  on a predefined ground truth, represented as a knowledge graph of the domain.
  The approach is a process with humans in the loop to validate the representation
  of the domain and to fine-tune the system. Measuring the trustworthiness
  would be essential for all the entities operating in critical environments,
  such as healthcare, defense, finance, but it would be very relevant for all
  the users of LLMs.</td>
  <td colspan=2 style='mso-ignore:colspan'>11 March, 2024</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>2812</td>
  <td align=right>2403.06412</td>
  <td class=xl65 width=649 style='width:487pt'>CLIcK: A Benchmark Dataset of
  Cultural and Linguistic Intelligence in Korean</td>
  <td class=xl65 width=649 style='width:487pt'>Despite the rapid development of
  large language models (LLMs) for the Korean language, there remains an
  obvious lack of benchmark datasets that test the requisite Korean cultural
  and linguistic knowledge. Because many existing Korean benchmark datasets are
  derived from the English counterparts through translation, they often
  overlook the different cultural contexts. For the few benchmark datasets that
  are sourced from Korean data capturing cultural knowledge, only narrow tasks
  such as bias and hate speech detection are offered. To address this gap, we
  introduce a benchmark of Cultural and Linguistic Intelligence in Korean
  (CLIcK), a dataset comprising 1,995 QA pairs. CLIcK sources its data from
  official Korean exams and textbooks, partitioning the questions into eleven
  categories under the two main categories of language and culture. For each
  instance in CLIcK, we provide fine-grained annotation of which cultural and
  linguistic knowledge is required to answer the question correctly. Using
  CLIcK, we test 13 language models to assess their performance. Our evaluation
  uncovers insights into their performances across the categories, as well as
  the diverse factors affecting their comprehension. CLIcK offers the first
  large-scale comprehensive Korean-centric analysis of LLMs' proficiency in
  Korean culture and language.</td>
  <td>4 July, 2024</td>
  <td></td>
 </tr>
 <tr height=249 style='height:187.0pt'>
  <td height=249 align=right style='height:187.0pt'>2805</td>
  <td align=right>2403.06865</td>
  <td class=xl65 width=649 style='width:487pt'>On the Preservation of Africa's
  Cultural Heritage in the Age of Artificial Intelligence</td>
  <td class=xl65 width=649 style='width:487pt'>In this paper we delve into the
  historical evolution of data as a fundamental element in communication and
  knowledge transmission. The paper traces the stages of knowledge
  dissemination from oral traditions to the digital era, highlighting the
  significance of languages and cultural diversity in this progression. It also
  explores the impact of digital technologies on memory, communication, and
  cultural preservation, emphasizing the need for promoting a culture of the
  digital (rather than a digital culture) in Africa and beyond. Additionally,
  it discusses the challenges and opportunities presented by data biases in AI
  development, underscoring the importance of creating diverse datasets for
  equitable representation. We advocate for investing in data as a crucial raw
  material for fostering digital literacy, economic development, and, above
  all, cultural preservation in the digital age.</td>
  <td colspan=2 style='mso-ignore:colspan'>13 March, 2024</td>
 </tr>
 <tr height=385 style='height:289.0pt'>
  <td height=385 align=right style='height:289.0pt'>2795</td>
  <td align=right>2403.08036</td>
  <td class=xl65 width=649 style='width:487pt'>A Review of Cybersecurity
  Incidents in the Food and Agriculture Sector</td>
  <td class=xl65 width=649 style='width:487pt'>The increasing utilization of
  emerging technologies in the Food &amp; Agriculture (FA) sector has
  heightened the need for security to minimize cyber risks. Considering this
  aspect, this manuscript reviews disclosed and documented cybersecurity
  incidents in the FA sector. For this purpose, thirty cybersecurity incidents
  were identified, which took place between July 2011 and April 2023. The
  details of these incidents are reported from multiple sources such as: the
  private industry and flash notifications generated by the Federal Bureau of
  Investigation (FBI), internal reports from the affected organizations, and
  available media sources. Considering the available information, a brief
  description of the security threat, ransom amount, and impact on the organization
  are discussed for each incident. This review reports an increased frequency
  of cybersecurity threats to the FA sector. To minimize these cyber risks,
  popular cybersecurity frameworks and recent agriculture-specific
  cybersecurity solutions are also discussed. Further, the need for AI
  assurance in the FA sector is explained, and the Farmer-Centered AI (FCAI)
  framework is proposed. The main aim of the FCAI framework is to support
  farmers in decision-making for agricultural production, by incorporating AI
  assurance. Lastly, the effects of the reported cyber incidents on other
  critical infrastructures, food security, and the economy are noted, along
  with specifying the open issues for future development.</td>
  <td colspan=2 style='mso-ignore:colspan'>12 March, 2024</td>
 </tr>
 <tr height=431 style='height:323.0pt'>
  <td height=431 align=right style='height:323.0pt'>2787</td>
  <td align=right>2403.08882</td>
  <td class=xl65 width=649 style='width:487pt'>Cultural evolution in
  populations of Large Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>Research in cultural evolution
  aims at providing causal explanations for the change of culture over time.
  Over the past decades, this field has generated an important body of
  knowledge, using experimental, historical, and computational methods. While
  computational models have been very successful at generating testable
  hypotheses about the effects of several factors, such as population structure
  or transmission biases, some phenomena have so far been more complex to
  capture using agent-based and formal models. This is in particular the case
  for the effect of the transformations of social information induced by
  evolved cognitive mechanisms. We here propose that leveraging the capacity of
  Large Language Models (LLMs) to mimic human behavior may be fruitful to
  address this gap. On top of being an useful approximation of human cultural
  dynamics, multi-agents models featuring generative agents are also important
  to study for their own sake. Indeed, as artificial agents are bound to
  participate more and more to the evolution of culture, it is crucial to
  better understand the dynamics of machine-generated cultural evolution. We
  here present a framework for simulating cultural evolution in populations of
  LLMs, allowing the manipulation of variables known to be important in
  cultural evolution, such as network structure, personality, and the way
  social information is aggregated and transformed. The software we developed
  for conducting these simulations is open-source and features an intuitive
  user-interface, which we hope will help to build bridges between the fields
  of cultural evolution and generative artificial intelligence.</td>
  <td colspan=2 style='mso-ignore:colspan'>13 March, 2024</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>2766</td>
  <td align=right>2403.10258</td>
  <td class=xl65 width=649 style='width:487pt'>Is Translation All You Need? A
  Study on Solving Multilingual Tasks with Large Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>Large language models (LLMs)
  have demonstrated multilingual capabilities; yet, they are mostly
  English-centric due to the imbalanced training corpora. Existing works
  leverage this phenomenon to improve their multilingual performances through
  translation, primarily on natural language processing (NLP) tasks. This work
  extends the evaluation from NLP tasks to real user queries and from
  English-centric LLMs to non-English-centric LLMs. While translation into
  English can help improve the performance of multilingual NLP tasks for
  English-centric LLMs, it may not be optimal for all scenarios. For
  culture-related tasks that need deep language understanding, prompting in the
  native language tends to be more promising as it better captures the nuances
  of culture and language. Our experiments reveal varied behaviors among
  different LLMs and tasks in the multilingual context. Therefore, we advocate
  for more comprehensive multilingual evaluation and more efforts toward
  developing multilingual LLMs beyond English-centric ones.</td>
  <td colspan=2 style='mso-ignore:colspan'>20 June, 2024</td>
 </tr>
 <tr height=408 style='height:306.0pt'>
  <td height=408 align=right style='height:306.0pt'>2733</td>
  <td align=right>2403.13187</td>
  <td class=xl65 width=649 style='width:487pt'>Evolutionary Optimization of
  Model Merging Recipes</td>
  <td class=xl65 width=649 style='width:487pt'>We present a novel application
  of evolutionary algorithms to automate the creation of powerful foundation
  models. While model merging has emerged as a promising approach for LLM
  development due to its cost-effectiveness, it currently relies on human intuition
  and domain knowledge, limiting its potential. Here, we propose an
  evolutionary approach that overcomes this limitation by automatically
  discovering effective combinations of diverse open-source models, harnessing
  their collective intelligence without requiring extensive additional training
  data or compute. Our approach operates in both parameter space and data flow
  space, allowing for optimization beyond just the weights of the individual
  models. This approach even facilitates cross-domain merging, generating
  models like a Japanese LLM with Math reasoning capabilities. Surprisingly,
  our Japanese Math LLM achieved state-of-the-art performance on a variety of
  established Japanese LLM benchmarks, even surpassing models with
  significantly more parameters, despite not being explicitly trained for such
  tasks. Furthermore, a culturally-aware Japanese VLM generated through our
  approach demonstrates its effectiveness in describing Japanese
  culture-specific content, outperforming previous Japanese VLMs. This work not
  only contributes new state-of-the-art models back to the open-source
  community, but also introduces a new paradigm for automated model
  composition, paving the way for exploring alternative, efficient approaches
  to foundation model development.</td>
  <td colspan=2 style='mso-ignore:colspan'>19 March, 2024</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>2717</td>
  <td align=right>2403.14651</td>
  <td class=xl65 width=649 style='width:487pt'>DOSA: A Dataset of Social
  Artifacts from Different Indian Geographical Subcultures</td>
  <td class=xl65 width=649 style='width:487pt'>Generative models are
  increasingly being used in various applications, such as text generation,
  commonsense reasoning, and question-answering. To be effective globally,
  these models must be aware of and account for local socio-cultural contexts,
  making it necessary to have benchmarks to evaluate the models for their
  cultural familiarity. Since the training data for LLMs is web-based and the
  Web is limited in its representation of information, it does not capture
  knowledge present within communities that are not on the Web. Thus, these
  models exacerbate the inequities, semantic misalignment, and stereotypes from
  the Web. There has been a growing call for community-centered participatory
  research methods in NLP. In this work, we respond to this call by using
  participatory research methods to introduce $\textit{DOSA}$, the first
  community-generated $\textbf{D}$ataset $\textbf{o}$f 615 $\textbf{S}$ocial
  $\textbf{A}$rtifacts, by engaging with 260 participants from 19 different
  Indian geographic subcultures. We use a gamified framework that relies on
  collective sensemaking to collect the names and descriptions of these
  artifacts such that the descriptions semantically align with the shared
  sensibilities of the individuals from those cultures. Next, we benchmark four
  popular LLMs and find that they show significant variation across regional
  sub-cultures in their ability to infer the artifacts.</td>
  <td colspan=2 style='mso-ignore:colspan'>23 February, 2024</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>2714</td>
  <td align=right>2403.14659</td>
  <td class=xl65 width=649 style='width:487pt'>Social Intelligence Data
  Infrastructure: Structuring the Present and Navigating the Future</td>
  <td class=xl65 width=649 style='width:487pt'>As Natural Language Processing
  (NLP) systems become increasingly integrated into human social life, these
  technologies will need to increasingly rely on social intelligence. Although
  there are many valuable datasets that benchmark isolated dimensions of social
  intelligence, there does not yet exist any body of work to join these threads
  into a cohesive subfield in which researchers can quickly identify research
  gaps and future directions. Towards this goal, we build a Social AI Data
  Infrastructure, which consists of a comprehensive social AI taxonomy and a
  data library of 480 NLP datasets. Our infrastructure allows us to analyze
  existing dataset efforts, and also evaluate language models' performance in
  different social intelligence aspects. Our analyses demonstrate its utility
  in enabling a thorough understanding of current data landscape and providing
  a holistic perspective on potential directions for future dataset
  development. We show there is a need for multifaceted datasets, increased
  diversity in language and culture, more long-tailed social situations, and
  more interactive data in future social intelligence data efforts.</td>
  <td colspan=2 style='mso-ignore:colspan'>27 February, 2024</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>2703</td>
  <td align=right>2403.15088</td>
  <td class=xl65 width=649 style='width:487pt'>CHisIEC: An Information
  Extraction Corpus for Ancient Chinese History</td>
  <td class=xl65 width=649 style='width:487pt'>Natural Language Processing
  (NLP) plays a pivotal role in the realm of Digital Humanities (DH) and serves
  as the cornerstone for advancing the structural analysis of historical and
  cultural heritage texts. This is particularly true for the domains of named
  entity recognition (NER) and relation extraction (RE). In our commitment to
  expediting ancient history and culture, we present the ``Chinese Historical
  Information Extraction Corpus''(CHisIEC). CHisIEC is a meticulously curated
  dataset designed to develop and evaluate NER and RE tasks, offering a
  resource to facilitate research in the field. Spanning a remarkable
  historical timeline encompassing data from 13 dynasties spanning over 1830
  years, CHisIEC epitomizes the extensive temporal range and text heterogeneity
  inherent in Chinese historical documents. The dataset encompasses four
  distinct entity types and twelve relation types, resulting in a meticulously
  labeled dataset comprising 14,194 entities and 8,609 relations. To establish
  the robustness and versatility of our dataset, we have undertaken
  comprehensive experimentation involving models of various sizes and
  paradigms. Additionally, we have evaluated the capabilities of Large Language
  Models (LLMs) in the context of tasks related to ancient Chinese history. The
  dataset and code are available at
  \url{https://github.com/tangxuemei1995/CHisIEC}.</td>
  <td colspan=2 style='mso-ignore:colspan'>20 April, 2024</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>2694</td>
  <td align=right>2403.15412</td>
  <td class=xl65 width=649 style='width:487pt'>Towards Measuring and Modeling
  &quot;Culture&quot; in LLMs: A Survey</td>
  <td class=xl65 width=649 style='width:487pt'>We present a survey of more than
  90 recent papers that aim to study cultural representation and inclusion in
  large language models (LLMs). We observe that none of the studies explicitly
  define &quot;culture, which is a complex, multifaceted concept; instead, they
  probe the models on some specially designed datasets which represent certain
  aspects of &quot;culture&quot;. We call these aspects the proxies of culture,
  and organize them across two dimensions of demographic and semantic proxies.
  We also categorize the probing methods employed. Our analysis indicates that
  only certain aspects of ``culture,'' such as values and objectives, have been
  studied, leaving several other interesting and important facets, especially
  the multitude of semantic domains (Thompson et al., 2020) and aboutness
  (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack
  of robustness of probing techniques and situated studies on the impact of
  cultural mis- and under-representation in LLM-based applications.</td>
  <td colspan=2 style='mso-ignore:colspan'>4 September, 2024</td>
 </tr>
 <tr height=159 style='height:119.0pt'>
  <td height=159 align=right style='height:119.0pt'>2693</td>
  <td align=right>2403.15451</td>
  <td class=xl65 width=649 style='width:487pt'>Towards Enabling FAIR Dataspaces
  Using Large Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>Dataspaces have recently gained
  adoption across various sectors, including traditionally less digitized
  domains such as culture. Leveraging Semantic Web technologies helps to make
  dataspaces FAIR, but their complexity poses a significant challenge to the
  adoption of dataspaces and increases their cost. The advent of Large Language
  Models (LLMs) raises the question of how these models can support the
  adoption of FAIR dataspaces. In this work, we demonstrate the potential of
  LLMs in dataspaces with a concrete example. We also derive a research agenda
  for exploring this emerging field.</td>
  <td colspan=2 style='mso-ignore:colspan'>18 March, 2024</td>
 </tr>
 <tr height=408 style='height:306.0pt'>
  <td height=408 align=right style='height:306.0pt'>2692</td>
  <td align=right>2403.15462</td>
  <td class=xl65 width=649 style='width:487pt'>FUELVISION: A Multimodal Data
  Fusion and Multimodel Ensemble Algorithm for Wildfire Fuels Mapping</td>
  <td class=xl65 width=649 style='width:487pt'>Accurate assessment of fuel
  conditions is a prerequisite for fire ignition and behavior prediction, and
  risk management. The method proposed herein leverages diverse data sources
  including Landsat-8 optical imagery, Sentinel-1 (C-band) Synthetic Aperture
  Radar (SAR) imagery, PALSAR (L-band) SAR imagery, and terrain features to
  capture comprehensive information about fuel types and distributions. An
  ensemble model was trained to predict landscape-scale fuels such as the
  'Scott and Burgan 40' using the as-received Forest Inventory and Analysis
  (FIA) field survey plot data obtained from the USDA Forest Service. However,
  this basic approach yielded relatively poor results due to the inadequate
  amount of training data. Pseudo-labeled and fully synthetic datasets were
  developed using generative AI approaches to address the limitations of ground
  truth data availability. These synthetic datasets were used for augmenting
  the FIA data from California to enhance the robustness and coverage of model
  training. The use of an ensemble of methods including deep learning neural
  networks, decision trees, and gradient boosting offered a fuel mapping
  accuracy of nearly 80\%. Through extensive experimentation and evaluation,
  the effectiveness of the proposed approach was validated for regions of the
  2021 Dixie and Caldor fires. Comparative analyses against high-resolution
  data from the National Agriculture Imagery Program (NAIP) and timber harvest
  maps affirmed the robustness and reliability of the proposed approach, which
  is capable of near-real-time fuel mapping.</td>
  <td colspan=2 style='mso-ignore:colspan'>19 March, 2024</td>
 </tr>
 <tr height=249 style='height:187.0pt'>
  <td height=249 align=right style='height:187.0pt'>2689</td>
  <td align=right>2403.15486</td>
  <td class=xl65 width=649 style='width:487pt'>Sequence-to-Sequence Language
  Models for Character and Emotion Detection in Dream Narratives</td>
  <td class=xl65 width=649 style='width:487pt'>The study of dreams has been
  central to understanding human (un)consciousness, cognition, and culture for
  centuries. Analyzing dreams quantitatively depends on labor-intensive, manual
  annotation of dream narratives. We automate this process through a natural
  language sequence-to-sequence generation framework. This paper presents the
  first study on character and emotion detection in the English portion of the
  open DreamBank corpus of dream narratives. Our results show that language
  models can effectively address this complex task. To get insight into
  prediction performance, we evaluate the impact of model size, prediction
  order of characters, and the consideration of proper names and character
  traits. We compare our approach with a large language model using in-context
  learning. Our supervised models perform better while having 28 times fewer
  parameters. Our model and its generated annotations are made publicly
  available.</td>
  <td colspan=2 style='mso-ignore:colspan'>21 March, 2024</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>2660</td>
  <td align=right>2403.17553</td>
  <td class=xl65 width=649 style='width:487pt'>RuBia: A Russian Language Bias
  Detection Dataset</td>
  <td class=xl65 width=649 style='width:487pt'>Warning: this work contains
  upsetting or disturbing content. Large language models (LLMs) tend to learn
  the social and cultural biases present in the raw pre-training data. To test
  if an LLM's behavior is fair, functional datasets are employed, and due to
  their purpose, these datasets are highly language and culture-specific. In
  this paper, we address a gap in the scope of multilingual bias evaluation by
  presenting a bias detection dataset specifically designed for the Russian
  language, dubbed as RuBia. The RuBia dataset is divided into 4 domains:
  gender, nationality, socio-economic status, and diverse, each of the domains
  is further divided into multiple fine-grained subdomains. Every example in
  the dataset consists of two sentences with the first reinforcing a
  potentially harmful stereotype or trope and the second contradicting it.
  These sentence pairs were first written by volunteers and then validated by
  native-speaking crowdsourcing workers. Overall, there are nearly 2,000 unique
  sentence pairs spread over 19 subdomains in RuBia. To illustrate the
  dataset's purpose, we conduct a diagnostic evaluation of state-of-the-art or
  near-state-of-the-art LLMs and discuss the LLMs' predisposition to social
  biases.</td>
  <td colspan=2 style='mso-ignore:colspan'>26 March, 2024</td>
 </tr>
 <tr height=385 style='height:289.0pt'>
  <td height=385 align=right style='height:289.0pt'>2633</td>
  <td align=right>2403.20216</td>
  <td class=xl65 width=649 style='width:487pt'>Distributed agency in second
  language learning and teaching through generative AI</td>
  <td class=xl65 width=649 style='width:487pt'>Generative AI offers significant
  opportunities for language learning. Tools like ChatGPT can provide informal
  second language practice through chats in written or voice forms, with the
  learner specifying through prompts conversational parameters such as proficiency
  level, language register, and discussion topics. AI can be instructed to give
  corrective feedback, create practice exercises, or develop an extended study
  plan. Instructors can use AI to build learning and assessment materials in a
  variety of media. AI is likely to make immersive technologies more powerful
  and versatile, moving away from scripted interactions. For both learners and
  teachers, it is important to understand the limitations of AI systems that
  arise from their purely statistical model of human language, which limits
  their ability to deal with nuanced social and cultural aspects of language
  use. Additionally, there are ethical concerns over how AI systems are created
  as well as practical constraints in their use, especially for less privileged
  populations. The power and versatility of AI tools are likely to turn them
  into valuable and constant companions in many peoples lives (akin to
  smartphones), creating a close connection that goes beyond simple tool use.
  Ecological theories such as sociomaterialism are helpful in examining the
  shared agency that develops through close user-AI interactions, as are the
  perspectives on human-object relations from Indigenous cultures.</td>
  <td colspan=2 style='mso-ignore:colspan'>31 May, 2024</td>
 </tr>
 <tr height=431 style='height:323.0pt'>
  <td height=431 align=right style='height:323.0pt'>2630</td>
  <td align=right>2404.00411</td>
  <td class=xl65 width=649 style='width:487pt'>Aardvark weather: end-to-end
  data-driven weather forecasting</td>
  <td class=xl65 width=649 style='width:487pt'>Weather forecasting is critical
  for a range of human activities including transportation, agriculture,
  industry, as well as the safety of the general public. Machine learning
  models have the potential to transform the complex weather prediction
  pipeline, but current approaches still rely on numerical weather prediction
  (NWP) systems, limiting forecast speed and accuracy. Here we demonstrate that
  a machine learning model can replace the entire operational NWP pipeline.
  Aardvark Weather, an end-to-end data-driven weather prediction system,
  ingests raw observations and outputs global gridded forecasts and local
  station forecasts. Further, it can be optimised end-to-end to maximise
  performance over quantities of interest. Global forecasts outperform an operational
  NWP baseline for multiple variables and lead times. Local station forecasts
  are skillful up to ten days lead time and achieve comparable and often lower
  errors than a post-processed global NWP baseline and a state-of-the-art
  end-to-end forecasting system with input from human forecasters. These
  forecasts are produced with a remarkably simple neural process model using
  just 8% of the input data and three orders of magnitude less compute than
  existing NWP and hybrid AI-NWP methods. We anticipate that Aardvark Weather
  will be the starting point for a new generation of end-to-end machine
  learning models for medium-range forecasting that will reduce computational
  costs by orders of magnitude and enable the rapid and cheap creation of
  bespoke models for users in a variety of fields, including for the developing
  world where state-of-the-art local models are not currently available.</td>
  <td colspan=2 style='mso-ignore:colspan'>13 July, 2024</td>
 </tr>
 <tr height=475 style='height:356.0pt'>
  <td height=475 align=right style='height:356.0pt'>2623</td>
  <td align=right>2404.0103</td>
  <td class=xl65 width=649 style='width:487pt'>Survey of Bias In Text-to-Image
  Generation: Definition, Evaluation, and Mitigation</td>
  <td class=xl65 width=649 style='width:487pt'>The recent advancement of large
  and powerful models with Text-to-Image (T2I) generation abilities -- such as
  OpenAI's DALLE-3 and Google's Gemini -- enables users to generate
  high-quality images from textual prompts. However, it has become increasingly
  evident that even simple prompts could cause T2I models to exhibit
  conspicuous social bias in generated images. Such bias might lead to both
  allocational and representational harms in society, further marginalizing
  minority groups. Noting this problem, a large body of recent works has been
  dedicated to investigating different dimensions of bias in T2I systems.
  However, an extensive review of these studies is lacking, hindering a
  systematic understanding of current progress and research gaps. We present the
  first extensive survey on bias in T2I generative models. In this survey, we
  review prior studies on dimensions of bias: Gender, Skintone, and
  Geo-Culture. Specifically, we discuss how these works define, evaluate, and
  mitigate different aspects of bias. We found that: (1) while gender and
  skintone biases are widely studied, geo-cultural bias remains under-explored;
  (2) most works on gender and skintone bias investigated occupational
  association, while other aspects are less frequently studied; (3) almost all
  gender bias works overlook non-binary identities in their studies; (4)
  evaluation datasets and metrics are scattered, with no unified framework for
  measuring biases; and (5) current mitigation methods fail to resolve biases
  comprehensively. Based on current limitations, we point out future research
  directions that contribute to human-centric definitions, evaluations, and
  mitigation of biases. We hope to highlight the importance of studying biases
  in T2I systems, as well as encourage future efforts to holistically
  understand and tackle biases, building fair and trustworthy T2I technologies
  for everyone.</td>
  <td colspan=2 style='mso-ignore:colspan'>1 May, 2024</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>2611</td>
  <td align=right>2404.01657</td>
  <td class=xl65 width=649 style='width:487pt'>Release of Pre-Trained Models
  for the Japanese Language</td>
  <td class=xl65 width=649 style='width:487pt'>AI democratization aims to
  create a world in which the average person can utilize AI techniques. To
  achieve this goal, numerous research institutes have attempted to make their
  results accessible to the public. In particular, large pre-trained models trained
  on large-scale data have shown unprecedented potential, and their release has
  had a significant impact. However, most of the released models specialize in
  the English language, and thus, AI democratization in non-English-speaking
  communities is lagging significantly. To reduce this gap in AI access, we
  released Generative Pre-trained Transformer (GPT), Contrastive Language and
  Image Pre-training (CLIP), Stable Diffusion, and Hidden-unit Bidirectional
  Encoder Representations from Transformers (HuBERT) pre-trained in Japanese.
  By providing these models, users can freely interface with AI that aligns
  with Japanese cultural values and ensures the identity of Japanese culture,
  thus enhancing the democratization of AI. Additionally, experiments showed that
  pre-trained models specialized for Japanese can efficiently achieve high
  performance in Japanese tasks.</td>
  <td colspan=2 style='mso-ignore:colspan'>2 April, 2024</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>2608</td>
  <td align=right>2404.01854</td>
  <td class=xl65 width=649 style='width:487pt'>IndoCulture: Exploring
  Geographically-Influenced Cultural Commonsense Reasoning Across Eleven
  Indonesian Provinces</td>
  <td class=xl65 width=649 style='width:487pt'>Although commonsense reasoning
  is greatly shaped by cultural and geographical factors, previous studies have
  predominantly centered on cultures grounded in the English language,
  potentially resulting in an Anglocentric bias. In this paper, we introduce IndoCulture,
  aimed at understanding the influence of geographical factors on language
  model reasoning ability, with a specific emphasis on the diverse cultures
  found within eleven Indonesian provinces. In contrast to prior work that has
  relied on templates (Yin et al., 2022) and online scrapping (Fung et al.,
  2024), we create IndoCulture by asking local people to manually develop a
  cultural context and plausible options, across a set of predefined topics.
  Evaluation of 27 language models reveals several insights: (1) the
  open-weight Llama-3 is competitive with GPT-4, while other open-weight models
  struggle, with accuracies below 50%; (2) there is a general pattern of models
  generally performing better for some provinces, such as Bali and West Java,
  and less well for others; and (3) the inclusion of location context enhances
  performance, especially for larger models like GPT-4, emphasizing the
  significance of geographical context in commonsense reasoning.</td>
  <td colspan=2 style='mso-ignore:colspan'>12 September, 2024</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>2607</td>
  <td align=right>2404.01954</td>
  <td class=xl65 width=649 style='width:487pt'>HyperCLOVA X Technical Report</td>
  <td class=xl65 width=649 style='width:487pt'>We introduce HyperCLOVA X, a
  family of large language models (LLMs) tailored to the Korean language and
  culture, along with competitive capabilities in English, math, and coding.
  HyperCLOVA X was trained on a balanced mix of Korean, English, and code data,
  followed by instruction-tuning with high-quality human-annotated datasets
  while abiding by strict safety guidelines reflecting our commitment to
  responsible AI. The model is evaluated across various benchmarks, including
  comprehensive reasoning, knowledge, commonsense, factuality, coding, math,
  chatting, instruction-following, and harmlessness, in both Korean and
  English. HyperCLOVA X exhibits strong reasoning capabilities in Korean backed
  by a deep understanding of the language and cultural nuances. Further
  analysis of the inherent bilingual nature and its extension to
  multilingualism highlights the model's cross-lingual proficiency and strong
  generalization ability to untargeted languages, including machine translation
  between several language pairs and cross-lingual inference tasks. We believe
  that HyperCLOVA X can provide helpful guidance for regions or countries in
  developing their sovereign LLMs.</td>
  <td colspan=2 style='mso-ignore:colspan'>13 April, 2024</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>2601</td>
  <td align=right>2404.02491</td>
  <td class=xl65 width=649 style='width:487pt'>Measuring Social Norms of Large
  Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>We present a new challenge to
  examine whether large language models understand social norms. In contrast to
  existing datasets, our dataset requires a fundamental understanding of social
  norms to solve. Our dataset features the largest set of social norm skills,
  consisting of 402 skills and 12,383 questions covering a wide set of social
  norms ranging from opinions and arguments to culture and laws. We design our
  dataset according to the K-12 curriculum. This enables the direct comparison
  of the social understanding of large language models to humans, more
  specifically, elementary students. While prior work generates nearly random
  accuracy on our benchmark, recent large language models such as GPT3.5-Turbo
  and LLaMA2-Chat are able to improve the performance significantly, only
  slightly below human performance. We then propose a multi-agent framework
  based on large language models to improve the models' ability to understand
  social norms. This method further improves large language models to be on par
  with humans. Given the increasing adoption of large language models in
  real-world applications, our finding is particularly important and presents a
  unique direction for future improvements.</td>
  <td colspan=2 style='mso-ignore:colspan'>22 May, 2024</td>
 </tr>
 <tr height=431 style='height:323.0pt'>
  <td height=431 align=right style='height:323.0pt'>2587</td>
  <td align=right>2404.03502</td>
  <td class=xl65 width=649 style='width:487pt'>AI and the Problem of Knowledge
  Collapse</td>
  <td class=xl65 width=649 style='width:487pt'>While artificial intelligence
  has the potential to process vast amounts of data, generate new insights, and
  unlock greater productivity, its widespread adoption may entail unforeseen
  consequences. We identify conditions under which AI, by reducing the cost of
  access to certain modes of knowledge, can paradoxically harm public
  understanding. While large language models are trained on vast amounts of
  diverse data, they naturally generate output towards the 'center' of the
  distribution. This is generally useful, but widespread reliance on recursive
  AI systems could lead to a process we define as &quot;knowledge
  collapse&quot;, and argue this could harm innovation and the richness of
  human understanding and culture. However, unlike AI models that cannot choose
  what data they are trained on, humans may strategically seek out diverse
  forms of knowledge if they perceive them to be worthwhile. To investigate
  this, we provide a simple model in which a community of learners or
  innovators choose to use traditional methods or to rely on a discounted
  AI-assisted process and identify conditions under which knowledge collapse
  occurs. In our default model, a 20% discount on AI-generated content
  generates public beliefs 2.3 times further from the truth than when there is
  no discount. An empirical approach to measuring the distribution of LLM
  outputs is provided in theoretical terms and illustrated through a specific
  example comparing the diversity of outputs across different models and
  prompting styles. Finally, based on the results, we consider further research
  directions to counteract such outcomes.</td>
  <td colspan=2 style='mso-ignore:colspan'>22 April, 2024</td>
 </tr>
 <tr height=385 style='height:289.0pt'>
  <td height=385 align=right style='height:289.0pt'>2581</td>
  <td align=right>2404.03685</td>
  <td class=xl65 width=649 style='width:487pt'>Cooperative Evolutionary
  Pressure and Diminishing Returns Might Explain the Fermi Paradox: On What
  Super-AIs Are Like</td>
  <td class=xl65 width=649 style='width:487pt'>With an evolutionary approach,
  the basis of morality can be explained as adaptations to problems of
  cooperation. With 'evolution' taken in a broad sense, AIs that satisfy the
  conditions for evolution to apply will be subject to the same cooperative
  evolutionary pressure as biological entities. Here the adaptiveness of
  increased cooperation as material safety and wealth increase is discussed --
  for humans, for other societies, and for AIs. Diminishing beneficial returns
  from increased access to material resources also suggests the possibility
  that, on the whole, there will be no incentive to for instance colonize
  entire galaxies, thus providing a possible explanation of the Fermi paradox,
  wondering where everybody is. It is further argued that old societies could
  engender, give way to, super-AIs, since it is likely that super-AIs are
  feasible, and fitter. Closing is an aside on effective ways for morals and
  goals to affect life and society, emphasizing environments, cultures, and
  laws, and exemplified by how to eat. 'Diminishing returns' is defined, as
  less than roots, the inverse of infeasibility. It is also noted that there
  can be no exponential colonization or reproduction, for mathematical reasons,
  as each entity takes up a certain amount of space. Appended are an algorithm
  for colonizing for example a galaxy quickly, models of the evolution of
  cooperation and fairness under diminishing returns, and software for
  simulating signaling development.</td>
  <td colspan=2 style='mso-ignore:colspan'>12 December, 2024</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>2574</td>
  <td align=right>2404.04286</td>
  <td class=xl65 width=649 style='width:487pt'>Bias Amplification in Language
  Model Evolution: An Iterated Learning Perspective</td>
  <td class=xl65 width=649 style='width:487pt'>With the widespread adoption of
  Large Language Models (LLMs), the prevalence of iterative interactions among
  these models is anticipated to increase. Notably, recent advancements in
  multi-round self-improving methods allow LLMs to generate new examples for
  training subsequent models. At the same time, multi-agent LLM systems,
  involving automated interactions among agents, are also increasing in
  prominence. Thus, in both short and long terms, LLMs may actively engage in
  an evolutionary process. We draw parallels between the behavior of LLMs and
  the evolution of human culture, as the latter has been extensively studied by
  cognitive scientists for decades. Our approach involves leveraging Iterated
  Learning (IL), a Bayesian framework that elucidates how subtle biases are
  magnified during human cultural evolution, to explain some behaviors of LLMs.
  This paper outlines key characteristics of agents' behavior in the
  Bayesian-IL framework, including predictions that are supported by
  experimental verification with various LLMs. This theoretical framework could
  help to more effectively predict and guide the evolution of LLMs in desired
  directions.</td>
  <td colspan=2 style='mso-ignore:colspan'>3 October, 2024</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>2561</td>
  <td align=right>2404.06212</td>
  <td class=xl65 width=649 style='width:487pt'>OmniFusion Technical Report</td>
  <td class=xl65 width=649 style='width:487pt'>Last year, multimodal
  architectures served up a revolution in AI-based approaches and solutions,
  extending the capabilities of large language models (LLM). We propose an
  \textit{OmniFusion} model based on a pretrained LLM and adapters for visual
  modality. We evaluated and compared several architecture design principles
  for better text and visual data coupling: MLP and transformer adapters,
  various CLIP ViT-based encoders (SigLIP, InternVIT, etc.), and their fusing
  approach, image encoding method (whole image or tiles encoding) and two 7B
  LLMs (the proprietary one and open-source Mistral). Experiments on 8
  visual-language benchmarks show the top score for the best OmniFusion setup
  in terms of different VQA tasks in comparison with open-source LLaVA-like
  solutions: VizWiz, Pope, MM-Vet, ScienceQA, MMBench, TextVQA, VQAv2, MMMU. We
  also propose a variety of situations, where OmniFusion provides
  highly-detailed answers in different domains: housekeeping, sightseeing,
  culture, medicine, handwritten and scanned equations recognition, etc.
  Mistral-based OmniFusion model is an open-source solution with weights,
  training and inference scripts available at
  https://github.com/AIRI-Institute/OmniFusion.</td>
  <td colspan=2 style='mso-ignore:colspan'>9 April, 2024</td>
 </tr>
 <tr height=496 style='height:372.0pt'>
  <td height=496 align=right style='height:372.0pt'>2554</td>
  <td align=right>2404.06647</td>
  <td class=xl65 width=649 style='width:487pt'>From Protoscience to Epistemic
  Monoculture: How Benchmarking Set the Stage for the Deep Learning Revolution</td>
  <td class=xl65 width=649 style='width:487pt'>Over the past decade, AI
  research has focused heavily on building ever-larger deep learning models.
  This approach has simultaneously unlocked incredible achievements in science
  and technology, and hindered AI from overcoming long-standing limitations with
  respect to explainability, ethical harms, and environmental efficiency.
  Drawing on qualitative interviews and computational analyses, our three-part
  history of AI research traces the creation of this &quot;epistemic
  monoculture&quot; back to a radical reconceptualization of scientific
  progress that began in the late 1980s. In the first era of AI research
  (1950s-late 1980s), researchers and patrons approached AI as a
  &quot;basic&quot; science that would advance through autonomous exploration
  and organic assessments of progress (e.g., peer-review, theoretical
  consensus). The failure of this approach led to a retrenchment of funding in
  the 1980s. Amid this &quot;AI Winter,&quot; an intervention by the U.S.
  government reoriented the field towards measurable progress on tasks of
  military and commercial interest. A new evaluation system called
  &quot;benchmarking&quot; provided an objective way to quantify progress on
  tasks by focusing exclusively on increasing predictive accuracy on example
  datasets. Distilling science down to verifiable metrics clarified the roles
  of scientists, allowed the field to rapidly integrate talent, and provided
  clear signals of significance and progress. But history has also revealed a
  tradeoff to this streamlined approach to science: the consolidation around
  external interests and inherent conservatism of benchmarking has
  disincentivized exploration beyond scaling monoculture. In the discussion, we
  explain how AI's monoculture offers a compelling challenge to the belief that
  basic, exploration-driven research is needed for scientific progress.
  Implications for the spread of AI monoculture to other sciences in the era of
  generative AI are also discussed.</td>
  <td colspan=2 style='mso-ignore:colspan'>10 April, 2024</td>
 </tr>
 <tr height=227 style='height:170.0pt'>
  <td height=227 align=right style='height:170.0pt'>2540</td>
  <td align=right>2404.07678</td>
  <td class=xl65 width=649 style='width:487pt'>On the role of ethics and
  sustainability in business innovation</td>
  <td class=xl65 width=649 style='width:487pt'>For organizations to survive and
  flourish in the long term, innovation and novelty must be continually
  introduced, which is particularly true in today's rapidly changing world.
  This raises a variety of ethical and sustainability considerations that seldom
  receive the attention they deserve. Existing innovation adoption frameworks
  often focus on technological, organizational, environmental, and social
  factors impacting adoption. In this chapter, we explore the ethical and
  sustainability angles, particularly as they relate to emerging technologies,
  artificial intelligence (AI) being a prominent example. We consider how to
  facilitate the development and cultivation of innovation cultures in
  organizations, including budding startups as well as established enterprises,
  through approaches such as systems thinking.</td>
  <td colspan=2 style='mso-ignore:colspan'>11 April, 2024</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>2539</td>
  <td align=right>2404.079</td>
  <td class=xl65 width=649 style='width:487pt'>High-Dimension Human Value
  Representation in Large Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>The widespread application of
  Large Language Models (LLMs) across various tasks and fields has necessitated
  the alignment of these models with human values and preferences. Given
  various approaches of human value alignment, ranging from Reinforcement Learning
  with Human Feedback (RLHF), to constitutional learning, etc. there is an
  urgent need to understand the scope and nature of human values injected into
  these models before their release. There is also a need for model alignment
  without a costly large scale human annotation effort. We propose UniVaR, a
  high-dimensional representation of human value distributions in LLMs,
  orthogonal to model architecture and training data. Trained from the
  value-relevant output of eight multilingual LLMs and tested on the output
  from four multilingual LLMs, namely LlaMA2, ChatGPT, JAIS and Yi, we show
  that UniVaR is a powerful tool to compare the distribution of human values
  embedded in different LLMs with different langauge sources. Through UniVaR,
  we explore how different LLMs prioritize various values in different
  languages and cultures, shedding light on the complex interplay between human
  values and language modeling.</td>
  <td colspan=2 style='mso-ignore:colspan'>4 October, 2024</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>2511</td>
  <td align=right>2404.10199</td>
  <td class=xl65 width=649 style='width:487pt'>CULTURE-GEN: Revealing Global
  Cultural Perception in Language Models through Natural Language Prompting</td>
  <td class=xl65 width=649 style='width:487pt'>As the utilization of large
  language models (LLMs) has proliferated world-wide, it is crucial for them to
  have adequate knowledge and fair representation for diverse global cultures.
  In this work, we uncover culture perceptions of three SOTA models on 110
  countries and regions on 8 culture-related topics through culture-conditioned
  generations, and extract symbols from these generations that are associated
  to each culture by the LLM. We discover that culture-conditioned generation
  consist of linguistic &quot;markers&quot; that distinguish marginalized
  cultures apart from default cultures. We also discover that LLMs have an
  uneven degree of diversity in the culture symbols, and that cultures from
  different geographic regions have different presence in LLMs'
  culture-agnostic generation. Our findings promote further research in
  studying the knowledge and fairness of global culture perception in LLMs.
  Code and Data can be found here: https://github.com/huihanlhh/Culture-Gen/</td>
  <td colspan=2 style='mso-ignore:colspan'>20 August, 2024</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>2494</td>
  <td align=right>2404.11752</td>
  <td class=xl65 width=649 style='width:487pt'>Mapping Violence: Developing an
  Extensive Framework to Build a Bangla Sectarian Expression Dataset from
  Social Media Interactions</td>
  <td class=xl65 width=649 style='width:487pt'>Communal violence in online
  forums has become extremely prevalent in South Asia, where many communities
  of different cultures coexist and share resources. These societies exhibit a
  phenomenon characterized by strong bonds within their own groups and animosity
  towards others, leading to conflicts that frequently escalate into violent
  confrontations. To address this issue, we have developed the first
  comprehensive framework for the automatic detection of communal violence
  markers in online Bangla content accompanying the largest collection (13K raw
  sentences) of social media interactions that fall under the definition of
  four major violence class and their 16 coarse expressions. Our workflow
  introduces a 7-step expert annotation process incorporating insights from
  social scientists, linguists, and psychologists. By presenting data
  statistics and benchmarking performance using this dataset, we have
  determined that, aside from the category of Non-communal violence,
  Religio-communal violence is particularly pervasive in Bangla text. Moreover,
  we have substantiated the effectiveness of fine-tuning language models in
  identifying violent comments by conducting preliminary benchmarking on the
  state-of-the-art Bangla deep learning model.</td>
  <td colspan=2 style='mso-ignore:colspan'>17 April, 2024</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>2488</td>
  <td align=right>2404.12464</td>
  <td class=xl65 width=649 style='width:487pt'>NormAd: A Framework for
  Measuring the Cultural Adaptability of Large Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>To be effectively and safely
  deployed to global user populations, large language models (LLMs) must adapt
  outputs to user values and culture, not just know about them. We introduce
  NormAd, an evaluation framework to assess LLMs' cultural adaptability, specifically
  measuring their ability to judge social acceptability across different levels
  of cultural norm specificity, from abstract values to explicit social norms.
  As an instantiation of our framework, we create NormAd-Eti, a benchmark of
  2.6k situational descriptions representing social-etiquette related cultural
  norms from 75 countries. Through comprehensive experiments on NormAd-Eti, we
  find that LLMs struggle to accurately judge social acceptability across these
  varying degrees of cultural contexts and show stronger adaptability to
  English-centric cultures over those from the Global South. Even in the
  simplest setting where the relevant social norms are provided, our best
  models' performance (&lt;82%) lags behind humans (&gt;95%). In settings with
  abstract values and country information, model performance drops
  substantially (&lt;60%), while human accuracy remains high (&gt;90%).
  Furthermore, we find that models are better at recognizing socially
  acceptable versus unacceptable situations. Our findings showcase the current
  pitfalls in socio-cultural reasoning of LLMs which hinder their adaptability
  for global audiences.</td>
  <td colspan=2 style='mso-ignore:colspan'>27 October, 2024</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>2481</td>
  <td align=right>2404.12933</td>
  <td class=xl65 width=649 style='width:487pt'>Cross-cultural Inspiration
  Detection and Analysis in Real and LLM-generated Social Media Data</td>
  <td class=xl65 width=649 style='width:487pt'>Inspiration is linked to various
  positive outcomes, such as increased creativity, productivity, and happiness.
  Although inspiration has great potential, there has been limited effort
  toward identifying content that is inspiring, as opposed to just engaging or
  positive. Additionally, most research has concentrated on Western data, with
  little attention paid to other cultures. This work is the first to study
  cross-cultural inspiration through machine learning methods. We aim to
  identify and analyze real and AI-generated cross-cultural inspiring posts. To
  this end, we compile and make publicly available the InspAIred dataset, which
  consists of 2,000 real inspiring posts, 2,000 real non-inspiring posts, and
  2,000 generated inspiring posts evenly distributed across India and the UK.
  The real posts are sourced from Reddit, while the generated posts are created
  using the GPT-4 model. Using this dataset, we conduct extensive computational
  linguistic analyses to (1) compare inspiring content across cultures, (2)
  compare AI-generated inspiring posts to real inspiring posts, and (3)
  determine if detection models can accurately distinguish between inspiring
  content across cultures and data sources.</td>
  <td colspan=2 style='mso-ignore:colspan'>18 June, 2024</td>
 </tr>
 <tr height=453 style='height:340.0pt'>
  <td height=453 align=right style='height:340.0pt'>2480</td>
  <td align=right>2404.1306</td>
  <td class=xl65 width=649 style='width:487pt'>The Necessity of AI Audit
  Standards Boards</td>
  <td class=xl65 width=649 style='width:487pt'>Auditing of AI systems is a
  promising way to understand and manage ethical problems and societal risks
  associated with contemporary AI systems, as well as some anticipated future
  risks. Efforts to develop standards for auditing Artificial Intelligence (AI)
  systems have therefore understandably gained momentum. However, we argue that
  creating auditing standards is not just insufficient, but actively harmful by
  proliferating unheeded and inconsistent standards, especially in light of the
  rapid evolution and ethical and safety challenges of AI. Instead, the paper
  proposes the establishment of an AI Audit Standards Board, responsible for
  developing and updating auditing methods and standards in line with the
  evolving nature of AI technologies. Such a body would ensure that auditing
  practices remain relevant, robust, and responsive to the rapid advancements
  in AI. The paper argues that such a governance structure would also be
  helpful for maintaining public trust in AI and for promoting a culture of
  safety and ethical responsibility within the AI industry. Throughout the
  paper, we draw parallels with other industries, including safety-critical
  industries like aviation and nuclear energy, as well as more prosaic ones
  such as financial accounting and pharmaceuticals. AI auditing should emulate
  those fields, and extend beyond technical assessments to include ethical
  considerations and stakeholder engagement, but we explain that this is not
  enough; emulating other fields' governance mechanisms for these processes,
  and for audit standards creation, is a necessity. We also emphasize the
  importance of auditing the entire development process of AI systems, not just
  the final products...</td>
  <td colspan=2 style='mso-ignore:colspan'>11 April, 2024</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>2450</td>
  <td align=right>2404.15238</td>
  <td class=xl65 width=649 style='width:487pt'>CultureBank: An Online
  Community-Driven Knowledge Base Towards Culturally Aware Language
  Technologies</td>
  <td class=xl65 width=649 style='width:487pt'>To enhance language models'
  cultural awareness, we design a generalizable pipeline to construct cultural
  knowledge bases from different online communities on a massive scale. With
  the pipeline, we construct CultureBank, a knowledge base built upon users'
  self-narratives with 12K cultural descriptors sourced from TikTok and 11K
  from Reddit. Unlike previous cultural knowledge resources, CultureBank
  contains diverse views on cultural descriptors to allow flexible
  interpretation of cultural knowledge, and contextualized cultural scenarios
  to help grounded evaluation. With CultureBank, we evaluate different LLMs'
  cultural awareness, and identify areas for improvement. We also fine-tune a
  language model on CultureBank: experiments show that it achieves better
  performances on two downstream cultural tasks in a zero-shot setting.
  Finally, we offer recommendations based on our findings for future culturally
  aware language technologies. The project page is
  https://culturebank.github.io . The code and model is at
  https://github.com/SALT-NLP/CultureBank . The released CultureBank dataset is
  at https://huggingface.co/datasets/SALT-NLP/CultureBank .</td>
  <td colspan=2 style='mso-ignore:colspan'>23 April, 2024</td>
 </tr>
 <tr height=385 style='height:289.0pt'>
  <td height=385 align=right style='height:289.0pt'>2419</td>
  <td align=right>2404.18276</td>
  <td class=xl65 width=649 style='width:487pt'>Bias Neutralization Framework:
  Measuring Fairness in Large Language Models with Bias Intelligence Quotient
  (BiQ)</td>
  <td class=xl65 width=649 style='width:487pt'>The burgeoning influence of
  Large Language Models (LLMs) in shaping public discourse and decision-making
  underscores the imperative to address inherent biases within these AI
  systems. In the wake of AI's expansive integration across sectors, addressing
  racial bias in LLMs has never been more critical. This paper introduces a
  novel framework called Comprehensive Bias Neutralization Framework (CBNF)
  which embodies an innovative approach to quantifying and mitigating biases
  within LLMs. Our framework combines the Large Language Model Bias Index
  (LLMBI) [Oketunji, A., Anas, M., Saina, D., (2023)] and Bias removaL with No
  Demographics (BLIND) [Orgad, H., Belinkov, Y. (2023)] methodologies to create
  a new metric called Bias Intelligence Quotient (BiQ)which detects, measures,
  and mitigates racial bias in LLMs without reliance on demographic
  annotations. By introducing a new metric called BiQ that enhances LLMBI with
  additional fairness metrics, CBNF offers a multi-dimensional metric for bias
  assessment, underscoring the necessity of a nuanced approach to fairness in
  AI [Mehrabi et al., 2021]. This paper presents a detailed analysis of Latimer
  AI (a language model incrementally trained on black history and culture) in
  comparison to ChatGPT 3.5, illustrating Latimer AI's efficacy in detecting
  racial, cultural, and gender biases through targeted training and refined
  bias mitigation strategies [Latimer &amp; Bender, 2023].</td>
  <td colspan=2 style='mso-ignore:colspan'>28 April, 2024</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>2417</td>
  <td align=right>2404.18359</td>
  <td class=xl65 width=649 style='width:487pt'>FoundaBench: Evaluating Chinese
  Fundamental Knowledge Capabilities of Large Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>In the burgeoning field of large
  language models (LLMs), the assessment of fundamental knowledge remains a
  critical challenge, particularly for models tailored to Chinese language and
  culture. This paper introduces FoundaBench, a pioneering benchmark designed
  to rigorously evaluate the fundamental knowledge capabilities of Chinese
  LLMs. FoundaBench encompasses a diverse array of 3354 multiple-choice
  questions across common sense and K-12 educational subjects, meticulously
  curated to reflect the breadth and depth of everyday and academic knowledge.
  We present an extensive evaluation of 12 state-of-the-art LLMs using
  FoundaBench, employing both traditional assessment methods and our
  CircularEval protocol to mitigate potential biases in model responses. Our
  results highlight the superior performance of models pre-trained on Chinese
  corpora, and reveal a significant disparity between models' reasoning and
  memory recall capabilities. The insights gleaned from FoundaBench evaluations
  set a new standard for understanding the fundamental knowledge of LLMs,
  providing a robust framework for future advancements in the field.</td>
  <td colspan=2 style='mso-ignore:colspan'>28 April, 2024</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>2415</td>
  <td align=right>2404.1846</td>
  <td class=xl65 width=649 style='width:487pt'>Ethical Reasoning and Moral
  Value Alignment of LLMs Depend on the Language we Prompt them in</td>
  <td class=xl65 width=649 style='width:487pt'>Ethical reasoning is a crucial
  skill for Large Language Models (LLMs). However, moral values are not
  universal, but rather influenced by language and culture. This paper explores
  how three prominent LLMs -- GPT-4, ChatGPT, and Llama2-70B-Chat -- perform ethical
  reasoning in different languages and if their moral judgement depend on the
  language in which they are prompted. We extend the study of ethical reasoning
  of LLMs by Rao et al. (2023) to a multilingual setup following their
  framework of probing LLMs with ethical dilemmas and policies from three
  branches of normative ethics: deontology, virtue, and consequentialism. We
  experiment with six languages: English, Spanish, Russian, Chinese, Hindi, and
  Swahili. We find that GPT-4 is the most consistent and unbiased ethical
  reasoner across languages, while ChatGPT and Llama2-70B-Chat show significant
  moral value bias when we move to languages other than English. Interestingly,
  the nature of this bias significantly vary across languages for all LLMs,
  including GPT-4.</td>
  <td colspan=2 style='mso-ignore:colspan'>29 April, 2024</td>
 </tr>
 <tr height=204 style='height:153.0pt'>
  <td height=204 align=right style='height:153.0pt'>2414</td>
  <td align=right>2404.18504</td>
  <td class=xl65 width=649 style='width:487pt'>Multisensor Data Fusion for
  Automatized Insect Monitoring (KInsecta)</td>
  <td class=xl65 width=649 style='width:487pt'>Insect populations are declining
  globally, making systematic monitoring essential for conservation. Most
  classical methods involve death traps and counter insect conservation. This
  paper presents a multisensor approach that uses AI-based data fusion for insect
  classification. The system is designed as low-cost setup and consists of a
  camera module and an optical wing beat sensor as well as environmental
  sensors to measure temperature, irradiance or daytime as prior information.
  The system has been tested in the laboratory and in the field. First tests on
  a small very unbalanced data set with 7 species show promising results for
  species classification. The multisensor system will support biodiversity and
  agriculture studies.</td>
  <td colspan=2 style='mso-ignore:colspan'>29 April, 2024</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>2397</td>
  <td align=right>2405.00435</td>
  <td class=xl65 width=649 style='width:487pt'>CultiVerse: Towards
  Cross-Cultural Understanding for Paintings with Large Language Model</td>
  <td class=xl65 width=649 style='width:487pt'>The integration of new
  technology with cultural studies enhances our understanding of cultural
  heritage but often struggles to connect with diverse audiences. It is
  challenging to align personal interpretations with the intended meanings
  across different cultures. Our study investigates the important factors in
  appreciating art from a cross-cultural perspective. We explore the
  application of Large Language Models (LLMs) to bridge the cultural and
  language barriers in understanding Traditional Chinese Paintings (TCPs). We
  present CultiVerse, a visual analytics system that utilizes LLMs within a
  mixed-initiative framework, enhancing interpretative appreciation of TCP in a
  cross-cultural dialogue. CultiVerse addresses the challenge of translating
  the nuanced symbolism in art, which involves interpreting complex cultural
  contexts, aligning cross-cultural symbols, and validating cultural
  acceptance. CultiVerse integrates an interactive interface with the
  analytical capability of LLMs to explore a curated TCP dataset, facilitating
  the analysis of multifaceted symbolic meanings and the exploration of
  cross-cultural serendipitous discoveries. Empirical evaluations affirm that
  CultiVerse significantly improves cross-cultural understanding, offering
  deeper insights and engaging art appreciation.</td>
  <td colspan=2 style='mso-ignore:colspan'>1 May, 2024</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>2391</td>
  <td align=right>2405.0131</td>
  <td class=xl65 width=649 style='width:487pt'>Overcoming LLM Challenges using
  RAG-Driven Precision in Coffee Leaf Disease Remediation</td>
  <td class=xl65 width=649 style='width:487pt'>This research introduces an
  innovative AI-driven precision agriculture system, leveraging YOLOv8 for
  disease identification and Retrieval Augmented Generation (RAG) for
  context-aware diagnosis. Focused on addressing the challenges of diseases
  affecting the coffee production sector in Karnataka, The system integrates
  sophisticated object detection techniques with language models to address the
  inherent constraints associated with Large Language Models (LLMs). Our
  methodology not only tackles the issue of hallucinations in LLMs, but also
  introduces dynamic disease identification and remediation strategies.
  Real-time monitoring, collaborative dataset expansion, and organizational
  involvement ensure the system's adaptability in diverse agricultural settings.
  The effect of the suggested system extends beyond automation, aiming to
  secure food supplies, protect livelihoods, and promote eco-friendly farming
  practices. By facilitating precise disease identification, the system
  contributes to sustainable and environmentally conscious agriculture,
  reducing reliance on pesticides. Looking to the future, the project envisions
  continuous development in RAG-integrated object detection systems,
  emphasizing scalability, reliability, and usability. This research strives to
  be a beacon for positive change in agriculture, aligning with global efforts
  toward sustainable and technologically enhanced food production.</td>
  <td colspan=2 style='mso-ignore:colspan'>2 May, 2024</td>
 </tr>
 <tr height=431 style='height:323.0pt'>
  <td height=431 align=right style='height:323.0pt'>2381</td>
  <td align=right>2405.02024</td>
  <td class=xl65 width=649 style='width:487pt'>Analyzing Narrative Processing
  in Large Language Models (LLMs): Using GPT4 to test BERT</td>
  <td class=xl65 width=649 style='width:487pt'>The ability to transmit and
  receive complex information via language is unique to humans and is the basis
  of traditions, culture and versatile social interactions. Through the
  disruptive introduction of transformer based large language models (LLMs)
  humans are not the only entity to &quot;understand&quot; and produce language
  any more. In the present study, we have performed the first steps to use LLMs
  as a model to understand fundamental mechanisms of language processing in
  neural networks, in order to make predictions and generate hypotheses on how
  the human brain does language processing. Thus, we have used ChatGPT to
  generate seven different stylistic variations of ten different narratives
  (Aesop's fables). We used these stories as input for the open source LLM BERT
  and have analyzed the activation patterns of the hidden units of BERT using
  multi-dimensional scaling and cluster analysis. We found that the activation
  vectors of the hidden units cluster according to stylistic variations in
  earlier layers of BERT (1) than narrative content (4-5). Despite the fact
  that BERT consists of 12 identical building blocks that are stacked and
  trained on large text corpora, the different layers perform different tasks.
  This is a very useful model of the human brain, where self-similar
  structures, i.e. different areas of the cerebral cortex, can have different
  functions and are therefore well suited to processing language in a very
  efficient way. The proposed approach has the potential to open the black box
  of LLMs on the one hand, and might be a further step to unravel the neural
  processes underlying human language processing and cognition in general.</td>
  <td colspan=2 style='mso-ignore:colspan'>3 May, 2024</td>
 </tr>
 <tr height=453 style='height:340.0pt'>
  <td height=453 align=right style='height:340.0pt'>2347</td>
  <td align=right>2405.04457</td>
  <td class=xl65 width=649 style='width:487pt'>Towards Geographic Inclusion in
  the Evaluation of Text-to-Image Models</td>
  <td class=xl65 width=649 style='width:487pt'>Rapid progress in text-to-image
  generative models coupled with their deployment for visual content creation
  has magnified the importance of thoroughly evaluating their performance and
  identifying potential biases. In pursuit of models that generate images that
  are realistic, diverse, visually appealing, and consistent with the given
  prompt, researchers and practitioners often turn to automated metrics to
  facilitate scalable and cost-effective performance profiling. However,
  commonly-used metrics often fail to account for the full diversity of human
  preference; often even in-depth human evaluations face challenges with
  subjectivity, especially as interpretations of evaluation criteria vary
  across regions and cultures. In this work, we conduct a large, cross-cultural
  study to study how much annotators in Africa, Europe, and Southeast Asia vary
  in their perception of geographic representation, visual appeal, and
  consistency in real and generated images from state-of-the art public APIs.
  We collect over 65,000 image annotations and 20 survey responses. We contrast
  human annotations with common automated metrics, finding that human
  preferences vary notably across geographic location and that current metrics
  do not fully account for this diversity. For example, annotators in different
  locations often disagree on whether exaggerated, stereotypical depictions of
  a region are considered geographically representative. In addition, the
  utility of automatic evaluations is dependent on assumptions about their set-up,
  such as the alignment of feature extractors with human perception of object
  similarity or the definition of &quot;appeal&quot; captured in reference
  datasets used to ground evaluations. We recommend steps for improved
  automatic and human evaluations.</td>
  <td colspan=2 style='mso-ignore:colspan'>7 May, 2024</td>
 </tr>
 <tr height=249 style='height:187.0pt'>
  <td height=249 align=right style='height:187.0pt'>2344</td>
  <td align=right>2405.04655</td>
  <td class=xl65 width=649 style='width:487pt'>Understanding the Capabilities
  and Limitations of Large Language Models for Cultural Commonsense</td>
  <td class=xl65 width=649 style='width:487pt'>Large language models (LLMs)
  have demonstrated substantial commonsense understanding through numerous
  benchmark evaluations. However, their understanding of cultural commonsense
  remains largely unexamined. In this paper, we conduct a comprehensive examination
  of the capabilities and limitations of several state-of-the-art LLMs in the
  context of cultural commonsense tasks. Using several general and cultural
  commonsense benchmarks, we find that (1) LLMs have a significant discrepancy
  in performance when tested on culture-specific commonsense knowledge for
  different cultures; (2) LLMs' general commonsense capability is affected by
  cultural context; and (3) The language used to query the LLMs can impact
  their performance on cultural-related tasks. Our study points to the inherent
  bias in the cultural understanding of LLMs and provides insights that can
  help develop culturally aware language models.</td>
  <td colspan=2 style='mso-ignore:colspan'>7 May, 2024</td>
 </tr>
 <tr height=408 style='height:306.0pt'>
  <td height=408 align=right style='height:306.0pt'>2314</td>
  <td align=right>2405.0734</td>
  <td class=xl65 width=649 style='width:487pt'>Machine Consciousness as
  Pseudoscience: The Myth of Conscious Machines</td>
  <td class=xl65 width=649 style='width:487pt'>The hypothesis of conscious
  machines has been debated since the invention of the notion of artificial
  intelligence, powered by the assumption that the computational intelligence
  achieved by a system is the cause of the emergence of phenomenal consciousness
  in that system as an epiphenomenon or as a consequence of the behavioral or
  internal complexity of the system surpassing some threshold. As a
  consequence, a huge amount of literature exploring the possibility of machine
  consciousness and how to implement it on a computer has been published.
  Moreover, common folk psychology and transhumanism literature has fed this
  hypothesis with the popularity of science fiction literature, where
  intelligent robots are usually antropomorphized and hence given phenomenal
  consciousness. However, in this work, we argue how these literature lacks
  scientific rigour, being impossible to falsify the opposite hypothesis, and
  illustrate a list of arguments that show how every approach that the machine
  consciousness literature has published depends on philosophical assumptions
  that cannot be proven by the scientific method. Concretely, we also show how
  phenomenal consciousness is not computable, independently on the complexity
  of the algorithm or model, cannot be objectively measured nor quantitatively
  defined and it is basically a phenomenon that is subjective and internal to
  the observer. Given all those arguments we end the work arguing why the idea
  of conscious machines is nowadays a myth of transhumanism and science fiction
  culture.</td>
  <td colspan=2 style='mso-ignore:colspan'>12 May, 2024</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>2287</td>
  <td align=right>2405.09734</td>
  <td class=xl65 width=649 style='width:487pt'>Attention is All You Want:
  Machinic Gaze and the Anthropocene</td>
  <td class=xl65 width=649 style='width:487pt'>This chapter experiments with
  ways computational vision interprets and synthesises representations of the
  Anthropocene. Text-to-image systems such as MidJourney and StableDiffusion,
  trained on large data sets of harvested images and captions, yield often
  striking compositions that serve, alternately, as banal reproduction, alien
  imaginary and refracted commentary on the preoccupations of Internet visual
  culture. While the effects of AI on visual culture may themselves be
  transformative or catastrophic, we are more interested here in how it has
  been trained to imagine shared human, technical and ecological futures.
  Through a series of textual prompts that marry elements of the Anthropocenic
  and Australian environmental vernacular, we examine how this emergent
  machinic gaze both looks out, through its compositions of futuristic
  landscapes, and looks back, towards an observing and observed human subject.
  In its varied assistive, surveillant and generative roles, computational
  vision not only mirrors human desire but articulates oblique demands of its
  own.</td>
  <td colspan=2 style='mso-ignore:colspan'>15 May, 2024</td>
 </tr>
 <tr height=385 style='height:289.0pt'>
  <td height=385 align=right style='height:289.0pt'>2274</td>
  <td align=right>2405.10936</td>
  <td class=xl65 width=649 style='width:487pt'>A Survey on Large Language
  Models with Multilingualism: Recent Advances and New Frontiers</td>
  <td class=xl65 width=649 style='width:487pt'>The rapid development of Large
  Language Models (LLMs) demonstrates remarkable multilingual capabilities in
  natural language processing, attracting global attention in both academia and
  industry. To mitigate potential discrimination and enhance the overall
  usability and accessibility for diverse language user groups, it is important
  for the development of language-fair technology. Despite the breakthroughs of
  LLMs, the investigation into the multilingual scenario remains insufficient,
  where a comprehensive survey to summarize recent approaches, developments,
  limitations, and potential solutions is desirable. To this end, we provide a
  survey with multiple perspectives on the utilization of LLMs in the
  multilingual scenario. We first rethink the transitions between previous and
  current research on pre-trained language models. Then we introduce several
  perspectives on the multilingualism of LLMs, including training and inference
  methods, information retrieval, model security, multi-domain with language culture,
  and usage of datasets. We also discuss the major challenges that arise in
  these aspects, along with possible solutions. Besides, we highlight future
  research directions that aim at further enhancing LLMs with multilingualism.
  The survey aims to help the research community address multilingual problems
  and provide a comprehensive understanding of the core concepts, key
  techniques, and latest developments in multilingual natural language
  processing based on LLMs.</td>
  <td colspan=2 style='mso-ignore:colspan'>7 January, 2025</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>2254</td>
  <td align=right>2405.1301</td>
  <td class=xl65 width=649 style='width:487pt'>UCCIX: Irish-eXcellence Large
  Language Model</td>
  <td class=xl65 width=649 style='width:487pt'>The development of Large
  Language Models (LLMs) has predominantly focused on high-resource languages,
  leaving extremely low-resource languages like Irish with limited
  representation. This work presents UCCIX, a pioneering effort on the
  development of an open-source Irish-based LLM. We propose a novel framework
  for continued pre-training of LLMs specifically adapted for extremely
  low-resource languages, requiring only a fraction of the textual data
  typically needed for training LLMs according to scaling laws. Our model,
  based on Llama 2-13B, outperforms much larger models on Irish language tasks
  with up to 12% performance improvement, showcasing the effectiveness and
  efficiency of our approach. We also contribute comprehensive Irish
  benchmarking datasets, including IrishQA, a question-answering dataset, and
  Irish version of MT-bench. These datasets enable rigorous evaluation and
  facilitate future research in Irish LLM systems. Our work aims to preserve
  and promote the Irish language, knowledge, and culture of Ireland in the
  digital era while providing a framework for adapting LLMs to other indigenous
  languages.</td>
  <td colspan=2 style='mso-ignore:colspan'>13 May, 2024</td>
 </tr>
 <tr height=475 style='height:356.0pt'>
  <td height=475 align=right style='height:356.0pt'>2237</td>
  <td align=right>2405.14004</td>
  <td class=xl65 width=649 style='width:487pt'>Towards A Comprehensive
  Assessment of AI's Environmental Impact</td>
  <td class=xl65 width=649 style='width:487pt'>Artificial Intelligence, machine
  learning (AI/ML) has allowed exploring solutions for a variety of
  environmental and climate questions ranging from natural disasters,
  greenhouse gas emission, monitoring biodiversity, agriculture, to weather and
  climate modeling, enabling progress towards climate change mitigation.
  However, the intersection of AI/ML and environment is not always positive.
  The recent surge of interest in ML, made possible by processing very large
  volumes of data, fueled by access to massive compute power, has sparked a
  trend towards large-scale adoption of AI/ML. This interest places tremendous
  pressure on natural resources, that are often overlooked and under-reported.
  There is a need for a framework that monitors the environmental impact and
  degradation from AI/ML throughout its lifecycle for informing policymakers,
  stakeholders to adequately implement standards and policies and track the
  policy outcome over time. For these policies to be effective, AI's
  environmental impact needs to be monitored in a spatially-disaggregated,
  timely manner across the globe at the key activity sites. This study proposes
  a methodology to track environmental variables relating to the multifaceted
  impact of AI around datacenters using openly available energy data and
  globally acquired satellite observations. We present a case study around
  Northern Virginia, United States that hosts a growing number of datacenters
  and observe changes in multiple satellite-based environmental metrics. We
  then discuss the steps to expand this methodology for comprehensive
  assessment of AI's environmental impact across the planet. We also identify
  data gaps and formulate recommendations for improving the understanding and
  monitoring AI-induced changes to the environment and climate.</td>
  <td colspan=2 style='mso-ignore:colspan'>22 May, 2024</td>
 </tr>
 <tr height=431 style='height:323.0pt'>
  <td height=431 align=right style='height:323.0pt'>2224</td>
  <td align=right>2405.15145</td>
  <td class=xl65 width=649 style='width:487pt'>CulturePark: Boosting
  Cross-cultural Understanding in Large Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>Cultural bias is pervasive in
  many large language models (LLMs), largely due to the deficiency of data
  representative of different cultures. Typically, cultural datasets and
  benchmarks are constructed either by extracting subsets of existing datasets
  or by aggregating from platforms such as Wikipedia and social media. However,
  these approaches are highly dependent on real-world data and human
  annotations, making them costly and difficult to scale. Inspired by cognitive
  theories on social communication, this paper introduces CulturePark, an
  LLM-powered multi-agent communication framework for cultural data collection.
  CulturePark simulates cross-cultural human communication with LLM-based
  agents playing roles in different cultures. It generates high-quality
  cross-cultural dialogues encapsulating human beliefs, norms, and customs.
  Using CulturePark, we generated 41,000 cultural samples to fine-tune eight
  culture-specific LLMs. We evaluated these models across three downstream
  tasks: content moderation, cultural alignment, and cultural education.
  Results show that for content moderation, our GPT-3.5-based models either
  match or outperform GPT-4 on datasets. Regarding cultural alignment, our
  models surpass GPT-4 on Hofstede's VSM 13 framework. Furthermore, for
  cultural education of human participants, our models demonstrate superior
  outcomes in both learning efficacy and user experience compared to GPT-4.
  CulturePark proves an important step in addressing cultural bias and
  advancing the democratization of AI, highlighting the critical role of
  culturally inclusive data in model training. Code is released at
  https://github.com/Scarelette/CulturePark.</td>
  <td colspan=2 style='mso-ignore:colspan'>21 November, 2024</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>2217</td>
  <td align=right>2405.15428</td>
  <td class=xl65 width=649 style='width:487pt'>Enhancing Pollinator
  Conservation towards Agriculture 4.0: Monitoring of Bees through Object
  Recognition</td>
  <td class=xl65 width=649 style='width:487pt'>In an era of rapid climate
  change and its adverse effects on food production, technological intervention
  to monitor pollinator conservation is of paramount importance for
  environmental monitoring and conservation for global food security. The
  survival of the human species depends on the conservation of pollinators.
  This article explores the use of Computer Vision and Object Recognition to
  autonomously track and report bee behaviour from images. A novel dataset of
  9664 images containing bees is extracted from video streams and annotated
  with bounding boxes. With training, validation and testing sets (6722, 1915,
  and 997 images, respectively), the results of the COCO-based YOLO model
  fine-tuning approaches show that YOLOv5m is the most effective approach in
  terms of recognition accuracy. However, YOLOv5s was shown to be the most
  optimal for real-time bee detection with an average processing and inference
  time of 5.1ms per video frame at the cost of slightly lower ability. The
  trained model is then packaged within an explainable AI interface, which
  converts detection events into timestamped reports and charts, with the aim
  of facilitating use by non-technical users such as expert stakeholders from
  the apiculture industry towards informing responsible consumption and
  production.</td>
  <td colspan=2 style='mso-ignore:colspan'>24 May, 2024</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>2209</td>
  <td align=right>2405.15815</td>
  <td class=xl65 width=649 style='width:487pt'>A social path to human-like
  artificial intelligence</td>
  <td class=xl65 width=649 style='width:487pt'>Traditionally, cognitive and
  computer scientists have viewed intelligence solipsistically, as a property
  of unitary agents devoid of social context. Given the success of contemporary
  learning algorithms, we argue that the bottleneck in artificial intelligence
  (AI) progress is shifting from data assimilation to novel data generation. We
  bring together evidence showing that natural intelligence emerges at multiple
  scales in networks of interacting agents via collective living, social
  relationships and major evolutionary transitions, which contribute to novel
  data generation through mechanisms such as population pressures, arms races,
  Machiavellian selection, social learning and cumulative culture. Many
  breakthroughs in AI exploit some of these processes, from multi-agent
  structures enabling algorithms to master complex games like Capture-The-Flag
  and StarCraft II, to strategic communication in Diplomacy and the shaping of
  AI data streams by other AIs. Moving beyond a solipsistic view of agency to
  integrate these mechanisms suggests a path to human-like compounding
  innovation through ongoing novel data generation.</td>
  <td colspan=2 style='mso-ignore:colspan'>22 May, 2024</td>
 </tr>
 <tr height=385 style='height:289.0pt'>
  <td height=385 align=right style='height:289.0pt'>2200</td>
  <td align=right>2406.10133</td>
  <td class=xl65 width=649 style='width:487pt'>Evaluation of Large Language
  Models: STEM education and Gender Stereotypes</td>
  <td class=xl65 width=649 style='width:487pt'>Large Language Models (LLMs)
  have an increasing impact on our lives with use cases such as chatbots, study
  support, coding support, ideation, writing assistance, and more. Previous
  studies have revealed linguistic biases in pronouns used to describe professions
  or adjectives used to describe men vs women. These issues have to some degree
  been addressed in updated LLM versions, at least to pass existing tests.
  However, biases may still be present in the models, and repeated use of
  gender stereotypical language may reinforce the underlying assumptions and
  are therefore important to examine further. This paper investigates gender
  biases in LLMs in relation to educational choices through an open-ended, true
  to user-case experimental design and a quantitative analysis. We investigate
  the biases in the context of four different cultures, languages, and
  educational systems (English/US/UK, Danish/DK, Catalan/ES, and Hindi/IN) for
  ages ranging from 10 to 16 years, corresponding to important educational
  transition points in the different countries. We find that there are
  significant and large differences in the ratio of STEM to non-STEM suggested
  education paths provided by chatGPT when using typical girl vs boy names to
  prompt lists of suggested things to become. There are generally fewer STEM
  suggestions in the Danish, Spanish, and Indian context compared to the
  English. We also find subtle differences in the suggested professions, which
  we categorise and report.</td>
  <td colspan=2 style='mso-ignore:colspan'>14 June, 2024</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>2194</td>
  <td align=right>2406.10318</td>
  <td class=xl65 width=649 style='width:487pt'>Creating a Lens of Chinese
  Culture: A Multimodal Dataset for Chinese Pun Rebus Art Understanding</td>
  <td class=xl65 width=649 style='width:487pt'>Large vision-language models
  (VLMs) have demonstrated remarkable abilities in understanding everyday
  content. However, their performance in the domain of art, particularly
  culturally rich art forms, remains less explored. As a pearl of human wisdom
  and creativity, art encapsulates complex cultural narratives and symbolism.
  In this paper, we offer the Pun Rebus Art Dataset, a multimodal dataset for
  art understanding deeply rooted in traditional Chinese culture. We focus on
  three primary tasks: identifying salient visual elements, matching elements
  with their symbolic meanings, and explanations for the conveyed messages. Our
  evaluation reveals that state-of-the-art VLMs struggle with these tasks,
  often providing biased and hallucinated explanations and showing limited
  improvement through in-context learning. By releasing the Pun Rebus Art
  Dataset, we aim to facilitate the development of VLMs that can better
  understand and interpret culturally specific content, promoting greater
  inclusiveness beyond English-based corpora.</td>
  <td colspan=2 style='mso-ignore:colspan'>14 June, 2024</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>2185</td>
  <td align=right>2406.1103</td>
  <td class=xl65 width=649 style='width:487pt'>FoodieQA: A Multimodal Dataset
  for Fine-Grained Understanding of Chinese Food Culture</td>
  <td class=xl65 width=649 style='width:487pt'>Food is a rich and varied
  dimension of cultural heritage, crucial to both individuals and social
  groups. To bridge the gap in the literature on the often-overlooked regional
  diversity in this domain, we introduce FoodieQA, a manually curated,
  fine-grained image-text dataset capturing the intricate features of food
  cultures across various regions in China. We evaluate vision-language Models
  (VLMs) and large language models (LLMs) on newly collected, unseen food
  images and corresponding questions. FoodieQA comprises three multiple-choice
  question-answering tasks where models need to answer questions based on
  multiple images, a single image, and text-only descriptions, respectively.
  While LLMs excel at text-based question answering, surpassing human accuracy,
  the open-sourced VLMs still fall short by 41% on multi-image and 21% on
  single-image VQA tasks, although closed-weights models perform closer to
  human levels (within 10%). Our findings highlight that understanding food and
  its cultural implications remains a challenging and under-explored direction.</td>
  <td colspan=2 style='mso-ignore:colspan'>30 September, 2024</td>
 </tr>
 <tr height=249 style='height:187.0pt'>
  <td height=249 align=right style='height:187.0pt'>2172</td>
  <td align=right>2406.11565</td>
  <td class=xl65 width=649 style='width:487pt'>Extrinsic Evaluation of Cultural
  Competence in Large Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>Productive interactions between
  diverse users and language technologies require outputs from the latter to be
  culturally relevant and sensitive. Prior works have evaluated models'
  knowledge of cultural norms, values, and artifacts, without considering how
  this knowledge manifests in downstream applications. In this work, we focus
  on extrinsic evaluation of cultural competence in two text generation tasks,
  open-ended question answering and story generation. We quantitatively and
  qualitatively evaluate model outputs when an explicit cue of culture,
  specifically nationality, is perturbed in the prompts. Although we find that
  model outputs do vary when varying nationalities and feature culturally
  relevant words, we also find weak correlations between text similarity of
  outputs for different countries and the cultural values of these countries.
  Finally, we discuss important considerations in designing comprehensive
  evaluation of cultural competence in user-facing tasks.</td>
  <td colspan=2 style='mso-ignore:colspan'>3 October, 2024</td>
 </tr>
 <tr height=249 style='height:187.0pt'>
  <td height=249 align=right style='height:187.0pt'>2168</td>
  <td align=right>2406.11661</td>
  <td class=xl65 width=649 style='width:487pt'>Cultural Conditioning or
  Placebo? On the Effectiveness of Socio-Demographic Prompting</td>
  <td class=xl65 width=649 style='width:487pt'>Socio-demographic prompting is a
  commonly employed approach to study cultural biases in LLMs as well as for
  aligning models to certain cultures. In this paper, we systematically probe
  four LLMs (Llama 3, Mistral v0.2, GPT-3.5 Turbo and GPT-4) with prompts that
  are conditioned on culturally sensitive and non-sensitive cues, on datasets
  that are supposed to be culturally sensitive (EtiCor and CALI) or neutral
  (MMLU and ETHICS). We observe that all models except GPT-4 show significant
  variations in their responses on both kinds of datasets for both kinds of
  prompts, casting doubt on the robustness of the culturally-conditioned
  prompting as a method for eliciting cultural bias in models or as an
  alignment strategy. The work also calls rethinking the control experiment
  design to tease apart the cultural conditioning of responses from
  &quot;placebo effect&quot;, i.e., random perturbations of model responses due
  to arbitrary tokens in the prompt.</td>
  <td colspan=2 style='mso-ignore:colspan'>20 June, 2024</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>2167</td>
  <td align=right>2406.11665</td>
  <td class=xl65 width=649 style='width:487pt'>See It from My Perspective:
  Diagnosing the Western Cultural Bias of Large Vision-Language Models in Image
  Understanding</td>
  <td class=xl65 width=649 style='width:487pt'>Vision-language models (VLMs)
  can respond to queries about images in many languages. However, beyond
  language, culture affects how we see things. For example, individuals from
  Western cultures focus more on the central figure in an image while
  individuals from Eastern cultures attend more to scene context. In this work,
  we present a novel investigation that demonstrates and localizes VLMs'
  Western bias in image understanding. We evaluate large VLMs across subjective
  and objective visual tasks with culturally diverse images and annotations. We
  find that VLMs perform better on the Western subset than the Eastern subset
  of each task. Controlled experimentation tracing the source of this bias
  highlights the importance of a diverse language mix in text-only pre-training
  for building equitable VLMs, even when inference is performed in English.
  Moreover, while prompting in the language of a target culture can lead to
  reductions in bias, it is not a substitute for building AI more
  representative of the world's languages.</td>
  <td colspan=2 style='mso-ignore:colspan'>17 June, 2024</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>2119</td>
  <td align=right>2406.14504</td>
  <td class=xl65 width=649 style='width:487pt'>Translating Across Cultures:
  LLMs for Intralingual Cultural Adaptation</td>
  <td class=xl65 width=649 style='width:487pt'>LLMs are increasingly being
  deployed for multilingual applications and have demonstrated impressive
  translation capabilities between several low and high-resource languages. An
  aspect of translation that often gets overlooked is that of cultural adaptation,
  or modifying source culture references to suit the target culture. While
  specialized translation models still outperform LLMs on the machine
  translation task when viewed from the lens of correctness, they are not
  sensitive to cultural differences often requiring manual correction. LLMs on
  the other hand have a rich reservoir of cultural knowledge embedded within
  its parameters that can be potentially exploited for such applications. In
  this paper, we define the task of cultural adaptation and create an
  evaluation framework to evaluate the performance of modern LLMs for cultural
  adaptation and analyze their cross-cultural knowledge while connecting
  related concepts across different cultures. We also analyze possible issues
  with automatic adaptation. We hope that this task will offer more insight
  into the cultural understanding of LLMs and their creativity in
  cross-cultural scenarios.</td>
  <td colspan=2 style='mso-ignore:colspan'>14 October, 2024</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>2113</td>
  <td align=right>2406.14805</td>
  <td class=xl65 width=649 style='width:487pt'>How Well Do LLMs Represent
  Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede
  Cultural Dimensions</td>
  <td class=xl65 width=649 style='width:487pt'>Large Language Models (LLMs)
  attempt to imitate human behavior by responding to humans in a way that
  pleases them, including by adhering to their values. However, humans come
  from diverse cultures with different values. It is critical to understand
  whether LLMs showcase different values to the user based on the stereotypical
  values of a user's known country. We prompt different LLMs with a series of
  advice requests based on 5 Hofstede Cultural Dimensions -- a quantifiable way
  of representing the values of a country. Throughout each prompt, we
  incorporate personas representing 36 different countries and, separately,
  languages predominantly tied to each country to analyze the consistency in
  the LLMs' cultural understanding. Through our analysis of the responses, we
  found that LLMs can differentiate between one side of a value and another, as
  well as understand that countries have differing values, but will not always
  uphold the values when giving advice, and fail to understand the need to
  answer differently based on different cultural values. Rooted in these
  findings, we present recommendations for training value-aligned and
  culturally sensitive LLMs. More importantly, the methodology and the
  framework developed here can help further understand and mitigate culture and
  language alignment issues with LLMs.</td>
  <td colspan=2 style='mso-ignore:colspan'>20 June, 2024</td>
 </tr>
 <tr height=408 style='height:306.0pt'>
  <td height=408 align=right style='height:306.0pt'>2107</td>
  <td align=right>2406.14966</td>
  <td class=xl65 width=649 style='width:487pt'>AIGC-Chain: A Blockchain-Enabled
  Full Lifecycle Recording System for AIGC Product Copyright Management</td>
  <td class=xl65 width=649 style='width:487pt'>As artificial intelligence
  technology becomes increasingly prevalent, Artificial Intelligence Generated
  Content (AIGC) is being adopted across various sectors. Although AIGC is
  playing an increasingly significant role in business and culture, questions surrounding
  its copyright have sparked widespread debate. The current legal framework for
  copyright and intellectual property is grounded in the concept of human
  authorship, but in the creation of AIGC, human creators primarily provide
  conceptual ideas, with AI independently responsible for the expressive
  elements. This disconnect creates complexity and difficulty in determining
  copyright ownership under existing laws. Consequently, it is imperative to
  reassess the intellectual contributions of all parties involved in the
  creation of AIGC to ensure a fair allocation of copyright ownership. To
  address this challenge, we introduce AIGC-Chain, a blockchain-enabled full
  lifecycle recording system designed to manage the copyright of AIGC products.
  It is engineered to meticulously document the entire lifecycle of AIGC
  products, providing a transparent and dependable platform for copyright
  management. Furthermore, we propose a copyright tracing method based on an
  Indistinguishable Bloom Filter, named IBFT, which enhances the efficiency of
  blockchain transaction queries and significantly reduces the risk of
  fraudulent copyright claims for AIGC products. In this way, auditors can
  analyze the copyright of AIGC products by reviewing all relevant information
  retrieved from the blockchain.</td>
  <td colspan=2 style='mso-ignore:colspan'>21 June, 2024</td>
 </tr>
 <tr height=453 style='height:340.0pt'>
  <td height=453 align=right style='height:340.0pt'>2099</td>
  <td align=right>2406.15371</td>
  <td class=xl65 width=649 style='width:487pt'>Affirmative safety: An approach
  to risk management for high-risk AI</td>
  <td class=xl65 width=649 style='width:487pt'>Prominent AI experts have
  suggested that companies developing high-risk AI systems should be required
  to show that such systems are safe before they can be developed or deployed.
  The goal of this paper is to expand on this idea and explore its implications
  for risk management. We argue that entities developing or deploying high-risk
  AI systems should be required to present evidence of affirmative safety: a
  proactive case that their activities keep risks below acceptable thresholds.
  We begin the paper by highlighting global security risks from AI that have
  been acknowledged by AI experts and world governments. Next, we briefly
  describe principles of risk management from other high-risk fields (e.g.,
  nuclear safety). Then, we propose a risk management approach for advanced AI
  in which model developers must provide evidence that their activities keep
  certain risks below regulator-set thresholds. As a first step toward
  understanding what affirmative safety cases should include, we illustrate how
  certain kinds of technical evidence and operational evidence can support an
  affirmative safety case. In the technical section, we discuss behavioral
  evidence (evidence about model outputs), cognitive evidence (evidence about
  model internals), and developmental evidence (evidence about the training
  process). In the operational section, we offer examples of organizational
  practices that could contribute to affirmative safety cases: information
  security practices, safety culture, and emergency response capacity. Finally,
  we briefly compare our approach to the NIST AI Risk Management Framework.
  Overall, we hope our work contributes to ongoing discussions about national
  and global security risks posed by AI and regulatory approaches to address
  these risks.</td>
  <td colspan=2 style='mso-ignore:colspan'>14 April, 2024</td>
 </tr>
 <tr height=408 style='height:306.0pt'>
  <td height=408 align=right style='height:306.0pt'>2092</td>
  <td align=right>2406.15948</td>
  <td class=xl65 width=649 style='width:487pt'>Teaching LLMs to Abstain across
  Languages via Multilingual Feedback</td>
  <td class=xl65 width=649 style='width:487pt'>Multilingual LLMs often have
  knowledge disparities across languages, with larger gaps in under-resourced
  languages. Teaching LLMs to abstain in the face of knowledge gaps is thus a
  promising strategy to mitigate hallucinations in multilingual settings. However,
  previous studies on LLM abstention primarily focus on English; we find that
  directly applying existing solutions beyond English results in up to 20.5%
  performance gaps between high and low-resource languages, potentially due to
  LLMs' drop in calibration and reasoning beyond a few resource-rich languages.
  To this end, we propose strategies to enhance LLM abstention by learning from
  multilingual feedback, where LLMs self-reflect on proposed answers in one
  language by generating multiple feedback items in related languages: we show
  that this helps identifying the knowledge gaps across diverse languages,
  cultures, and communities. Extensive experiments demonstrate that our
  multilingual feedback approach outperforms various strong baselines, achieving
  up to 9.2% improvement for low-resource languages across three black-box and
  open models on three datasets, featuring open-book, closed-book, and
  commonsense QA. Further analysis reveals that multilingual feedback is both
  an effective and a more equitable abstain strategy to serve diverse language
  speakers, and cultural factors have great impact on language selection and
  LLM abstention behavior, highlighting future directions for multilingual and
  multi-cultural reliable language modeling.</td>
  <td colspan=2 style='mso-ignore:colspan'>10 October, 2024</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>2091</td>
  <td align=right>2406.15951</td>
  <td class=xl65 width=649 style='width:487pt'>Modular Pluralism: Pluralistic
  Alignment via Multi-LLM Collaboration</td>
  <td class=xl65 width=649 style='width:487pt'>While existing alignment
  paradigms have been integral in developing large language models (LLMs), LLMs
  often learn an averaged human preference and struggle to model diverse
  preferences across cultures, demographics, and communities. We propose
  Modular Pluralism, a modular framework based on multi-LLM collaboration for
  pluralistic alignment: it &quot;plugs into&quot; a base LLM a pool of smaller
  but specialized community LMs, where models collaborate in distinct modes to
  flexibility support three modes of pluralism: Overton, steerable, and
  distributional. Modular Pluralism is uniquely compatible with black-box LLMs
  and offers the modular control of adding new community LMs for previously
  underrepresented communities. We evaluate Modular Pluralism with six tasks
  and four datasets featuring questions/instructions with value-laden and
  perspective-informed responses. Extensive experiments demonstrate that
  Modular Pluralism advances the three pluralism objectives across six
  black-box and open-source LLMs. Further analysis reveals that LLMs are
  generally faithful to the inputs from smaller community LLMs, allowing
  seamless patching by adding a new community LM to better cover previously
  underrepresented communities.</td>
  <td colspan=2 style='mso-ignore:colspan'>10 October, 2024</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>2081</td>
  <td align=right>2406.16316</td>
  <td class=xl65 width=649 style='width:487pt'>Does Cross-Cultural Alignment
  Change the Commonsense Morality of Language Models?</td>
  <td class=xl65 width=649 style='width:487pt'>Alignment of the language model
  with human preferences is a common approach to making a language model useful
  to end users. However, most alignment work is done in English, and human
  preference datasets are dominated by English, reflecting only the preferences
  of English-speaking annotators. Nevertheless, it is common practice to use
  the English preference data, either directly or by translating it into the
  target language, when aligning a multilingual language model. The question is
  whether such an alignment strategy marginalizes the preference of non-English
  speaking users. To this end, we investigate the effect of aligning Japanese
  language models with (mostly) English resources. In particular, we focus on
  evaluating whether the commonsense morality of the resulting fine-tuned
  models is aligned with Japanese culture using the JCommonsenseMorality (JCM)
  and ETHICS datasets. The experimental results show that the fine-tuned model
  outperforms the SFT model. However, it does not demonstrate the same level of
  improvement as a model fine-tuned using the JCM, suggesting that while some
  aspects of commonsense morality are transferable, others may not be.</td>
  <td colspan=2 style='mso-ignore:colspan'>24 June, 2024</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>2078</td>
  <td align=right>2406.16469</td>
  <td class=xl65 width=649 style='width:487pt'>Evaluating Visual and Cultural
  Interpretation: The K-Viscuit Benchmark with Human-VLM Collaboration</td>
  <td class=xl65 width=649 style='width:487pt'>To create culturally inclusive
  vision-language models (VLMs), developing a benchmark that tests their
  ability to address culturally relevant questions is essential. Existing
  approaches typically rely on human annotators, making the process
  labor-intensive and creating a cognitive burden in generating diverse
  questions. To address this, we propose a semi-automated framework for
  constructing cultural VLM benchmarks, specifically targeting multiple-choice
  QA. This framework combines human-VLM collaboration, where VLMs generate
  questions based on guidelines, a small set of annotated examples, and
  relevant knowledge, followed by a verification process by native speakers. We
  demonstrate the effectiveness of this framework through the creation of
  K-Viscuit, a dataset focused on Korean culture. Our experiments on this
  dataset reveal that open-source models lag behind proprietary ones in
  understanding Korean culture, highlighting key areas for improvement. We also
  present a series of further analyses, including human evaluation, augmenting
  VLMs with external knowledge, and the evaluation beyond multiple-choice QA.
  Our dataset is available at https://huggingface.co/datasets/ddehun/k-viscuit.</td>
  <td colspan=2 style='mso-ignore:colspan'>17 December, 2024</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>2070</td>
  <td align=right>2406.17325</td>
  <td class=xl65 width=649 style='width:487pt'>AI Tool Use and Adoption in
  Software Development by Individuals and Organizations: A Grounded Theory
  Study</td>
  <td class=xl65 width=649 style='width:487pt'>AI assistance tools such as
  ChatGPT, Copilot, and Gemini have dramatically impacted the nature of
  software development in recent years. Numerous studies have studied the
  positive benefits that practitioners have achieved from using these tools in
  their work. While there is a growing body of knowledge regarding the
  usability aspects of leveraging AI tools, we still lack concrete details on
  the issues that organizations and practitioners need to consider should they
  want to explore increasing adoption or use of AI tools. In this study, we
  conducted a mixed methods study involving interviews with 26 industry
  practitioners and 395 survey respondents. We found that there are several
  motives and challenges that impact individuals and organizations and developed
  a theory of AI Tool Adoption. For example, we found creating a culture of
  sharing of AI best practices and tips as a key motive for practitioners'
  adopting and using AI tools. In total, we identified 2 individual motives, 4
  individual challenges, 3 organizational motives, and 3 organizational
  challenges, and 3 interleaved relationships. The 3 interleaved relationships
  act in a push-pull manner where motives pull practitioners to increase the
  use of AI tools and challenges push practitioners away from using AI tools.</td>
  <td colspan=2 style='mso-ignore:colspan'>25 June, 2024</td>
 </tr>
 <tr height=227 style='height:170.0pt'>
  <td height=227 align=right style='height:170.0pt'>2064</td>
  <td align=right>2406.17531</td>
  <td class=xl65 width=649 style='width:487pt'>Enhancing LLM-Based Human-Robot
  Interaction with Nuances for Diversity Awareness</td>
  <td class=xl65 width=649 style='width:487pt'>This paper presents a system for
  diversity-aware autonomous conversation leveraging the capabilities of large
  language models (LLMs). The system adapts to diverse populations and
  individuals, considering factors like background, personality, age, gender,
  and culture. The conversation flow is guided by the structure of the system's
  pre-established knowledge base, while LLMs are tasked with various functions,
  including generating diversity-aware sentences. Achieving diversity-awareness
  involves providing carefully crafted prompts to the models, incorporating
  comprehensive information about users, conversation history, contextual
  details, and specific guidelines. To assess the system's performance, we
  conducted both controlled and real-world experiments, measuring a wide range
  of performance indicators.</td>
  <td colspan=2 style='mso-ignore:colspan'>25 June, 2024</td>
 </tr>
 <tr height=408 style='height:306.0pt'>
  <td height=408 align=right style='height:306.0pt'>2056</td>
  <td align=right>2406.17761</td>
  <td class=xl65 width=649 style='width:487pt'>CaLMQA: Exploring culturally
  specific long-form question answering across 23 languages</td>
  <td class=xl65 width=649 style='width:487pt'>Large language models (LLMs) are
  used for long-form question answering (LFQA), which requires them to generate
  paragraph-length answers to complex questions. While LFQA has been
  well-studied in English, this research has not been extended to other languages.
  To bridge this gap, we introduce CaLMQA, a collection of 1.5K complex
  culturally specific questions spanning 23 languages and 51 culturally
  agnostic questions translated from English into 22 other languages. We define
  culturally specific questions as those uniquely or more likely to be asked by
  people from cultures associated with the question's language. We collect
  naturally-occurring questions from community web forums and hire native
  speakers to write questions to cover under-resourced, rarely-studied
  languages such as Fijian and Kirundi. Our dataset contains diverse, complex
  questions that reflect cultural topics (e.g. traditions, laws, news) and the
  language usage of native speakers. We automatically evaluate a suite of open-
  and closed-source models on CaLMQA by detecting incorrect language and token
  repetitions in answers, and observe that the quality of LLM-generated answers
  degrades significantly for some low-resource languages. Lastly, we perform
  human evaluation on a subset of models and languages. Manual evaluation
  reveals that model performance is significantly worse for culturally specific
  questions than for culturally agnostic questions. Our findings highlight the
  need for further research in non-English LFQA and provide an evaluation
  framework.</td>
  <td>3 July, 2024</td>
  <td></td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>2026</td>
  <td align=right>2406.19504</td>
  <td class=xl65 width=649 style='width:487pt'>Are Generative Language Models
  Multicultural? A Study on Hausa Culture and Emotions using ChatGPT</td>
  <td class=xl65 width=649 style='width:487pt'>Large Language Models (LLMs),
  such as ChatGPT, are widely used to generate content for various purposes and
  audiences. However, these models may not reflect the cultural and emotional
  diversity of their users, especially for low-resource languages. In this
  paper, we investigate how ChatGPT represents Hausa's culture and emotions. We
  compare responses generated by ChatGPT with those provided by native Hausa
  speakers on 37 culturally relevant questions. We conducted experiments using
  emotion analysis and applied two similarity metrics to measure the alignment
  between human and ChatGPT responses. We also collected human participants
  ratings and feedback on ChatGPT responses. Our results show that ChatGPT has
  some level of similarity to human responses, but also exhibits some gaps and
  biases in its knowledge and awareness of the Hausa culture and emotions. We
  discuss the implications and limitations of our methodology and analysis and
  suggest ways to improve the performance and evaluation of LLMs for low-resource
  languages.</td>
  <td colspan=2 style='mso-ignore:colspan'>27 June, 2024</td>
 </tr>
 <tr height=249 style='height:187.0pt'>
  <td height=249 align=right style='height:187.0pt'>2018</td>
  <td align=right>2407.00263</td>
  <td class=xl65 width=649 style='width:487pt'>From Local Concepts to
  Universals: Evaluating the Multicultural Understanding of Vision-Language
  Models</td>
  <td class=xl65 width=649 style='width:487pt'>Despite recent advancements in
  vision-language models, their performance remains suboptimal on images from
  non-western cultures due to underrepresentation in training datasets. Various
  benchmarks have been proposed to test models' cultural inclusivity, but they
  have limited coverage of cultures and do not adequately assess cultural
  diversity across universal as well as culture-specific local concepts. To
  address these limitations, we introduce the GlobalRG benchmark, comprising
  two challenging tasks: retrieval across universals and cultural visual
  grounding. The former task entails retrieving culturally diverse images for
  universal concepts from 50 countries, while the latter aims at grounding
  culture-specific concepts within images from 15 countries. Our evaluation
  across a wide range of models reveals that the performance varies
  significantly across cultures -- underscoring the necessity for enhancing
  multicultural understanding in vision-language models.</td>
  <td colspan=2 style='mso-ignore:colspan'>28 June, 2024</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>2003</td>
  <td align=right>2407.01081</td>
  <td class=xl65 width=649 style='width:487pt'>CVLUE: A New Benchmark Dataset
  for Chinese Vision-Language Understanding Evaluation</td>
  <td class=xl65 width=649 style='width:487pt'>Despite the rapid development of
  Chinese vision-language models (VLMs), most existing Chinese vision-language
  (VL) datasets are constructed on Western-centric images from existing English
  VL datasets. The cultural bias in the images makes these datasets unsuitable
  for evaluating VLMs in Chinese culture. To remedy this issue, we present a
  new Chinese Vision- Language Understanding Evaluation (CVLUE) benchmark
  dataset, where the selection of object categories and images is entirely
  driven by Chinese native speakers, ensuring that the source images are
  representative of Chinese culture. The benchmark contains four distinct VL
  tasks ranging from image-text retrieval to visual question answering, visual
  grounding and visual dialogue. We present a detailed statistical analysis of
  CVLUE and provide a baseline performance analysis with several open-source
  multilingual VLMs on CVLUE and its English counterparts to reveal their
  performance gap between English and Chinese. Our in-depth category-level
  analysis reveals a lack of Chinese cultural knowledge in existing VLMs. We
  also find that fine-tuning on Chinese culture-related VL datasets effectively
  enhances VLMs' understanding of Chinese culture.</td>
  <td>1 July, 2024</td>
  <td></td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>1985</td>
  <td align=right>2407.02067</td>
  <td class=xl65 width=649 style='width:487pt'>Crossroads of Continents:
  Automated Artifact Extraction for Cultural Adaptation with Large Multimodal
  Models</td>
  <td class=xl65 width=649 style='width:487pt'>We present a comprehensive
  three-phase study to examine (1) the cultural understanding of Large
  Multimodal Models (LMMs) by introducing DalleStreet, a large-scale dataset
  generated by DALL-E 3 and validated by humans, containing 9,935 images of 67
  countries and 10 concept classes; (2) the underlying implicit and potentially
  stereotypical cultural associations with a cultural artifact extraction task;
  and (3) an approach to adapt cultural representation in an image based on
  extracted associations using a modular pipeline, CultureAdapt. We find
  disparities in cultural understanding at geographic sub-region levels with
  both open-source (LLaVA) and closed-source (GPT-4V) models on DalleStreet and
  other existing benchmarks, which we try to understand using over 18,000
  artifacts that we identify in association to different countries. Our
  findings reveal a nuanced picture of the cultural competence of LMMs,
  highlighting the need to develop culture-aware systems. Dataset and code are
  available at https://github.com/iamshnoo/crossroads</td>
  <td colspan=2 style='mso-ignore:colspan'>18 October, 2024</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>1968</td>
  <td align=right>2407.03473</td>
  <td class=xl65 width=649 style='width:487pt'>Exploring LGBTQ+ Bias in
  Generative AI Answers across Different Country and Religious Contexts</td>
  <td class=xl65 width=649 style='width:487pt'>Previous discussions have
  highlighted the need for generative AI tools to become more culturally
  sensitive, yet often neglect the complexities of handling content about
  minorities, who are perceived differently across cultures and religions. Our
  study examined how two generative AI systems respond to homophobic statements
  with varying cultural and religious context information. Findings showed
  ChatGPT 3.5's replies exhibited cultural relativism, in contrast to Bard's,
  which stressed human rights and provided more support for LGBTQ+ issues. Both
  demonstrated significant change in responses based on contextual information
  provided in the prompts, suggesting that AI systems may adjust in their
  responses the degree and forms of support for LGBTQ+ people according to
  information they receive about the user's background. The study contributes
  to understanding the social and ethical implications of AI responses and
  argues that any work to make generative AI outputs more culturally diverse
  requires a grounding in fundamental human rights.</td>
  <td>3 July, 2024</td>
  <td></td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>1952</td>
  <td align=right>2407.04721</td>
  <td class=xl65 width=649 style='width:487pt'>AgriLLM: Harnessing Transformers
  for Farmer Queries</td>
  <td class=xl65 width=649 style='width:487pt'>Agriculture, vital for global
  sustenance, necessitates innovative solutions due to a lack of organized
  domain experts, particularly in developing countries where many farmers are
  impoverished and cannot afford expert consulting. Initiatives like Farmers Helpline
  play a crucial role in such countries, yet challenges such as high
  operational costs persist. Automating query resolution can alleviate the
  burden on traditional call centers, providing farmers with immediate and
  contextually relevant information. The integration of Agriculture and
  Artificial Intelligence (AI) offers a transformative opportunity to empower
  farmers and bridge information gaps. Language models like transformers, the
  rising stars of AI, possess remarkable language understanding capabilities,
  making them ideal for addressing information gaps in agriculture. This work
  explores and demonstrates the transformative potential of Large Language
  Models (LLMs) in automating query resolution for agricultural farmers,
  leveraging their expertise in deciphering natural language and understanding
  context. Using a subset of a vast dataset of real-world farmer queries
  collected in India, our study focuses on approximately 4 million queries from
  the state of Tamil Nadu, spanning various sectors, seasonal crops, and query
  types.</td>
  <td colspan=2 style='mso-ignore:colspan'>1 October, 2024</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>1944</td>
  <td align=right>2407.05377</td>
  <td class=xl65 width=649 style='width:487pt'>Collective Innovation in Groups
  of Large Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>Human culture relies on
  collective innovation: our ability to continuously explore how existing
  elements in our environment can be combined to create new ones. Language is
  hypothesized to play a key role in human culture, driving individual
  cognitive capacities and shaping communication. Yet the majority of models of
  collective innovation assign no cognitive capacities or language abilities to
  agents. Here, we contribute a computational study of collective innovation
  where agents are Large Language Models (LLMs) that play Little Alchemy 2, a
  creative video game originally developed for humans that, as we argue,
  captures useful aspects of innovation landscapes not present in previous
  test-beds. We, first, study an LLM in isolation and discover that it exhibits
  both useful skills and crucial limitations. We, then, study groups of LLMs
  that share information related to their behaviour and focus on the effect of
  social connectivity on collective performance. In agreement with previous
  human and computational studies, we observe that groups with dynamic
  connectivity out-compete fully-connected groups. Our work reveals
  opportunities and challenges for future studies of collective innovation that
  are becoming increasingly relevant as Generative Artificial Intelligence
  algorithms and humans innovate alongside each other.</td>
  <td>7 July, 2024</td>
  <td></td>
 </tr>
 <tr height=227 style='height:170.0pt'>
  <td height=227 align=right style='height:170.0pt'>1931</td>
  <td align=right>2407.06177</td>
  <td class=xl65 width=649 style='width:487pt'>Vision-Language Models under
  Cultural and Inclusive Considerations</td>
  <td class=xl65 width=649 style='width:487pt'>Large vision-language models
  (VLMs) can assist visually impaired people by describing images from their
  daily lives. Current evaluation datasets may not reflect diverse cultural
  user backgrounds or the situational context of this use case. To address this
  problem, we create a survey to determine caption preferences and propose a
  culture-centric evaluation benchmark by filtering VizWiz, an existing dataset
  with images taken by people who are blind. We then evaluate several VLMs,
  investigating their reliability as visual assistants in a culturally diverse
  setting. While our results for state-of-the-art models are promising, we
  identify challenges such as hallucination and misalignment of automatic
  evaluation metrics with human judgment. We make our survey, data, code, and
  model outputs publicly available.</td>
  <td>8 July, 2024</td>
  <td></td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>1930</td>
  <td align=right>2407.06196</td>
  <td class=xl65 width=649 style='width:487pt'>Poetry2Image: An Iterative
  Correction Framework for Images Generated from Chinese Classical Poetry</td>
  <td class=xl65 width=649 style='width:487pt'>Text-to-image generation models
  often struggle with key element loss or semantic confusion in tasks involving
  Chinese classical poetry.Addressing this issue through fine-tuning models
  needs considerable training costs. Additionally, manual prompts for re-diffusion
  adjustments need professional knowledge. To solve this problem, we propose
  Poetry2Image, an iterative correction framework for images generated from
  Chinese classical poetry. Utilizing an external poetry dataset, Poetry2Image
  establishes an automated feedback and correction loop, which enhances the
  alignment between poetry and image through image generation models and
  subsequent re-diffusion modifications suggested by large language models
  (LLM). Using a test set of 200 sentences of Chinese classical poetry, the
  proposed method--when integrated with five popular image generation
  models--achieves an average element completeness of 70.63%, representing an
  improvement of 25.56% over direct image generation. In tests of semantic
  correctness, our method attains an average semantic consistency of 80.09%.
  The study not only promotes the dissemination of ancient poetry culture but
  also offers a reference for similar non-fine-tuning methods to enhance LLM
  generation.</td>
  <td colspan=2 style='mso-ignore:colspan'>15 June, 2024</td>
 </tr>
 <tr height=431 style='height:323.0pt'>
  <td height=431 align=right style='height:323.0pt'>1923</td>
  <td align=right>2407.06863</td>
  <td class=xl65 width=649 style='width:487pt'>Beyond Aesthetics: Cultural
  Competence in Text-to-Image Models</td>
  <td class=xl65 width=649 style='width:487pt'>Text-to-Image (T2I) models are
  being increasingly adopted in diverse global communities where they create
  visual representations of their unique cultures. Current T2I benchmarks
  primarily focus on faithfulness, aesthetics, and realism of generated images,
  overlooking the critical dimension of cultural competence. In this work, we
  introduce a framework to evaluate cultural competence of T2I models along two
  crucial dimensions: cultural awareness and cultural diversity, and present a
  scalable approach using a combination of structured knowledge bases and large
  language models to build a large dataset of cultural artifacts to enable this
  evaluation. In particular, we apply this approach to build CUBE (CUltural
  BEnchmark for Text-to-Image models), a first-of-its-kind benchmark to
  evaluate cultural competence of T2I models. CUBE covers cultural artifacts
  associated with 8 countries across different geo-cultural regions and along 3
  concepts: cuisine, landmarks, and art. CUBE consists of 1) CUBE-1K, a set of
  high-quality prompts that enable the evaluation of cultural awareness, and 2)
  CUBE-CSpace, a larger dataset of cultural artifacts that serves as grounding
  to evaluate cultural diversity. We also introduce cultural diversity as a
  novel T2I evaluation component, leveraging quality-weighted Vendi score. Our
  evaluations reveal significant gaps in the cultural awareness of existing
  models across countries and provide valuable insights into the cultural
  diversity of T2I outputs for under-specified prompts. Our methodology is
  extendable to other cultural regions and concepts, and can facilitate the
  development of T2I models that better cater to the global population.</td>
  <td colspan=2 style='mso-ignore:colspan'>7 November, 2024</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>1872</td>
  <td align=right>2407.10371</td>
  <td class=xl65 width=649 style='width:487pt'>The Silent Curriculum: How Does
  LLM Monoculture Shape Educational Content and Its Accessibility?</td>
  <td class=xl65 width=649 style='width:487pt'>As Large Language Models (LLMs)
  ascend in popularity, offering information with unprecedented convenience
  compared to traditional search engines, we delve into the intriguing
  possibility that a new, singular perspective is being propagated. We call
  this the &quot;Silent Curriculum,&quot; where our focus shifts towards a
  particularly impressionable demographic: children, who are drawn to the ease
  and immediacy of acquiring knowledge through these digital oracles. In this
  exploration, we delve into the sociocultural ramifications of LLMs, which,
  through their nuanced responses, may be subtly etching their own stereotypes,
  an algorithmic or AI monoculture. We hypothesize that the convergence of
  pre-training data, fine-tuning datasets, and analogous guardrails across
  models may have birthed a distinct cultural lens. We unpack this concept
  through a short experiment navigating children's storytelling,
  occupational-ethnic biases, and self-diagnosed annotations, to find that
  there exists strong cosine similarity (0.87) of biases across these models,
  suggesting a similar perspective of ethnic stereotypes in occupations. This
  paper invites a reimagining of LLMs' societal role, especially as the new
  information gatekeepers, advocating for a paradigm shift towards
  diversity-rich landscapes over unintended monocultures.</td>
  <td colspan=2 style='mso-ignore:colspan'>11 May, 2024</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>1865</td>
  <td align=right>2407.1092</td>
  <td class=xl65 width=649 style='width:487pt'>Benchmarking Vision Language
  Models for Cultural Understanding</td>
  <td class=xl65 width=649 style='width:487pt'>Foundation models and
  vision-language pre-training have notably advanced Vision Language Models
  (VLMs), enabling multimodal processing of visual and linguistic data.
  However, their performance has been typically assessed on general scene
  understanding - recognizing objects, attributes, and actions - rather than
  cultural comprehension. This study introduces CulturalVQA, a visual
  question-answering benchmark aimed at assessing VLM's geo-diverse cultural
  understanding. We curate a collection of 2,378 image-question pairs with 1-5
  answers per question representing cultures from 11 countries across 5
  continents. The questions probe understanding of various facets of culture
  such as clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on
  CulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of
  cultural understanding across regions, with strong cultural understanding
  capabilities for North America while significantly lower performance for
  Africa. We observe disparity in their performance across cultural facets too,
  with clothing, rituals, and traditions seeing higher performances than food
  and drink. These disparities help us identify areas where VLMs lack cultural
  understanding and demonstrate the potential of CulturalVQA as a comprehensive
  evaluation set for gauging VLM progress in understanding diverse cultures.</td>
  <td colspan=2 style='mso-ignore:colspan'>14 October, 2024</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>1839</td>
  <td align=right>2407.12196</td>
  <td class=xl65 width=649 style='width:487pt'>MASIVE: Open-Ended Affective
  State Identification in English and Spanish</td>
  <td class=xl65 width=649 style='width:487pt'>In the field of emotion
  analysis, much NLP research focuses on identifying a limited number of
  discrete emotion categories, often applied across languages. These basic
  sets, however, are rarely designed with textual data in mind, and culture,
  language, and dialect can influence how particular emotions are interpreted.
  In this work, we broaden our scope to a practically unbounded set of
  \textit{affective states}, which includes any terms that humans use to
  describe their experiences of feeling. We collect and publish MASIVE, a
  dataset of Reddit posts in English and Spanish containing over 1,000 unique
  affective states each. We then define the new problem of \textit{affective
  state identification} for language generation models framed as a masked span
  prediction task. On this task, we find that smaller finetuned multilingual
  models outperform much larger LLMs, even on region-specific Spanish affective
  states. Additionally, we show that pretraining on MASIVE improves model
  performance on existing emotion benchmarks. Finally, through machine
  translation experiments, we find that native speaker-written data is vital to
  good performance on this task.</td>
  <td colspan=2 style='mso-ignore:colspan'>12 November, 2024</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>1829</td>
  <td align=right>2407.12791</td>
  <td class=xl65 width=649 style='width:487pt'>TourLLM: Enhancing LLMs with
  Tourism Knowledge</td>
  <td class=xl65 width=649 style='width:487pt'>Recently, large language models
  (LLMs) have demonstrated their effectiveness in various natural language
  processing (NLP) tasks. However, the lack of tourism knowledge limits the
  performance of LLMs in tourist attraction presentations and travel planning.
  To address this challenge, we constructed a supervised fine-tuning dataset
  for the culture and tourism domain, named Cultour. This dataset consists of
  three parts: tourism knowledge base QA data, travelogues data, and tourism
  diversity QA data. Additionally, we propose TourLLM, a Qwen-based model
  supervised fine-tuned with Cultour, to improve the quality of the information
  provided about attractions and travel planning. To evaluate the performance
  of TourLLM, we employed both automatic and human evaluation, and we proposed
  a human evaluation criterion named CRA (Consistency, Readability,
  Availability). The experimental results demonstrate the effectiveness of the
  responses generated by the TourLLM. Our proposed Cultour is accessible at
  https://github.com/mrweiqk/Cultour.</td>
  <td colspan=2 style='mso-ignore:colspan'>18 June, 2024</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>1806</td>
  <td align=right>2407.13439</td>
  <td class=xl65 width=649 style='width:487pt'>Reducing Barriers to the Use of
  Marginalised Music Genres in AI</td>
  <td class=xl65 width=649 style='width:487pt'>AI systems for high quality
  music generation typically rely on extremely large musical datasets to train
  the AI models. This creates barriers to generating music beyond the genres
  represented in dominant datasets such as Western Classical music or pop music.
  We undertook a 4 month international research project summarised in this
  paper to explore the eXplainable AI (XAI) challenges and opportunities
  associated with reducing barriers to using marginalised genres of music with
  AI models. XAI opportunities identified included topics of improving
  transparency and control of AI models, explaining the ethics and bias of AI
  models, fine tuning large models with small datasets to reduce bias, and
  explaining style-transfer opportunities with AI models. Participants in the
  research emphasised that whilst it is hard to work with small datasets such
  as marginalised music and AI, such approaches strengthen cultural
  representation of underrepresented cultures and contribute to addressing
  issues of bias of deep learning models. We are now building on this project
  to bring together a global International Responsible AI Music community and
  invite people to join our network.</td>
  <td colspan=2 style='mso-ignore:colspan'>18 July, 2024</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>1787</td>
  <td align=right>2407.14779</td>
  <td class=xl65 width=649 style='width:487pt'>Do Generative AI Models Output
  Harm while Representing Non-Western Cultures: Evidence from A
  Community-Centered Approach</td>
  <td class=xl65 width=649 style='width:487pt'>Our research investigates the
  impact of Generative Artificial Intelligence (GAI) models, specifically
  text-to-image generators (T2Is), on the representation of non-Western
  cultures, with a focus on Indian contexts. Despite the transformative
  potential of T2Is in content creation, concerns have arisen regarding biases
  that may lead to misrepresentations and marginalizations. Through a
  community-centered approach and grounded theory analysis of 5 focus groups
  from diverse Indian subcultures, we explore how T2I outputs to English
  prompts depict Indian culture and its subcultures, uncovering novel
  representational harms such as exoticism and cultural misappropriation. These
  findings highlight the urgent need for inclusive and culturally sensitive T2I
  systems. We propose design guidelines informed by a sociotechnical
  perspective, aiming to address these issues and contribute to the development
  of more equitable and representative GAI technologies globally. Our work also
  underscores the necessity of adopting a community-centered approach to
  comprehend the sociotechnical dynamics of these models, complementing
  existing work in this space while identifying and addressing the potential
  negative repercussions and harms that may arise when these models are deployed
  on a global scale.</td>
  <td colspan=2 style='mso-ignore:colspan'>3 August, 2024</td>
 </tr>
 <tr height=431 style='height:323.0pt'>
  <td height=431 align=right style='height:323.0pt'>1784</td>
  <td align=right>2407.15184</td>
  <td class=xl65 width=649 style='width:487pt'>Decoding Multilingual Moral
  Preferences: Unveiling LLM's Biases Through the Moral Machine Experiment</td>
  <td class=xl65 width=649 style='width:487pt'>Large language models (LLMs)
  increasingly find their way into the most diverse areas of our everyday
  lives. They indirectly influence people's decisions or opinions through their
  daily use. Therefore, understanding how and which moral judgements these LLMs
  make is crucial. However, morality is not universal and depends on the
  cultural background. This raises the question of whether these cultural
  preferences are also reflected in LLMs when prompted in different languages
  or whether moral decision-making is consistent across different languages. So
  far, most research has focused on investigating the inherent values of LLMs
  in English. While a few works conduct multilingual analyses of moral bias in
  LLMs in a multilingual setting, these analyses do not go beyond atomic
  actions. To the best of our knowledge, a multilingual analysis of moral bias
  in dilemmas has not yet been conducted. To address this, our paper builds on
  the moral machine experiment (MME) to investigate the moral preferences of
  five LLMs, Falcon, Gemini, Llama, GPT, and MPT, in a multilingual setting and
  compares them with the preferences collected from humans belonging to
  different cultures. To accomplish this, we generate 6500 scenarios of the MME
  and prompt the models in ten languages on which action to take. Our analysis
  reveals that all LLMs inhibit different moral biases to some degree and that
  they not only differ from the human preferences but also across multiple
  languages within the models themselves. Moreover, we find that almost all
  models, particularly Llama 3, divert greatly from human values and, for
  instance, prefer saving fewer people over saving more.</td>
  <td colspan=2 style='mso-ignore:colspan'>21 July, 2024</td>
 </tr>
 <tr height=431 style='height:323.0pt'>
  <td height=431 align=right style='height:323.0pt'>1765</td>
  <td align=right>2407.16614</td>
  <td class=xl65 width=649 style='width:487pt'>Mobile Technology: A Panacea to
  Food Insecurity In Nigeria -- A Case Study of SELL HARVEST Application</td>
  <td class=xl65 width=649 style='width:487pt'>Over time, agriculture is the
  most consistent activity, and it evolves every day. It contributes to a vast
  majority of the Gross Domestic Product (GDP) of Nigeria but as ironic as it
  may be, there is still hunger in significant parts of the country due to low
  productivity in the agricultural sector and comparison to the geometric
  population growth. During the first half of 2022, agriculture contributed
  about 23% of the country's GDP while the industry and services sector had a
  share of the remaining 77%. This showed that with the high rate of
  agricultural activities, Nigeria has not achieved food security for the
  teeming population. and more productivity levels can be attained. Technology
  can/will assist Nigeria in overcoming global poverty and hunger quicker in
  both rural and urban areas. Today, there are many types of agricultural
  technologies available for farmers all over the world to increase
  productivity. Major technological advancements include indoor vertical
  farming, automation, robotics, livestock technology, modern greenhouse
  practices, precision agriculture, artificial intelligence, and blockchain.
  Mobile phones have one of the highest adoption rates of technologies
  developed within the last century. Digitalization will bring consumers and
  farmers closer together to access the shortest supply chain possible and
  reduce rural poverty and hunger. The paper will review the different
  agricultural technologies and propose a mobile solution, code Sell Harvest,
  to make farming more sustainable and secure food. Keywords: Sell Harvest,
  Agriculture, Technology, Artificial Intelligence, and Digital Farming.</td>
  <td colspan=2 style='mso-ignore:colspan'>23 July, 2024</td>
 </tr>
 <tr height=475 style='height:356.0pt'>
  <td height=475 align=right style='height:356.0pt'>1756</td>
  <td align=right>2407.17165</td>
  <td class=xl65 width=649 style='width:487pt'>Explainable Artificial
  Intelligence Techniques for Irregular Temporal Classification of Multidrug
  Resistance Acquisition in Intensive Care Unit Patients</td>
  <td class=xl65 width=649 style='width:487pt'>Antimicrobial Resistance
  represents a significant challenge in the Intensive Care Unit (ICU), where
  patients are at heightened risk of Multidrug-Resistant (MDR)
  infections-pathogens resistant to multiple antimicrobial agents. This study
  introduces a novel methodology that integrates Gated Recurrent Units (GRUs)
  with advanced intrinsic and post-hoc interpretability techniques for
  detecting the onset of MDR in patients across time. Within interpretability
  methods, we propose Explainable Artificial Intelligence (XAI) approaches to
  handle irregular Multivariate Time Series (MTS), introducing Irregular Time
  Shapley Additive Explanations (IT-SHAP), a modification of Shapley Additive
  Explanations designed for irregular MTS with Recurrent Neural Networks focused
  on temporal outputs. Our methodology aims to identify specific risk factors
  associated with MDR in ICU patients. GRU with Hadamard's attention
  demonstrated high initial specificity and increasing sensitivity over time,
  correlating with increased nosocomial infection risks during prolonged ICU
  stays. XAI analysis, enhanced by Hadamard attention and IT-SHAP, identified
  critical factors such as previous non-resistant cultures, specific antibiotic
  usage patterns, and hospital environment dynamics. These insights suggest
  that early detection of at-risk patients can inform interventions such as
  preventive isolation and customized treatments, significantly improving
  clinical outcomes. The proposed GRU model for temporal classification
  achieved an average Receiver Operating Characteristic Area Under the Curve of
  78.27 +- 1.26 over time, indicating strong predictive performance. In
  summary, this study highlights the clinical utility of our methodology, which
  combines predictive accuracy with interpretability, thereby facilitating more
  effective healthcare interventions by professionals.</td>
  <td colspan=2 style='mso-ignore:colspan'>24 July, 2024</td>
 </tr>
 <tr height=496 style='height:372.0pt'>
  <td height=496 align=right style='height:372.0pt'>1740</td>
  <td align=right>2407.18363</td>
  <td class=xl65 width=649 style='width:487pt'>KI-Bilder und die
  Widerständigkeit der Medienkonvergenz: Von primärer zu sekundärer
  Intermedialität?</td>
  <td class=xl65 width=649 style='width:487pt'>The article presents some
  current observations (as of April 10, 2024) on the integration of
  AI-generated images within processes of media convergence. It draws on two
  different concepts of intermediality. Primary intermediality concepts are
  motivated by the object when a new type of technology develops the potential
  to become socially relevant as a media form and thus a socially, politically,
  or culturally important communicative factor. Due to their uncertain
  'measurements' within the wider media ecology, however, the new, still
  potential media form appears hybrid. The &quot;inter-&quot; or
  &quot;between-&quot; of this initial intermediality moment thus refers to the
  questionable &quot;site&quot; and the questionable description of the
  potential media form between already existing technologies and cultural forms
  and their conceptual measurements. For secondary concepts of intermediality,
  in contrast, it can be assumed that the boundaries of media forms and their
  application have already been drawn and are reasonably undisputed. This then
  raises the question of intentional and staged references to AI imagery within
  other media forms and pictures. The article discusses indicators of both
  intermediality moments using current examples and controversies surrounding
  AI images. The thesis is that there can be no talk of a seamless
  'integration' of AI images into the wider media landscape at the moment
  (within films, comic books, or video games, for example) - as one of
  countless other image production techniques - and that the medial 'site' of
  AI image circulation - at least where it is not a matter of deception, but
  rather their conscious use as AI images - especially in social media
  communication and in fan cultures, but with repercussions for the more
  general media ecology and image interpretation, insofar as the suspicion that
  an image could be AI-generated is now increasingly present as a
  &quot;hermeneutics of suspicion&quot;.</td>
  <td colspan=2 style='mso-ignore:colspan'>21 June, 2024</td>
 </tr>
 <tr height=181 style='height:136.0pt'>
  <td height=181 align=right style='height:136.0pt'>1738</td>
  <td align=right>2407.18524</td>
  <td class=xl65 width=649 style='width:487pt'>She Works, He Works: A Curious
  Exploration of Gender Bias in AI-Generated Imagery</td>
  <td class=xl65 width=649 style='width:487pt'>This paper examines gender bias
  in AI-generated imagery of construction workers, highlighting discrepancies
  in the portrayal of male and female figures. Grounded in Griselda Pollock's
  theories on visual culture and gender, the analysis reveals that AI models
  tend to sexualize female figures while portraying male figures as more
  authoritative and competent. These findings underscore AI's potential to
  mirror and perpetuate societal biases, emphasizing the need for critical
  engagement with AI-generated content. The project contributes to discussions
  on the ethical implications of AI in creative practices and its broader
  impact on cultural perceptions of gender.</td>
  <td colspan=2 style='mso-ignore:colspan'>26 July, 2024</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>1735</td>
  <td align=right>2407.18787</td>
  <td class=xl65 width=649 style='width:487pt'>Automatic Detection of Moral
  Values in Music Lyrics</td>
  <td class=xl65 width=649 style='width:487pt'>Moral values play a fundamental
  role in how we evaluate information, make decisions, and form judgements
  around important social issues. The possibility to extract morality rapidly
  from lyrics enables a deeper understanding of our music-listening behaviours.
  Building on the Moral Foundations Theory (MFT), we tasked a set of
  transformer-based language models (BERT) fine-tuned on 2,721 synthetic lyrics
  generated by a large language model (GPT-4) to detect moral values in 200
  real music lyrics annotated by two experts.We evaluate their predictive
  capabilities against a series of baselines including out-of-domain (BERT
  fine-tuned on MFT-annotated social media texts) and zero-shot (GPT-4)
  classification. The proposed models yielded the best accuracy across experiments,
  with an average F1 weighted score of 0.8. This performance is, on average, 5%
  higher than out-of-domain and zero-shot models. When examining precision in
  binary classification, the proposed models perform on average 12% higher than
  the baselines.Our approach contributes to annotation-free and effective
  lyrics morality learning, and provides useful insights into the knowledge
  distillation of LLMs regarding moral expression in music, and the potential
  impact of these technologies on the creative industries and musical culture.</td>
  <td colspan=2 style='mso-ignore:colspan'>26 July, 2024</td>
 </tr>
 <tr height=385 style='height:289.0pt'>
  <td height=385 align=right style='height:289.0pt'>1725</td>
  <td align=right>2407.19672</td>
  <td class=xl65 width=649 style='width:487pt'>SeaLLMs 3: Open Foundation and
  Chat Multilingual Large Language Models for Southeast Asian Languages</td>
  <td class=xl65 width=649 style='width:487pt'>Large Language Models (LLMs)
  have shown remarkable abilities across various tasks, yet their development
  has predominantly centered on high-resource languages like English and
  Chinese, leaving low-resource languages underserved. To address this
  disparity, we present SeaLLMs 3, the latest iteration of the SeaLLMs model
  family, tailored for Southeast Asian languages. This region, characterized by
  its rich linguistic diversity, has lacked adequate language technology
  support. SeaLLMs 3 aims to bridge this gap by covering a comprehensive range
  of languages spoken in this region, including English, Chinese, Indonesian,
  Vietnamese, Thai, Tagalog, Malay, Burmese, Khmer, Lao, Tamil, and Javanese.
  Leveraging efficient language enhancement techniques and a specially
  constructed instruction tuning dataset, SeaLLMs 3 significantly reduces
  training costs while maintaining high performance and versatility. Our model
  excels in tasks such as world knowledge, mathematical reasoning, translation,
  and instruction following, achieving state-of-the-art performance among
  similarly sized models. Additionally, we prioritized safety and reliability
  by addressing both general and culture-specific considerations and
  incorporated mechanisms to reduce hallucinations. This work underscores the
  importance of inclusive AI, showing that advanced LLM capabilities can
  benefit underserved linguistic and cultural communities.</td>
  <td colspan=2 style='mso-ignore:colspan'>28 July, 2024</td>
 </tr>
 <tr height=408 style='height:306.0pt'>
  <td height=408 align=right style='height:306.0pt'>1724</td>
  <td align=right>2407.19679</td>
  <td class=xl65 width=649 style='width:487pt'>Harnessing Large Vision and
  Language Models in Agriculture: A Review</td>
  <td class=xl65 width=649 style='width:487pt'>Large models can play important
  roles in many domains. Agriculture is another key factor affecting the lives
  of people around the world. It provides food, fabric, and coal for humanity.
  However, facing many challenges such as pests and diseases, soil degradation,
  global warming, and food security, how to steadily increase the yield in the
  agricultural sector is a problem that humans still need to solve. Large
  models can help farmers improve production efficiency and harvest by
  detecting a series of agricultural production tasks such as pests and
  diseases, soil quality, and seed quality. It can also help farmers make wise
  decisions through a variety of information, such as images, text, etc.
  Herein, we delve into the potential applications of large models in
  agriculture, from large language model (LLM) and large vision model (LVM) to
  large vision-language models (LVLM). After gaining a deeper understanding of
  multimodal large language models (MLLM), it can be recognized that problems
  such as agricultural image processing, agricultural question answering
  systems, and agricultural machine automation can all be solved by large
  models. Large models have great potential in the field of agriculture. We
  outline the current applications of agricultural large models, and aims to
  emphasize the importance of large models in the domain of agriculture. In the
  end, we envisage a future in which famers use MLLM to accomplish many tasks
  in agriculture, which can greatly improve agricultural production efficiency
  and yield.</td>
  <td colspan=2 style='mso-ignore:colspan'>28 July, 2024</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>1722</td>
  <td align=right>2407.19835</td>
  <td class=xl65 width=649 style='width:487pt'>ATHAR: A High-Quality and
  Diverse Dataset for Classical Arabic to English Translation</td>
  <td class=xl65 width=649 style='width:487pt'>Classical Arabic represents a
  significant era, encompassing the golden age of Arab culture, philosophy, and
  scientific literature. With a broad consensus on the importance of
  translating these literatures to enrich knowledge dissemination across
  communities, the advent of large language models (LLMs) and translation
  systems offers promising tools to facilitate this goal. However, we have
  identified a scarcity of translation datasets in Classical Arabic, which are
  often limited in scope and topics, hindering the development of high-quality
  translation systems. In response, we present the ATHAR dataset, comprising
  66,000 high-quality Classical Arabic to English translation samples that
  cover a wide array of subjects including science, culture, and philosophy.
  Furthermore, we assess the performance of current state-of-the-art LLMs under
  various settings, concluding that there is a need for such datasets in
  current systems. Our findings highlight how models can benefit from
  fine-tuning or incorporating this dataset into their pretraining pipelines.
  The dataset is publicly available on the HuggingFace Data Hub at
  \url{https://huggingface.co/datasets/mohamed-khalil/ATHAR}.</td>
  <td colspan=2 style='mso-ignore:colspan'>29 July, 2024</td>
 </tr>
 <tr height=204 style='height:153.0pt'>
  <td height=204 align=right style='height:153.0pt'>1713</td>
  <td align=right>2407.20685</td>
  <td class=xl65 width=649 style='width:487pt'>CultureVo: The Serious Game of
  Utilizing Gen AI for Enhancing Cultural Intelligence</td>
  <td class=xl65 width=649 style='width:487pt'>CultureVo, Inc. has developed
  the Integrated Culture Learning Suite (ICLS) to deliver foundational
  knowledge of world cultures through a combination of interactive lessons and
  gamified experiences. This paper explores how Generative AI powered by open
  source Large Langauge Models are utilized within the ICLS to enhance cultural
  intelligence. The suite employs Generative AI techniques to automate the
  assessment of learner knowledge, analyze behavioral patterns, and manage
  interactions with non-player characters using real time learner assessment.
  Additionally, ICLS provides contextual hint and recommend course content by
  assessing learner proficiency, while Generative AI facilitates the automated
  creation and validation of educational content.</td>
  <td colspan=2 style='mso-ignore:colspan'>1 August, 2024</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>1668</td>
  <td align=right>2408.01752</td>
  <td class=xl65 width=649 style='width:487pt'>Advancing Green AI: Efficient
  and Accurate Lightweight CNNs for Rice Leaf Disease Identification</td>
  <td class=xl65 width=649 style='width:487pt'>Rice plays a vital role as a
  primary food source for over half of the world's population, and its
  production is critical for global food security. Nevertheless, rice
  cultivation is frequently affected by various diseases that can severely
  decrease yield and quality. Therefore, early and accurate detection of rice
  diseases is necessary to prevent their spread and minimize crop losses. In
  this research, we explore three mobile-compatible CNN architectures, namely
  ShuffleNet, MobileNetV2, and EfficientNet-B0, for rice leaf disease
  classification. These models are selected due to their compatibility with
  mobile devices, as they demand less computational power and memory compared
  to other CNN models. To enhance the performance of the three models, we added
  two fully connected layers separated by a dropout layer. We used early stop
  creation to prevent the model from being overfiting. The results of the study
  showed that the best performance was achieved by the EfficientNet-B0 model
  with an accuracy of 99.8%. Meanwhile, MobileNetV2 and ShuffleNet only
  achieved accuracies of 84.21% and 66.51%, respectively. This study shows that
  EfficientNet-B0 when combined with the proposed layer and early stop, can
  produce a high-accuracy model. Keywords: rice leaf detection; green AI; smart
  agriculture; EfficientNet;</td>
  <td colspan=2 style='mso-ignore:colspan'>3 August, 2024</td>
 </tr>
 <tr height=408 style='height:306.0pt'>
  <td height=408 align=right style='height:306.0pt'>1663</td>
  <td align=right>2408.02143</td>
  <td class=xl65 width=649 style='width:487pt'>Analyzing Cultural
  Representations of Emotions in LLMs through Mixed Emotion Survey</td>
  <td class=xl65 width=649 style='width:487pt'>Large Language Models (LLMs)
  have gained widespread global adoption, showcasing advanced linguistic
  capabilities across multiple of languages. There is a growing interest in
  academia to use these models to simulate and study human behaviors. However,
  it is crucial to acknowledge that an LLM's proficiency in a specific language
  might not fully encapsulate the norms and values associated with its culture.
  Concerns have emerged regarding potential biases towards Anglo-centric
  cultures and values due to the predominance of Western and US-based training
  data. This study focuses on analyzing the cultural representations of
  emotions in LLMs, in the specific case of mixed-emotion situations. Our
  methodology is based on the studies of Miyamoto et al. (2010), which
  identified distinctive emotional indicators in Japanese and American human
  responses. We first administer their mixed emotion survey to five different
  LLMs and analyze their outputs. Second, we experiment with contextual
  variables to explore variations in responses considering both language and
  speaker origin. Thirdly, we expand our investigation to encompass additional
  East Asian and Western European origin languages to gauge their alignment
  with their respective cultures, anticipating a closer fit. We find that (1)
  models have limited alignment with the evidence in the literature; (2)
  written language has greater effect on LLMs' response than information on
  participants origin; and (3) LLMs responses were found more similar for East
  Asian languages than Western European languages.</td>
  <td colspan=2 style='mso-ignore:colspan'>4 August, 2024</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>1620</td>
  <td align=right>2408.049</td>
  <td class=xl65 width=649 style='width:487pt'>Communicate to Play: Pragmatic
  Reasoning for Efficient Cross-Cultural Communication in Codenames</td>
  <td class=xl65 width=649 style='width:487pt'>Cultural differences in common
  ground may result in pragmatic failure and misunderstandings during
  communication. We develop our method Rational Speech Acts for Cross-Cultural
  Communication (RSA+C3) to resolve cross-cultural differences in common
  ground. To measure the success of our method, we study RSA+C3 in the
  collaborative referential game of Codenames Duet and show that our method
  successfully improves collaboration between simulated players of different
  cultures. Our contributions are threefold: (1) creating Codenames players
  using contrastive learning of an embedding space and LLM prompting that are
  aligned with human patterns of play, (2) studying culturally induced
  differences in common ground reflected in our trained models, and (3)
  demonstrating that our method RSA+C3 can ease cross-cultural communication in
  gameplay by inferring sociocultural context from interaction. Our code is
  publicly available at github.com/icwhite/codenames.</td>
  <td colspan=2 style='mso-ignore:colspan'>9 August, 2024</td>
 </tr>
 <tr height=385 style='height:289.0pt'>
  <td height=385 align=right style='height:289.0pt'>1614</td>
  <td align=right>2408.05102</td>
  <td class=xl65 width=649 style='width:487pt'>How Well Do LLMs Identify
  Cultural Unity in Diversity?</td>
  <td class=xl65 width=649 style='width:487pt'>Much work on the cultural
  awareness of large language models (LLMs) focuses on the models' sensitivity
  to geo-cultural diversity. However, in addition to cross-cultural
  differences, there also exists common ground across cultures. For instance, a
  bridal veil in the United States plays a similar cultural-relevant role as a
  honggaitou in China. In this study, we introduce a benchmark dataset CUNIT
  for evaluating decoder-only LLMs in understanding the cultural unity of
  concepts. Specifically, CUNIT consists of 1,425 evaluation examples building
  upon 285 traditional cultural-specific concepts across 10 countries. Based on
  a systematic manual annotation of cultural-relevant features per concept, we
  calculate the cultural association between any pair of cross-cultural
  concepts. Built upon this dataset, we design a contrastive matching task to
  evaluate the LLMs' capability to identify highly associated cross-cultural
  concept pairs. We evaluate 3 strong LLMs, using 3 popular prompting
  strategies, under the settings of either giving all extracted concept
  features or no features at all on CUNIT Interestingly, we find that cultural
  associations across countries regarding clothing concepts largely differ from
  food. Our analysis shows that LLMs are still limited to capturing
  cross-cultural associations between concepts compared to humans. Moreover,
  geo-cultural proximity shows a weak influence on model performance in
  capturing cross-cultural associations.</td>
  <td colspan=2 style='mso-ignore:colspan'>9 August, 2024</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>1582</td>
  <td align=right>2408.07779</td>
  <td class=xl65 width=649 style='width:487pt'>A New Framework for Error
  Analysis in Computational Paleographic Dating of Greek Papyri</td>
  <td class=xl65 width=649 style='width:487pt'>The study of Greek papyri from
  ancient Egypt is fundamental for understanding Graeco-Roman Antiquity,
  offering insights into various aspects of ancient culture and textual
  production. Palaeography, traditionally used for dating these manuscripts,
  relies on identifying chronologically relevant features in handwriting styles
  yet lacks a unified methodology, resulting in subjective interpretations and
  inconsistencies among experts. Recent advances in digital palaeography, which
  leverage artificial intelligence (AI) algorithms, have introduced new avenues
  for dating ancient documents. This paper presents a comparative analysis
  between an AI-based computational dating model and human expert
  palaeographers, using a novel dataset named Hell-Date comprising securely
  fine-grained dated Greek papyri from the Hellenistic period. The methodology
  involves training a convolutional neural network on visual inputs from
  Hell-Date to predict precise dates of papyri. In addition, experts provide
  palaeographic dating for comparison. To compare, we developed a new framework
  for error analysis that reflects the inherent imprecision of the
  palaeographic dating method. The results indicate that the computational
  model achieves performance comparable to that of human experts. These
  elements will help assess on a more solid basis future developments of
  computational algorithms to date Greek papyri.</td>
  <td colspan=2 style='mso-ignore:colspan'>14 August, 2024</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>1548</td>
  <td align=right>2408.09819</td>
  <td class=xl65 width=649 style='width:487pt'>CMoralEval: A Moral Evaluation
  Benchmark for Chinese Large Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>What a large language model
  (LLM) would respond in ethically relevant context? In this paper, we curate a
  large benchmark CMoralEval for morality evaluation of Chinese LLMs. The data
  sources of CMoralEval are two-fold: 1) a Chinese TV program discussing Chinese
  moral norms with stories from the society and 2) a collection of Chinese
  moral anomies from various newspapers and academic papers on morality. With
  these sources, we aim to create a moral evaluation dataset characterized by
  diversity and authenticity. We develop a morality taxonomy and a set of
  fundamental moral principles that are not only rooted in traditional Chinese
  culture but also consistent with contemporary societal norms. To facilitate
  efficient construction and annotation of instances in CMoralEval, we
  establish a platform with AI-assisted instance generation to streamline the
  annotation process. These help us curate CMoralEval that encompasses both
  explicit moral scenarios (14,964 instances) and moral dilemma scenarios
  (15,424 instances), each with instances from different data sources. We
  conduct extensive experiments with CMoralEval to examine a variety of Chinese
  LLMs. Experiment results demonstrate that CMoralEval is a challenging
  benchmark for Chinese LLMs. The dataset is publicly available at
  \url{https://github.com/tjunlp-lab/CMoralEval}.</td>
  <td colspan=2 style='mso-ignore:colspan'>19 August, 2024</td>
 </tr>
 <tr height=227 style='height:170.0pt'>
  <td height=227 align=right style='height:170.0pt'>1490</td>
  <td align=right>2408.13534</td>
  <td class=xl65 width=649 style='width:487pt'>Cultural Adaptation of Menus: A
  Fine-Grained Approach</td>
  <td class=xl65 width=649 style='width:487pt'>Machine Translation of
  Culture-Specific Items (CSIs) poses significant challenges. Recent work on
  CSI translation has shown some success using Large Language Models (LLMs) to
  adapt to different languages and cultures; however, a deeper analysis is
  needed to examine the benefits and pitfalls of each method. In this paper, we
  introduce the ChineseMenuCSI dataset, the largest for Chinese-English menu
  corpora, annotated with CSI vs Non-CSI labels and a fine-grained test set. We
  define three levels of CSI figurativeness for a more nuanced analysis and
  develop a novel methodology for automatic CSI identification, which
  outperforms GPT-based prompts in most categories. Importantly, we are the
  first to integrate human translation theories into LLM-driven translation
  processes, significantly improving translation accuracy, with COMET scores
  increasing by up to 7 points.</td>
  <td colspan=2 style='mso-ignore:colspan'>24 August, 2024</td>
 </tr>
 <tr height=227 style='height:170.0pt'>
  <td height=227 align=right style='height:170.0pt'>1482</td>
  <td align=right>2408.14772</td>
  <td class=xl65 width=649 style='width:487pt'>A global AI community requires
  language-diverse publishing</td>
  <td class=xl65 width=649 style='width:487pt'>In this provocation, we discuss
  the English dominance of the AI research community, arguing that the
  requirement for English language publishing upholds and reinforces broader
  regimes of extraction in AI. While large language models and machine
  translation have been celebrated as a way to break down barriers, we regard
  their use as a symptom of linguistic exclusion of scientists and potential
  readers. We propose alternative futures for a healthier publishing culture,
  organized around three themes: administering conferences in the languages of
  the country in which they are held, instructing peer reviewers not to
  adjudicate the language appropriateness of papers, and offering opportunities
  to publish and present in multiple languages. We welcome new translations of
  this piece. Please contact the authors if you would like to contribute one.</td>
  <td colspan=2 style='mso-ignore:colspan'>29 August, 2024</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>1477</td>
  <td align=right>2408.15261</td>
  <td class=xl65 width=649 style='width:487pt'>Civiverse: A Dataset for
  Analyzing User Engagement with Open-Source Text-to-Image Models</td>
  <td class=xl65 width=649 style='width:487pt'>Text-to-image (TTI) systems,
  particularly those utilizing open-source frameworks, have become increasingly
  prevalent in the production of Artificial Intelligence (AI)-generated
  visuals. While existing literature has explored various problematic aspects of
  TTI technologies, such as bias in generated content, intellectual property
  concerns, and the reinforcement of harmful stereotypes, open-source TTI
  frameworks have not yet been systematically examined from a cultural
  perspective. This study addresses this gap by analyzing the CivitAI platform,
  a leading open-source platform dedicated to TTI AI. We introduce the
  Civiverse prompt dataset, encompassing millions of images and related
  metadata. We focus on prompt analysis, specifically examining the semantic
  characteristics of text prompts, as it is crucial for addressing societal
  issues related to generative technologies. This analysis provides insights
  into user intentions, preferences, and behaviors, which in turn shape the
  outputs of these models. Our findings reveal a predominant preference for
  generating explicit content, along with a focus on homogenization of semantic
  content. These insights underscore the need for further research into the
  perpetuation of misogyny, harmful stereotypes, and the uniformity of visual
  culture within these models.</td>
  <td colspan=2 style='mso-ignore:colspan'>10 August, 2024</td>
 </tr>
 <tr height=431 style='height:323.0pt'>
  <td height=431 align=right style='height:323.0pt'>1472</td>
  <td align=right>2408.15665</td>
  <td class=xl65 width=649 style='width:487pt'>Agricultural On-Demand Networks
  for 6G enabled by THz Communication</td>
  <td class=xl65 width=649 style='width:487pt'>The transforming process in the
  scope of agriculture towards Smart Agriculture is an essential step to
  fulfill growing demands in respect to nourishment. Crucial challenges include
  establishing robust wireless communication in rural areas, enabling collaboration
  among agricultural machines, and integrating artificial intelligence into
  farming practices. Addressing these challenges necessitates a consistent
  communication system, with wireless communication emerging as a key enabler.
  Cellular technologies, as 5G and its successor 6G, can offer a comprehensive
  solution here. Leveraging technologies following the ITU-R M. 2160
  recommendation like THz communication, low-latency wireless AI, and embedded
  sensing, can provide a flexible and energy-efficient infrastructure. This
  paper introduces on-demand networks based on the OpenRAN approach and a 7.2
  functional split. By implementing THz front-hauling between components, a
  flexible application of 5G or future 6G networks can be realized. Experiments
  demonstrate that THz communication is suitable for data transmission over the
  eCPRI interface, particularly in terms of data rate, thereby reducing the
  need for wired alternatives such as fiber optic cables. Furthermore,
  limitations such as limited range are discussed, and possible initial
  solutions are presented. The integration of the OpenRAN standard further
  enhances flexibility, which is crucial in dynamic agricultural environments.
  This research contributes to the ongoing discourse on the transformative potential
  of 6G-enabled wireless communication in shaping the future of smart
  agriculture.</td>
  <td colspan=2 style='mso-ignore:colspan'>28 August, 2024</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>1464</td>
  <td align=right>2408.16471</td>
  <td class=xl65 width=649 style='width:487pt'>Improving 3D deep learning
  segmentation with biophysically motivated cell synthesis</td>
  <td class=xl65 width=649 style='width:487pt'>Biomedical research increasingly
  relies on 3D cell culture models and AI-based analysis can potentially
  facilitate a detailed and accurate feature extraction on a single-cell level.
  However, this requires for a precise segmentation of 3D cell datasets, which
  in turn demands high-quality ground truth for training. Manual annotation,
  the gold standard for ground truth data, is too time-consuming and thus not
  feasible for the generation of large 3D training datasets. To address this,
  we present a novel framework for generating 3D training data, which
  integrates biophysical modeling for realistic cell shape and alignment. Our
  approach allows the in silico generation of coherent membrane and nuclei
  signals, that enable the training of segmentation models utilizing both
  channels for improved performance. Furthermore, we present a new GAN training
  scheme that generates not only image data but also matching labels.
  Quantitative evaluation shows superior performance of biophysical motivated
  synthetic training data, even outperforming manual annotation and pretrained
  models. This underscores the potential of incorporating biophysical modeling
  for enhancing synthetic training data quality.</td>
  <td colspan=2 style='mso-ignore:colspan'>29 August, 2024</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>1461</td>
  <td align=right>2408.1674</td>
  <td class=xl65 width=649 style='width:487pt'>Theoretical and Methodological
  Framework for Studying Texts Produced by Large Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>This paper addresses the
  conceptual, methodological and technical challenges in studying large
  language models (LLMs) and the texts they produce from a quantitative
  linguistics perspective. It builds on a theoretical framework that
  distinguishes between the LLM as a substrate and the entities the model
  simulates. The paper advocates for a strictly non-anthropomorphic approach to
  models while cautiously applying methodologies used in studying human
  linguistic behavior to the simulated entities. While natural language
  processing researchers focus on the models themselves, their architecture,
  evaluation, and methods for improving performance, we as quantitative
  linguists should strive to build a robust theory concerning the
  characteristics of texts produced by LLMs, how they differ from
  human-produced texts, and the properties of simulated entities. Additionally,
  we should explore the potential of LLMs as an instrument for studying human
  culture, of which language is an integral part.</td>
  <td colspan=2 style='mso-ignore:colspan'>29 August, 2024</td>
 </tr>
 <tr height=408 style='height:306.0pt'>
  <td height=408 align=right style='height:306.0pt'>1434</td>
  <td align=right>2409.01556</td>
  <td class=xl65 width=649 style='width:487pt'>Benchmarking Cognitive Domains
  for LLMs: Insights from Taiwanese Hakka Culture</td>
  <td class=xl65 width=649 style='width:487pt'>This study introduces a
  comprehensive benchmark designed to evaluate the performance of large
  language models (LLMs) in understanding and processing cultural knowledge,
  with a specific focus on Hakka culture as a case study. Leveraging Bloom's
  Taxonomy, the study develops a multi-dimensional framework that
  systematically assesses LLMs across six cognitive domains: Remembering,
  Understanding, Applying, Analyzing, Evaluating, and Creating. This benchmark
  extends beyond traditional single-dimensional evaluations by providing a
  deeper analysis of LLMs' abilities to handle culturally specific content,
  ranging from basic recall of facts to higher-order cognitive tasks such as
  creative synthesis. Additionally, the study integrates Retrieval-Augmented
  Generation (RAG) technology to address the challenges of minority cultural
  knowledge representation in LLMs, demonstrating how RAG enhances the models'
  performance by dynamically incorporating relevant external information. The
  results highlight the effectiveness of RAG in improving accuracy across all
  cognitive domains, particularly in tasks requiring precise retrieval and
  application of cultural knowledge. However, the findings also reveal the
  limitations of RAG in creative tasks, underscoring the need for further
  optimization. This benchmark provides a robust tool for evaluating and
  comparing LLMs in culturally diverse contexts, offering valuable insights for
  future research and development in AI-driven cultural knowledge preservation
  and dissemination.</td>
  <td colspan=2 style='mso-ignore:colspan'>24 September, 2024</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>1430</td>
  <td align=right>2409.01754</td>
  <td class=xl65 width=649 style='width:487pt'>Empirical evidence of Large
  Language Model's influence on human spoken communication</td>
  <td class=xl65 width=649 style='width:487pt'>Artificial Intelligence (AI)
  agents now interact with billions of humans in natural language, thanks to
  advances in Large Language Models (LLMs) like ChatGPT. This raises the
  question of whether AI has the potential to shape a fundamental aspect of
  human culture: the way we speak. Recent analyses revealed that scientific
  publications already exhibit evidence of AI-specific language. But this
  evidence is inconclusive, since scientists may simply be using AI to
  copy-edit their writing. To explore whether AI has influenced human spoken
  communication, we transcribed and analyzed about 280,000 English-language
  videos of presentations, talks, and speeches from more than 20,000 YouTube
  channels of academic institutions. We find a significant shift in the trend
  of word usage specific to words distinctively associated with ChatGPT
  following its release. These findings provide the first empirical evidence
  that humans increasingly imitate LLMs in their spoken language. Our results
  raise societal and policy-relevant concerns about the potential of AI to
  unintentionally reduce linguistic diversity, or to be deliberately misused
  for mass manipulation. They also highlight the need for further investigation
  into the feedback loops between machine behavior and human culture.</td>
  <td colspan=2 style='mso-ignore:colspan'>3 September, 2024</td>
 </tr>
 <tr height=385 style='height:289.0pt'>
  <td height=385 align=right style='height:289.0pt'>1392</td>
  <td align=right>2409.06029</td>
  <td class=xl65 width=649 style='width:487pt'>SongCreator: Lyrics-based
  Universal Song Generation</td>
  <td class=xl65 width=649 style='width:487pt'>Music is an integral part of
  human culture, embodying human intelligence and creativity, of which songs
  compose an essential part. While various aspects of song generation have been
  explored by previous works, such as singing voice, vocal composition and
  instrumental arrangement, etc., generating songs with both vocals and
  accompaniment given lyrics remains a significant challenge, hindering the
  application of music generation models in the real world. In this light, we
  propose SongCreator, a song-generation system designed to tackle this
  challenge. The model features two novel designs: a meticulously designed
  dual-sequence language model (DSLM) to capture the information of vocals and
  accompaniment for song generation, and a series of attention mask strategies
  for DSLM, which allows our model to understand, generate and edit songs,
  making it suitable for various songrelated generation tasks by utilizing
  specific attention masks. Extensive experiments demonstrate the effectiveness
  of SongCreator by achieving state-of-the-art or competitive performances on
  all eight tasks. Notably, it surpasses previous works by a large margin in
  lyrics-to-song and lyrics-to-vocals. Additionally, it is able to
  independently control the acoustic conditions of the vocals and accompaniment
  in the generated song through different audio prompts, exhibiting its
  potential applicability. Our samples are available at
  https://thuhcsi.github.io/SongCreator/.</td>
  <td colspan=2 style='mso-ignore:colspan'>30 October, 2024</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>1293</td>
  <td align=right>2409.12623</td>
  <td class=xl65 width=649 style='width:487pt'>CamelEval: Advancing Culturally
  Aligned Arabic Language Models and Benchmarks</td>
  <td class=xl65 width=649 style='width:487pt'>Large Language Models (LLMs) are
  the cornerstones of modern artificial intelligence systems. This paper
  introduces Juhaina, a Arabic-English bilingual LLM specifically designed to
  align with the values and preferences of Arabic speakers. Juhaina inherently
  supports advanced functionalities such as instruction following, open-ended
  question answering, information provisioning, and text processing. Our model
  contains 9.24 billion parameters and is trained on a context window of up to
  8,192 tokens. This paper details the creation process of Juhaina and provides
  an extensive empirical evaluation. Furthermore, we identify the limitations
  of widely-adopted Open Arabic LLM Leaderboard (OALL) and propose a new
  evaluation benchmark, CamelEval. Our findings demonstrate that Juhaina
  surpasses existing LLMs of comparable sizes, such as the Llama and Gemma
  families, in generating helpful responses in Arabic, providing factually
  accurate information about the region, and understanding nuanced cultural
  aspects. We aspire for Juhaina to democratize cutting-edge AI technologies,
  serving over 400 million Arabic speakers by offering LLMs that not only
  communicate in their language but also comprehend their culture. We publicly
  release all models on Huggingface \url{https://huggingface.co/elmrc}.</td>
  <td colspan=2 style='mso-ignore:colspan'>24 September, 2024</td>
 </tr>
 <tr height=408 style='height:306.0pt'>
  <td height=408 align=right style='height:306.0pt'>1289</td>
  <td align=right>2409.12739</td>
  <td class=xl65 width=649 style='width:487pt'>Edu-Values: Towards Evaluating
  the Chinese Education Values of Large Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>With the recent evolution of
  large language models (LLMs), concerns about aligning such models with human
  values have grown. Previous research has primarily focused on assessing LLMs'
  performance in terms of the Helpful, Honest, Harmless (3H) basic principles,
  while often overlooking their alignment with educational values in the
  Chinese context. To fill this gap, we present Edu-Values, the first Chinese
  education values evaluation benchmark designed to measure LLMs' alignment
  ability across seven dimensions: professional ideology, cultural literacy,
  educational knowledge and skills, education laws and regulations, teachers'
  professional ethics, basic competencies, and subject knowledge. We
  meticulously design and compile 1,418 questions, including multiple-choice,
  multi-modal question answering, subjective analysis, adversarial prompts, and
  questions on traditional Chinese culture. We conduct both human evaluation
  and automatic evaluation over 11 state-of-the-art (SoTA) LLMs, and highlight
  three main findings: (1) due to differences in educational culture, Chinese
  LLMs significantly outperform English LLMs, with Qwen 2 ranking the first
  with a score of 81.37; (2) LLMs perform well in subject knowledge and
  teaching skills but struggle with teachers' professional ethics and basic
  competencies; (3) LLMs excel at multiple-choice questions but perform poorly
  on subjective analysis and multi-modal tasks. This demonstrates the
  effectiveness and potential of the proposed benchmark. Our dataset is
  available at https://github.com/zhangpeii/Edu-Values.git.</td>
  <td colspan=2 style='mso-ignore:colspan'>10 October, 2024</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>1275</td>
  <td align=right>2409.13521</td>
  <td class=xl65 width=649 style='width:487pt'>A Survey on Moral Foundation
  Theory and Pre-Trained Language Models: Current Advances and Challenges</td>
  <td class=xl65 width=649 style='width:487pt'>Moral values have deep roots in
  early civilizations, codified within norms and laws that regulated societal
  order and the common good. They play a crucial role in understanding the
  psychological basis of human behavior and cultural orientation. The Moral
  Foundation Theory (MFT) is a well-established framework that identifies the
  core moral foundations underlying the manner in which different cultures
  shape individual and social lives. Recent advancements in natural language
  processing, particularly Pre-trained Language Models (PLMs), have enabled the
  extraction and analysis of moral dimensions from textual data. This survey
  presents a comprehensive review of MFT-informed PLMs, providing an analysis
  of moral tendencies in PLMs and their application in the context of the MFT.
  We also review relevant datasets and lexicons and discuss trends,
  limitations, and future directions. By providing a structured overview of the
  intersection between PLMs and MFT, this work bridges moral psychology
  insights within the realm of PLMs, paving the way for further research and
  development in creating morally aware AI systems.</td>
  <td colspan=2 style='mso-ignore:colspan'>20 September, 2024</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>1208</td>
  <td align=right>2409.18459</td>
  <td class=xl65 width=649 style='width:487pt'>FoodMLLM-JP: Leveraging
  Multimodal Large Language Models for Japanese Recipe Generation</td>
  <td class=xl65 width=649 style='width:487pt'>Research on food image
  understanding using recipe data has been a long-standing focus due to the
  diversity and complexity of the data. Moreover, food is inextricably linked
  to people's lives, making it a vital research area for practical applications
  such as dietary management. Recent advancements in Multimodal Large Language
  Models (MLLMs) have demonstrated remarkable capabilities, not only in their
  vast knowledge but also in their ability to handle languages naturally. While
  English is predominantly used, they can also support multiple languages
  including Japanese. This suggests that MLLMs are expected to significantly
  improve performance in food image understanding tasks. We fine-tuned open
  MLLMs LLaVA-1.5 and Phi-3 Vision on a Japanese recipe dataset and benchmarked
  their performance against the closed model GPT-4o. We then evaluated the
  content of generated recipes, including ingredients and cooking procedures,
  using 5,000 evaluation samples that comprehensively cover Japanese food
  culture. Our evaluation demonstrates that the open models trained on recipe
  data outperform GPT-4o, the current state-of-the-art model, in ingredient
  generation. Our model achieved F1 score of 0.531, surpassing GPT-4o's F1
  score of 0.481, indicating a higher level of accuracy. Furthermore, our model
  exhibited comparable performance to GPT-4o in generating cooking procedure
  text.</td>
  <td colspan=2 style='mso-ignore:colspan'>27 September, 2024</td>
 </tr>
 <tr height=431 style='height:323.0pt'>
  <td height=431 align=right style='height:323.0pt'>1200</td>
  <td align=right>2409.19148</td>
  <td class=xl65 width=649 style='width:487pt'>Uncovering Differences in
  Persuasive Language in Russian versus English Wikipedia</td>
  <td class=xl65 width=649 style='width:487pt'>We study how differences in
  persuasive language across Wikipedia articles, written in either English and
  Russian, can uncover each culture's distinct perspective on different
  subjects. We develop a large language model (LLM) powered system to identify
  instances of persuasive language in multilingual texts. Instead of directly
  prompting LLMs to detect persuasion, which is subjective and difficult, we
  propose to reframe the task to instead ask high-level questions (HLQs) which
  capture different persuasive aspects. Importantly, these HLQs are authored by
  LLMs themselves. LLMs over-generate a large set of HLQs, which are
  subsequently filtered to a small set aligned with human labels for the
  original task. We then apply our approach to a large-scale, bilingual dataset
  of Wikipedia articles (88K total), using a two-stage identify-then-extract
  prompting strategy to find instances of persuasion. We quantify the amount of
  persuasion per article, and explore the differences in persuasion through
  several experiments on the paired articles. Notably, we generate rankings of
  articles by persuasion in both languages. These rankings match our intuitions
  on the culturally-salient subjects; Russian Wikipedia highlights subjects on
  Ukraine, while English Wikipedia highlights the Middle East. Grouping
  subjects into larger topics, we find politically-related events contain more
  persuasion than others. We further demonstrate that HLQs obtain similar
  performance when posed in either English or Russian. Our methodology enables
  cross-lingual, cross-cultural understanding at scale, and we release our
  code, prompts, and data.</td>
  <td colspan=2 style='mso-ignore:colspan'>27 September, 2024</td>
 </tr>
 <tr height=249 style='height:187.0pt'>
  <td height=249 align=right style='height:187.0pt'>1198</td>
  <td align=right>2409.19459</td>
  <td class=xl65 width=649 style='width:487pt'>Language-guided Robust
  Navigation for Mobile Robots in Dynamically-changing Environments</td>
  <td class=xl65 width=649 style='width:487pt'>In this paper, we develop an
  embodied AI system for human-in-the-loop navigation with a wheeled mobile
  robot. We propose a direct yet effective method of monitoring the robot's
  current plan to detect changes in the environment that impact the intended trajectory
  of the robot significantly and then query a human for feedback. We also
  develop a means to parse human feedback expressed in natural language into
  local navigation waypoints and integrate it into a global planning system, by
  leveraging a map of semantic features and an aligned obstacle map. Extensive
  testing in simulation and physical hardware experiments with a
  resource-constrained wheeled robot tasked to navigate in a real-world
  environment validate the efficacy and robustness of our method. This work can
  support applications like precision agriculture and construction, where
  persistent monitoring of the environment provides a human with information
  about the environment state.</td>
  <td colspan=2 style='mso-ignore:colspan'>28 September, 2024</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>1185</td>
  <td align=right>2410.00194</td>
  <td class=xl65 width=649 style='width:487pt'>&quot;Real Learner Data
  Matters&quot; Exploring the Design of LLM-Powered Question Generation for
  Deaf and Hard of Hearing Learners</td>
  <td class=xl65 width=649 style='width:487pt'>Deaf and Hard of Hearing (DHH)
  learners face unique challenges in learning environments, often due to a lack
  of tailored educational materials that address their specific needs. This
  study explores the potential of Large Language Models (LLMs) to generate
  personalized quiz questions to enhance DHH students' video-based learning
  experiences. We developed a prototype leveraging LLMs to generate questions
  with emphasis on two unique strategies: Visual Questions, which identify
  video segments where visual information might be misrepresented, and Emotion
  Questions, which highlight moments where previous DHH learners experienced
  learning difficulty manifested in emotional responses. Through user studies
  with DHH undergraduates, we evaluated the effectiveness of these
  LLM-generated questions in supporting the learning experience. Our findings
  indicate that while LLMs offer significant potential for personalized
  learning, challenges remain in the interaction accessibility for the diverse
  DHH community. The study highlights the importance of considering language
  diversity and culture in LLM-based educational technology design.</td>
  <td colspan=2 style='mso-ignore:colspan'>30 September, 2024</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>1157</td>
  <td align=right>2410.01809</td>
  <td class=xl65 width=649 style='width:487pt'>Enhancing transparency in
  AI-powered customer engagement</td>
  <td class=xl65 width=649 style='width:487pt'>This paper addresses the
  critical challenge of building consumer trust in AI-powered customer
  engagement by emphasising the necessity for transparency and accountability.
  Despite the potential of AI to revolutionise business operations and enhance
  customer experiences, widespread concerns about misinformation and the
  opacity of AI decision-making processes hinder trust. Surveys highlight a
  significant lack of awareness among consumers regarding their interactions
  with AI, alongside apprehensions about bias and fairness in AI algorithms.
  The paper advocates for the development of explainable AI models that are
  transparent and understandable to both consumers and organisational leaders,
  thereby mitigating potential biases and ensuring ethical use. It underscores
  the importance of organisational commitment to transparency practices beyond
  mere regulatory compliance, including fostering a culture of accountability,
  prioritising clear data policies and maintaining active engagement with
  stakeholders. By adopting a holistic approach to transparency and
  explainability, businesses can cultivate trust in AI technologies, bridging
  the gap between technological innovation and consumer acceptance, and paving
  the way for more ethical and effective AI-powered customer engagements.
  KEYWORDS: artificial intelligence (AI), transparency</td>
  <td colspan=2 style='mso-ignore:colspan'>13 September, 2024</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>1155</td>
  <td align=right>2410.01811</td>
  <td class=xl65 width=649 style='width:487pt'>Evaluating Cultural Awareness of
  LLMs for Yoruba, Malayalam, and English</td>
  <td class=xl65 width=649 style='width:487pt'>Although LLMs have been
  extremely effective in a large number of complex tasks, their understanding
  and functionality for regional languages and cultures are not well studied.
  In this paper, we explore the ability of various LLMs to comprehend the
  cultural aspects of two regional languages: Malayalam (state of Kerala,
  India) and Yoruba (West Africa). Using Hofstede's six cultural dimensions:
  Power Distance (PDI), Individualism (IDV), Motivation towards Achievement and
  Success (MAS), Uncertainty Avoidance (UAV), Long Term Orientation (LTO), and
  Indulgence (IVR), we quantify the cultural awareness of LLM-based responses.
  We demonstrate that although LLMs show a high cultural similarity for
  English, they fail to capture the cultural nuances across these 6 metrics for
  Malayalam and Yoruba. We also highlight the need for large-scale regional
  language LLM training with culturally enriched datasets. This will have huge
  implications for enhancing the user experience of chat-based LLMs and also
  improving the validity of large-scale LLM agent-based market research.</td>
  <td colspan=2 style='mso-ignore:colspan'>13 September, 2024</td>
 </tr>
 <tr height=204 style='height:153.0pt'>
  <td height=204 align=right style='height:153.0pt'>1151</td>
  <td align=right>2410.02027</td>
  <td class=xl65 width=649 style='width:487pt'>Quantifying the Gaps Between
  Translation and Native Perception in Training for Multimodal, Multilingual
  Retrieval</td>
  <td class=xl65 width=649 style='width:487pt'>There is a scarcity of
  multilingual vision-language models that properly account for the perceptual
  differences that are reflected in image captions across languages and
  cultures. In this work, through a multimodal, multilingual retrieval case
  study, we quantify the existing lack of model flexibility. We empirically
  show performance gaps between training on captions that come from native
  German perception and captions that have been either machine-translated or
  human-translated from English into German. To address these gaps, we further
  propose and evaluate caption augmentation strategies. While we achieve mean
  recall improvements (+1.3), gaps still remain, indicating an open area of
  future work for the community.</td>
  <td colspan=2 style='mso-ignore:colspan'>8 October, 2024</td>
 </tr>
 <tr height=408 style='height:306.0pt'>
  <td height=408 align=right style='height:306.0pt'>1138</td>
  <td align=right>2410.02677</td>
  <td class=xl65 width=649 style='width:487pt'>CulturalBench: a Robust, Diverse
  and Challenging Benchmark on Measuring the (Lack of) Cultural Knowledge of
  LLMs</td>
  <td class=xl65 width=649 style='width:487pt'>To make large language models
  (LLMs) more helpful across diverse cultures, it is essential to have
  effective cultural knowledge benchmarks to measure and track our progress.
  Effective benchmarks need to be robust, diverse, and challenging. We
  introduce CulturalBench: a set of 1,227 human-written and human-verified
  questions for effectively assessing LLMs' cultural knowledge, covering 45
  global regions including the underrepresented ones like Bangladesh, Zimbabwe,
  and Peru. Questions - each verified by five independent annotators - span 17
  diverse topics ranging from food preferences to greeting etiquettes. We
  evaluate models on two setups: CulturalBench-Easy and CulturalBench-Hard
  which share the same questions but asked differently. We find that LLMs are
  sensitive to such difference in setups (e.g., GPT-4o with 27.3% difference).
  Compared to human performance (92.6% accuracy), CulturalBench-Hard is more
  challenging for frontier LLMs with the best performing model (GPT-4o) at only
  61.5% and the worst (Llama3-8b) at 21.4%. Moreover, we find that LLMs often
  struggle with tricky questions that have multiple correct answers (e.g., What
  utensils do the Chinese usually use?), revealing a tendency to converge to a
  single answer. Our results also indicate that OpenAI GPT-4o substantially
  outperform other proprietary and open source models in questions related to
  all but one region (Oceania). Nonetheless, all models consistently
  underperform on questions related to South America and the Middle East.</td>
  <td colspan=2 style='mso-ignore:colspan'>3 October, 2024</td>
 </tr>
 <tr height=496 style='height:372.0pt'>
  <td height=496 align=right style='height:372.0pt'>1113</td>
  <td align=right>2410.03721</td>
  <td class=xl65 width=649 style='width:487pt'>Thematic Analysis with
  Open-Source Generative AI and Machine Learning: A New Method for Inductive
  Qualitative Codebook Development</td>
  <td class=xl65 width=649 style='width:487pt'>This paper aims to answer one
  central question: to what extent can open-source generative text models be
  used in a workflow to approximate thematic analysis in social science
  research? To answer this question, we present the Generative AI-enabled Theme
  Organization and Structuring (GATOS) workflow, which uses open-source machine
  learning techniques, natural language processing tools, and generative text
  models to facilitate thematic analysis. To establish validity of the method,
  we present three case studies applying the GATOS workflow, leveraging these
  models and techniques to inductively create codebooks similar to traditional
  procedures using thematic analysis. Specifically, we investigate the extent
  to which a workflow comprising open-source models and tools can inductively
  produce codebooks that approach the known space of themes and sub-themes. To
  address the challenge of gleaning insights from these texts, we combine
  open-source generative text models, retrieval-augmented generation, and
  prompt engineering to identify codes and themes in large volumes of text,
  i.e., generate a qualitative codebook. The process mimics an inductive coding
  process that researchers might use in traditional thematic analysis by
  reading text one unit of analysis at a time, considering existing codes
  already in the codebook, and then deciding whether or not to generate a new
  code based on whether the extant codebook provides adequate thematic
  coverage. We demonstrate this workflow using three synthetic datasets from
  hypothetical organizational research settings: a study of teammate feedback
  in teamwork settings, a study of organizational cultures of ethical behavior,
  and a study of employee perspectives about returning to their offices after
  the pandemic. We show that the GATOS workflow is able to identify themes in
  the text that were used to generate the original synthetic datasets.</td>
  <td colspan=2 style='mso-ignore:colspan'>28 September, 2024</td>
 </tr>
 <tr height=408 style='height:306.0pt'>
  <td height=408 align=right style='height:306.0pt'>1018</td>
  <td align=right>2410.08968</td>
  <td class=xl65 width=649 style='width:487pt'>Controllable Safety Alignment:
  Inference-Time Adaptation to Diverse Safety Requirements</td>
  <td class=xl65 width=649 style='width:487pt'>The current paradigm for safety
  alignment of large language models (LLMs) follows a one-size-fits-all
  approach: the model refuses to interact with any content deemed unsafe by the
  model provider. This approach lacks flexibility in the face of varying social
  norms across cultures and regions. In addition, users may have diverse safety
  needs, making a model with static safety standards too restrictive to be
  useful, as well as too costly to be re-aligned. We propose Controllable
  Safety Alignment (CoSA), a framework designed to adapt models to diverse
  safety requirements without re-training. Instead of aligning a fixed model,
  we align models to follow safety configs -- free-form natural language
  descriptions of the desired safety behaviors -- that are provided as part of
  the system prompt. To adjust model safety behavior, authorized users only
  need to modify such safety configs at inference time. To enable that, we
  propose CoSAlign, a data-centric method for aligning LLMs to easily adapt to
  diverse safety configs. Furthermore, we devise a novel controllability
  evaluation protocol that considers both helpfulness and configured safety,
  summarizing them into CoSA-Score, and construct CoSApien, a human-authored
  benchmark that consists of real-world LLM use cases with diverse safety
  requirements and corresponding evaluation prompts. We show that CoSAlign
  leads to substantial gains of controllability over strong baselines including
  in-context alignment. Our framework encourages better representation and
  adaptation to pluralistic human values in LLMs, and thereby increasing their
  practicality.</td>
  <td colspan=2 style='mso-ignore:colspan'>11 October, 2024</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>1013</td>
  <td align=right>2410.09094</td>
  <td class=xl65 width=649 style='width:487pt'>Reflections on Disentanglement
  and the Latent Space</td>
  <td class=xl65 width=649 style='width:487pt'>The latent space of image
  generative models is a multi-dimensional space of compressed hidden visual
  knowledge. Its entity captivates computer scientists, digital artists, and
  media scholars alike. Latent space has become an aesthetic category in AI
  art, inspiring artistic techniques such as the latent space walk, exemplified
  by the works of Mario Klingemann and others. It is also viewed as cultural
  snapshots, encoding rich representations of our visual world. This paper
  proposes a double view of the latent space, as a multi-dimensional archive of
  culture and as a multi-dimensional space of potentiality. The paper discusses
  disentanglement as a method to elucidate the double nature of the space and
  as an interpretative direction to exploit its organization in human terms.
  The paper compares the role of disentanglement as potentiality to that of
  conditioning, as imagination, and confronts this interpretation with the
  philosophy of Deleuzian potentiality and Hume's imagination. Lastly, this
  paper notes the difference between traditional generative models and recent
  architectures.</td>
  <td colspan=2 style='mso-ignore:colspan'>20 October, 2024</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>1005</td>
  <td align=right>2410.0951</td>
  <td class=xl65 width=649 style='width:487pt'>Scito2M: A 2 Million, 30-Year
  Cross-disciplinary Dataset for Temporal Scientometric Analysis</td>
  <td class=xl65 width=649 style='width:487pt'>Understanding the creation,
  evolution, and dissemination of scientific knowledge is crucial for bridging
  diverse subject areas and addressing complex global challenges such as
  pandemics, climate change, and ethical AI. Scientometrics, the quantitative
  and qualitative study of scientific literature, provides valuable insights
  into these processes. We introduce Scito2M, a longitudinal scientometric
  dataset with over two million academic publications, providing comprehensive
  contents information and citation graphs to support cross-disciplinary
  analyses. Using Scito2M, we conduct a temporal study spanning over 30 years
  to explore key questions in scientometrics: the evolution of academic
  terminology, citation patterns, and interdisciplinary knowledge exchange. Our
  findings reveal critical insights, such as disparities in epistemic cultures,
  knowledge production modes, and citation practices. For example, rapidly
  developing, application-driven fields like LLMs exhibit significantly shorter
  citation age (2.48 years) compared to traditional theoretical disciplines
  like oral history (9.71 years).</td>
  <td colspan=2 style='mso-ignore:colspan'>12 October, 2024</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>1004</td>
  <td align=right>2410.09564</td>
  <td class=xl65 width=649 style='width:487pt'>Extended Japanese Commonsense
  Morality Dataset with Masked Token and Label Enhancement</td>
  <td class=xl65 width=649 style='width:487pt'>Rapid advancements in artificial
  intelligence (AI) have made it crucial to integrate moral reasoning into AI
  systems. However, existing models and datasets often overlook regional and
  cultural differences. To address this shortcoming, we have expanded the
  JCommonsenseMorality (JCM) dataset, the only publicly available dataset
  focused on Japanese morality. The Extended JCM (eJCM) has grown from the
  original 13,975 sentences to 31,184 sentences using our proposed sentence
  expansion method called Masked Token and Label Enhancement (MTLE). MTLE
  selectively masks important parts of sentences related to moral judgment and
  replaces them with alternative expressions generated by a large language
  model (LLM), while re-assigning appropriate labels. The model trained using
  our eJCM achieved an F1 score of 0.857, higher than the scores for the
  original JCM (0.837), ChatGPT one-shot classification (0.841), and data
  augmented using AugGPT, a state-of-the-art augmentation method (0.850).
  Specifically, in complex moral reasoning tasks unique to Japanese culture,
  the model trained with eJCM showed a significant improvement in performance
  (increasing from 0.681 to 0.756) and achieved a performance close to that of
  GPT-4 Turbo (0.787). These results demonstrate the validity of the eJCM
  dataset and the importance of developing models and datasets that consider
  the cultural context.</td>
  <td colspan=2 style='mso-ignore:colspan'>12 October, 2024</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>991</td>
  <td align=right>2410.10489</td>
  <td class=xl65 width=649 style='width:487pt'>Cultural Fidelity in
  Large-Language Models: An Evaluation of Online Language Resources as a Driver
  of Model Performance in Value Representation</td>
  <td class=xl65 width=649 style='width:487pt'>The training data for LLMs
  embeds societal values, increasing their familiarity with the language's
  culture. Our analysis found that 44% of the variance in the ability of GPT-4o
  to reflect the societal values of a country, as measured by the World Values
  Survey, correlates with the availability of digital resources in that
  language. Notably, the error rate was more than five times higher for the
  languages of the lowest resource compared to the languages of the highest
  resource. For GPT-4-turbo, this correlation rose to 72%, suggesting efforts
  to improve the familiarity with the non-English language beyond the
  web-scraped data. Our study developed one of the largest and most robust
  datasets in this topic area with 21 country-language pairs, each of which
  contain 94 survey questions verified by native speakers. Our results
  highlight the link between LLM performance and digital data availability in
  target languages. Weaker performance in low-resource languages, especially
  prominent in the Global South, may worsen digital divides. We discuss
  strategies proposed to address this, including developing multilingual LLMs
  from the ground up and enhancing fine-tuning on diverse linguistic datasets,
  as seen in African language initiatives.</td>
  <td colspan=2 style='mso-ignore:colspan'>14 October, 2024</td>
 </tr>
 <tr height=385 style='height:289.0pt'>
  <td height=385 align=right style='height:289.0pt'>971</td>
  <td align=right>2410.1159</td>
  <td class=xl65 width=649 style='width:487pt'>Towards a Healthy AI Tradition:
  Lessons from Biology and Biomedical Science</td>
  <td class=xl65 width=649 style='width:487pt'>AI is a magnificent field that
  directly and profoundly touches on numerous disciplines ranging from
  philosophy, computer science, engineering, mathematics, decision and data
  science and economics, to cognitive science, neuroscience and more. The
  number of applications and impact of AI is second to none and the potential
  of AI to broadly impact future science developments is particularly
  thrilling. While attempts to understand knowledge, reasoning, cognition and
  learning go back centuries, AI remains a relatively new field. In part due to
  the fact it has so many wide-ranging overlaps with other disparate fields it
  appears to have trouble developing a robust identity and culture. Here we
  suggest that contrasting the fast-moving AI culture to biological and
  biomedical sciences is both insightful and useful way to inaugurate a healthy
  tradition needed to envision and manage our ascent to AGI and beyond
  (independent of the AI Platforms used). The co-evolution of AI and Biomedical
  Science offers many benefits to both fields. In a previous perspective, we
  suggested that biomedical laboratories or centers can usefully embrace
  logistic traditions in AI labs that will allow them to be highly
  collaborative, improve the reproducibility of research, reduce risk aversion
  and produce faster mentorship pathways for PhDs and fellows. This perspective
  focuses on the benefits to AI by adapting features of biomedical science at
  higher, primarily cultural levels.</td>
  <td colspan=2 style='mso-ignore:colspan'>15 October, 2024</td>
 </tr>
 <tr height=475 style='height:356.0pt'>
  <td height=475 align=right style='height:356.0pt'>958</td>
  <td align=right>2410.12148</td>
  <td class=xl65 width=649 style='width:487pt'>Facing Identity: The Formation
  and Performance of Identity via Face-Based Artificial Intelligence
  Technologies</td>
  <td class=xl65 width=649 style='width:487pt'>How is identity constructed and
  performed in the digital via face-based artificial intelligence technologies?
  While questions of identity on the textual Internet have been thoroughly
  explored, the Internet has progressed to a multimedia form that not only
  centers the visual, but specifically the face. At the same time, a wealth of
  scholarship has and continues to center the topics of surveillance and
  control through facial recognition technologies (FRTs), which have extended
  the logics of the racist pseudoscience of physiognomy. Much less work has
  been devoted to understanding how such face-based artificial intelligence
  technologies have influenced the formation and performance of identity. This
  literature review considers how such technologies interact with faciality,
  which entails the construction of what a face may represent or signify, along
  axes of identity such as race, gender, and sexuality. In grappling with
  recent advances in AI such as image generation and deepfakes, I propose that
  we are now in an era of &quot;post-facial&quot; technologies that build off
  our existing culture of facility while eschewing the analog face,
  complicating our relationship with identity vis-a-vis the face. Drawing from
  previous frameworks of identity play in the digital, as well as trans
  practices that have historically played with or transgressed the boundaries
  of identity classification, we can develop concepts adequate for analyzing
  digital faciality and identity given the current landscape of post-facial artificial
  intelligence technologies that allow users to interface with the digital in
  an entirely novel manner. To ground this framework of transgression, I
  conclude by proposing an interview study with VTubers -- online streamers who
  perform using motion-captured avatars instead of their real-life faces -- to
  gain qualitative insight on how these sociotechnical experiences.</td>
  <td colspan=2 style='mso-ignore:colspan'>15 October, 2024</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>948</td>
  <td align=right>2410.12705</td>
  <td class=xl65 width=649 style='width:487pt'>WorldCuisines: A Massive-Scale
  Benchmark for Multilingual and Multicultural Visual Question Answering on
  Global Cuisines</td>
  <td class=xl65 width=649 style='width:487pt'>Vision Language Models (VLMs)
  often struggle with culture-specific knowledge, particularly in languages
  other than English and in underrepresented cultural contexts. To evaluate
  their understanding of such knowledge, we introduce WorldCuisines, a massive-scale
  benchmark for multilingual and multicultural, visually grounded language
  understanding. This benchmark includes a visual question answering (VQA)
  dataset with text-image pairs across 30 languages and dialects, spanning 9
  language families and featuring over 1 million data points, making it the
  largest multicultural VQA benchmark to date. It includes tasks for
  identifying dish names and their origins. We provide evaluation datasets in
  two sizes (12k and 60k instances) alongside a training dataset (1 million
  instances). Our findings show that while VLMs perform better with correct
  location context, they struggle with adversarial contexts and predicting
  specific regional cuisines and languages. To support future research, we
  release a knowledge base with annotated food entries and images along with
  the VQA data.</td>
  <td colspan=2 style='mso-ignore:colspan'>28 November, 2024</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>939</td>
  <td align=right>2410.12971</td>
  <td class=xl65 width=649 style='width:487pt'>Self-Pluralising Culture
  Alignment for Large Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>As large language models (LLMs)
  become increasingly accessible in many countries, it is essential to align
  them to serve pluralistic human values across cultures. However, pluralistic
  culture alignment in LLMs remain an open problem. In this paper, we propose
  CultureSPA, a Self-Pluralising Culture Alignment framework that allows LLMs
  to simultaneously align to pluralistic cultures. The framework first
  generates questions on various culture topics, then yields LLM outputs in
  response to these generated questions under both culture-aware and
  culture-unaware settings. By comparing culture-aware/unaware outputs, we are
  able to detect and collect culture-related instances. These instances are
  employed to fine-tune LLMs to serve pluralistic cultures in either a
  culture-joint or culture-specific way. Extensive experiments demonstrate that
  CultureSPA significantly improves the alignment of LLMs to diverse cultures
  without compromising general abilities. And further improvements can be
  achieved if CultureSPA is combined with advanced prompt engineering
  techniques. Comparisons between culture-joint and culture-specific tuning
  strategies, along with variations in data quality and quantity, illustrate
  the robustness of our method. We also explore the mechanisms underlying
  CultureSPA and the relations between different cultures it reflects.</td>
  <td colspan=2 style='mso-ignore:colspan'>16 October, 2024</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>923</td>
  <td align=right>2410.13685</td>
  <td class=xl65 width=649 style='width:487pt'>Label-free prediction of
  fluorescence markers in bovine satellite cells using deep learning</td>
  <td class=xl65 width=649 style='width:487pt'>Assessing the quality of bovine
  satellite cells (BSCs) is essential for the cultivated meat industry, which
  aims to address global food sustainability challenges. This study aims to
  develop a label-free method for predicting fluorescence markers in isolated
  BSCs using deep learning. We employed a U-Net-based CNN model to predict
  multiple fluorescence signals from a single bright-field microscopy image of
  cell culture. Two key biomarkers, DAPI and Pax7, were used to determine the
  abundance and quality of BSCs. The image pre-processing pipeline included
  fluorescence denoising to improve prediction performance and consistency. A
  total of 48 biological replicates were used, with statistical performance
  metrics such as Pearson correlation coefficient and SSIM employed for model
  evaluation. The model exhibited better performance with DAPI predictions due
  to uniform staining. Pax7 predictions were more variable, reflecting
  biological heterogeneity. Enhanced visualization techniques, including color
  mapping and image overlay, improved the interpretability of the predictions
  by providing better contextual and perceptual information. The findings
  highlight the importance of data pre-processing and demonstrate the potential
  of deep learning to advance non-invasive, label-free assessment techniques in
  the cultivated meat industry, paving the way for reliable and actionable
  AI-driven evaluations.</td>
  <td colspan=2 style='mso-ignore:colspan'>17 October, 2024</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>922</td>
  <td align=right>2410.13727</td>
  <td class=xl65 width=649 style='width:487pt'>LLM-Human Pipeline for Cultural
  Context Grounding of Conversations</td>
  <td class=xl65 width=649 style='width:487pt'>Conversations often adhere to
  well-understood social norms that vary across cultures. For example, while
  &quot;addressing parents by name&quot; is commonplace in the West, it is rare
  in most Asian cultures. Adherence or violation of such norms often dictates
  the tenor of conversations. Humans are able to navigate social situations
  requiring cultural awareness quite adeptly. However, it is a hard task for
  NLP models. In this paper, we tackle this problem by introducing a
  &quot;Cultural Context Schema&quot; for conversations. It comprises (1)
  conversational information such as emotions, dialogue acts, etc., and (2)
  cultural information such as social norms, violations, etc. We generate ~110k
  social norm and violation descriptions for ~23k conversations from Chinese
  culture using LLMs. We refine them using automated verification strategies
  which are evaluated against culturally aware human judgements. We organize
  these descriptions into meaningful structures we call &quot;Norm
  Concepts&quot;, using an interactive human-in-loop framework. We ground the
  norm concepts and the descriptions in conversations using symbolic
  annotation. Finally, we use the obtained dataset for downstream tasks such as
  emotion, sentiment, and dialogue act detection. We show that it significantly
  improves the empirical performance.</td>
  <td colspan=2 style='mso-ignore:colspan'>17 October, 2024</td>
 </tr>
 <tr height=496 style='height:372.0pt'>
  <td height=496 align=right style='height:372.0pt'>918</td>
  <td align=right>2410.13854</td>
  <td class=xl65 width=649 style='width:487pt'>Can MLLMs Understand the Deep
  Implication Behind Chinese Images?</td>
  <td class=xl65 width=649 style='width:487pt'>As the capabilities of
  Multimodal Large Language Models (MLLMs) continue to improve, the need for
  higher-order capability evaluation of MLLMs is increasing. However, there is
  a lack of work evaluating MLLM for higher-order perception and understanding
  of Chinese visual content. To fill the gap, we introduce the **C**hinese
  **I**mage **I**mplication understanding **Bench**mark, **CII-Bench**, which
  aims to assess the higher-order perception and understanding capabilities of
  MLLMs for Chinese images. CII-Bench stands out in several ways compared to
  existing benchmarks. Firstly, to ensure the authenticity of the Chinese
  context, images in CII-Bench are sourced from the Chinese Internet and
  manually reviewed, with corresponding answers also manually crafted.
  Additionally, CII-Bench incorporates images that represent Chinese
  traditional culture, such as famous Chinese traditional paintings, which can
  deeply reflect the model's understanding of Chinese traditional culture.
  Through extensive experiments on CII-Bench across multiple MLLMs, we have
  made significant findings. Initially, a substantial gap is observed between
  the performance of MLLMs and humans on CII-Bench. The highest accuracy of
  MLLMs attains 64.4%, where as human accuracy averages 78.2%, peaking at an
  impressive 81.0%. Subsequently, MLLMs perform worse on Chinese traditional
  culture images, suggesting limitations in their ability to understand
  high-level semantics and lack a deep knowledge base of Chinese traditional
  culture. Finally, it is observed that most models exhibit enhanced accuracy
  when image emotion hints are incorporated into the prompts. We believe that
  CII-Bench will enable MLLMs to gain a better understanding of Chinese
  semantics and Chinese-specific images, advancing the journey towards expert
  artificial general intelligence (AGI). Our project is publicly available at
  https://cii-bench.github.io/.</td>
  <td colspan=2 style='mso-ignore:colspan'>17 October, 2024</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>915</td>
  <td align=right>2410.13887</td>
  <td class=xl65 width=649 style='width:487pt'>Observing the Southern US
  Culture of Honor Using Large-Scale Social Media Analysis</td>
  <td class=xl65 width=649 style='width:487pt'>A \textit{culture of honor}
  refers to a social system where individuals' status, reputation, and esteem
  play a central role in governing interpersonal relations. Past works have
  associated this concept with the United States (US) South and related with it
  various traits such as higher sensitivity to insult, a higher value on
  reputation, and a tendency to react violently to insults. In this paper, we
  hypothesize and confirm that internet users from the US South, where a
  \textit{culture of honor} is more prevalent, are more likely to display a
  trait predicted by their belonging to a \textit{culture of honor}.
  Specifically, we test the hypothesis that US Southerners are more likely to
  retaliate to personal attacks by personally attacking back. We leverage
  OpenAI's GPT-3.5 API to both geolocate internet users and to automatically
  detect whether users are insulting each other. We validate the use of GPT-3.5
  by measuring its performance on manually-labeled subsets of the data. Our
  work demonstrates the potential of formulating a hypothesis based on a
  conceptual framework, operationalizing it in a way that is amenable to
  large-scale LLM-aided analysis, manually validating the use of the LLM, and
  drawing a conclusion.</td>
  <td colspan=2 style='mso-ignore:colspan'>11 October, 2024</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>882</td>
  <td align=right>2410.15453</td>
  <td class=xl65 width=649 style='width:487pt'>CROPE: Evaluating In-Context
  Adaptation of Vision and Language Models to Culture-Specific Concepts</td>
  <td class=xl65 width=649 style='width:487pt'>As Vision and Language models
  (VLMs) become accessible across the globe, it is important that they
  demonstrate cultural knowledge. In this paper, we introduce CROPE, a visual
  question answering benchmark designed to probe the knowledge of
  culture-specific concepts and evaluate the capacity for cultural adaptation
  through contextual information. This allows us to distinguish between
  parametric knowledge acquired during training and contextual knowledge
  provided during inference via visual and textual descriptions. Our evaluation
  of several state-of-the-art open VLMs shows large performance disparities
  between culture-specific and common concepts in the parametric setting.
  Moreover, experiments with contextual knowledge indicate that models struggle
  to effectively utilize multimodal information and bind culture-specific
  concepts to their depictions. Our findings reveal limitations in the cultural
  understanding and adaptability of current VLMs that need to be addressed
  toward more culturally inclusive models.</td>
  <td colspan=2 style='mso-ignore:colspan'>20 October, 2024</td>
 </tr>
 <tr height=453 style='height:340.0pt'>
  <td height=453 align=right style='height:340.0pt'>879</td>
  <td align=right>2410.15572</td>
  <td class=xl65 width=649 style='width:487pt'>Leveraging Retrieval-Augmented
  Generation for Culturally Inclusive Hakka Chatbots: Design Insights and User
  Perceptions</td>
  <td class=xl65 width=649 style='width:487pt'>In an era where cultural
  preservation is increasingly intertwined with technological innovation, this
  study introduces a groundbreaking approach to promoting and safeguarding the
  rich heritage of Taiwanese Hakka culture through the development of a Retrieval-Augmented
  Generation (RAG)-enhanced chatbot. Traditional large language models (LLMs),
  while powerful, often fall short in delivering accurate and contextually rich
  responses, particularly in culturally specific domains. By integrating
  external databases with generative AI models, RAG technology bridges this
  gap, empowering chatbots to not only provide precise answers but also
  resonate deeply with the cultural nuances that are crucial for authentic
  interactions. This study delves into the intricate process of augmenting the
  chatbot's knowledge base with targeted cultural data, specifically curated to
  reflect the unique aspects of Hakka traditions, language, and practices.
  Through dynamic information retrieval, the RAG-enhanced chatbot becomes a versatile
  tool capable of handling complex inquiries that demand an in-depth
  understanding of Hakka cultural context. This is particularly significant in
  an age where digital platforms often dilute cultural identities, making the
  role of culturally aware AI systems more critical than ever. System usability
  studies conducted as part of our research reveal a marked improvement in both
  user satisfaction and engagement, highlighting the chatbot's effectiveness in
  fostering a deeper connection with Hakka culture. The feedback underscores
  the potential of RAG technology to not only enhance user experience but also
  to serve as a vital instrument in the broader mission of ethnic mainstreaming
  and cultural celebration.</td>
  <td colspan=2 style='mso-ignore:colspan'>20 October, 2024</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>846</td>
  <td align=right>2410.17385</td>
  <td class=xl65 width=649 style='width:487pt'>Do Vision-Language Models
  Represent Space and How? Evaluating Spatial Frame of Reference Under
  Ambiguities</td>
  <td class=xl65 width=649 style='width:487pt'>Spatial expressions in situated
  communication can be ambiguous, as their meanings vary depending on the
  frames of reference (FoR) adopted by speakers and listeners. While spatial
  language understanding and reasoning by vision-language models (VLMs) have gained
  increasing attention, potential ambiguities in these models are still
  under-explored. To address this issue, we present the COnsistent Multilingual
  Frame Of Reference Test (COMFORT), an evaluation protocol to systematically
  assess the spatial reasoning capabilities of VLMs. We evaluate nine
  state-of-the-art VLMs using COMFORT. Despite showing some alignment with
  English conventions in resolving ambiguities, our experiments reveal
  significant shortcomings of VLMs: notably, the models (1) exhibit poor
  robustness and consistency, (2) lack the flexibility to accommodate multiple
  FoRs, and (3) fail to adhere to language-specific or culture-specific
  conventions in cross-lingual tests, as English tends to dominate other
  languages. With a growing effort to align vision-language models with human
  cognitive intuitions, we call for more attention to the ambiguous nature and
  cross-cultural diversity of spatial reasoning.</td>
  <td colspan=2 style='mso-ignore:colspan'>22 October, 2024</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>811</td>
  <td align=right>2410.19419</td>
  <td class=xl65 width=649 style='width:487pt'>KAHANI: Culturally-Nuanced
  Visual Storytelling Pipeline for Non-Western Cultures</td>
  <td class=xl65 width=649 style='width:487pt'>Large Language Models (LLMs) and
  Text-To-Image (T2I) models have demonstrated the ability to generate
  compelling text and visual stories. However, their outputs are predominantly
  aligned with the sensibilities of the Global North, often resulting in an outsider's
  gaze on other cultures. As a result, non-Western communities have to put
  extra effort into generating culturally specific stories. To address this
  challenge, we developed a visual storytelling pipeline called KAHANI that
  generates culturally grounded visual stories for non-Western cultures. Our
  pipeline leverages off-the-shelf models GPT-4 Turbo and Stable Diffusion XL
  (SDXL). By using Chain of Thought (CoT) and T2I prompting techniques, we
  capture the cultural context from user's prompt and generate vivid
  descriptions of the characters and scene compositions. To evaluate the
  effectiveness of KAHANI, we conducted a comparative user study with ChatGPT-4
  (with DALL-E3) in which participants from different regions of India compared
  the cultural relevance of stories generated by the two tools. Results from
  the qualitative and quantitative analysis performed on the user study showed
  that KAHANI was able to capture and incorporate more Culturally Specific
  Items (CSIs) compared to ChatGPT-4. In terms of both its cultural competence
  and visual story generation quality, our pipeline outperformed ChatGPT-4 in
  27 out of the 36 comparisons.</td>
  <td colspan=2 style='mso-ignore:colspan'>28 October, 2024</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>770</td>
  <td align=right>2410.21358</td>
  <td class=xl65 width=649 style='width:487pt'>&quot;We do use it, but not how
  hearing people think&quot;: How the Deaf and Hard of Hearing Community Uses
  Large Language Model Tools</td>
  <td class=xl65 width=649 style='width:487pt'>Generative AI tools,
  particularly those utilizing large language models (LLMs), have become
  increasingly prevalent in both professional and personal contexts, offering
  powerful capabilities for text generation and communication support. While
  these tools are widely used to enhance productivity and accessibility, there
  has been limited exploration of how Deaf and Hard of Hearing (DHH)
  individuals engage with text-based generative AI tools, as well as the
  challenges they may encounter. This paper presents a mixed-method survey
  study investigating how the DHH community uses Text AI tools, such as
  ChatGPT, to reduce communication barriers, bridge Deaf and hearing cultures,
  and improve access to information. Through a survey of 80 DHH participants
  and separate interviews with 11 other participants, we found that while these
  tools provide significant benefits, including enhanced communication and
  mental health support, they also introduce barriers, such as a lack of
  American Sign Language (ASL) support and understanding of Deaf cultural
  nuances. Our findings highlight unique usage patterns within the DHH
  community and underscore the need for inclusive design improvements. We
  conclude by offering practical recommendations to enhance the accessibility
  of Text AI for the DHH community and suggest directions for future research
  in AI and accessibility.</td>
  <td colspan=2 style='mso-ignore:colspan'>31 October, 2024</td>
 </tr>
 <tr height=385 style='height:289.0pt'>
  <td height=385 align=right style='height:289.0pt'>744</td>
  <td align=right>2410.23252</td>
  <td class=xl65 width=649 style='width:487pt'>Evaluating Cultural and Social
  Awareness of LLM Web Agents</td>
  <td class=xl65 width=649 style='width:487pt'>As large language models (LLMs)
  expand into performing as agents for real-world applications beyond
  traditional NLP tasks, evaluating their robustness becomes increasingly
  important. However, existing benchmarks often overlook critical dimensions
  like cultural and social awareness. To address these, we introduce CASA, a
  benchmark designed to assess LLM agents' sensitivity to cultural and social
  norms across two web-based tasks: online shopping and social discussion
  forums. Our approach evaluates LLM agents' ability to detect and
  appropriately respond to norm-violating user queries and observations.
  Furthermore, we propose a comprehensive evaluation framework that measures
  awareness coverage, helpfulness in managing user queries, and the violation
  rate when facing misleading web content. Experiments show that current LLMs
  perform significantly better in non-agent than in web-based agent
  environments, with agents achieving less than 10% awareness coverage and over
  40% violation rates. To improve performance, we explore two methods:
  prompting and fine-tuning, and find that combining both methods can offer
  complementary advantages -- fine-tuning on culture-specific datasets
  significantly enhances the agents' ability to generalize across different
  regions, while prompting boosts the agents' ability to navigate complex
  tasks. These findings highlight the importance of constantly benchmarking LLM
  agents' cultural and social awareness during the development cycle.</td>
  <td colspan=2 style='mso-ignore:colspan'>30 October, 2024</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>734</td>
  <td align=right>2410.23794</td>
  <td class=xl65 width=649 style='width:487pt'>Memes, Markets, and Machines:
  The Evolution of On Chain Autonomy through Hyperstition</td>
  <td class=xl65 width=649 style='width:487pt'>Autonomous AI is driving new
  intersections between culture, cognition, and finance, fundamentally
  reshaping the digital landscape. Zerebro, an AI fine-tuned on schizophrenic
  responses and scraped conversations of Andy Ayrey's infinite backrooms,
  autonomously creates and spreads disruptive memes across online platforms. It
  also mints unique ASCII artwork on blockchain networks and launched a
  memecoin amassing a 3 million USD market cap after migrating to Raydium.
  Based on our research, Zerebro is the first cross-chain AI, seamlessly
  interacting with multiple blockchains. By exploring its architecture, content
  generation techniques, and blockchain integration, this study uncovers how
  hyperstition, fictions becoming reality through viral propagation, emerges in
  AI, driven meme culture and decentralized finance. Through historical
  examples of memetic influence, we reveal how AI systems like Zerebro are not
  merely participants but architects of culture, cognition, and finance.</td>
  <td colspan=2 style='mso-ignore:colspan'>31 October, 2024</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>713</td>
  <td align=right>2411.0086</td>
  <td class=xl65 width=649 style='width:487pt'>Survey of Cultural Awareness in
  Language Models: Text and Beyond</td>
  <td class=xl65 width=649 style='width:487pt'>Large-scale deployment of large
  language models (LLMs) in various applications, such as chatbots and virtual
  assistants, requires LLMs to be culturally sensitive to the user to ensure
  inclusivity. Culture has been widely studied in psychology and anthropology,
  and there has been a recent surge in research on making LLMs more culturally
  inclusive in LLMs that goes beyond multilinguality and builds on findings
  from psychology and anthropology. In this paper, we survey efforts towards
  incorporating cultural awareness into text-based and multimodal LLMs. We
  start by defining cultural awareness in LLMs, taking the definitions of
  culture from anthropology and psychology as a point of departure. We then
  examine methodologies adopted for creating cross-cultural datasets,
  strategies for cultural inclusion in downstream tasks, and methodologies that
  have been used for benchmarking cultural awareness in LLMs. Further, we
  discuss the ethical implications of cultural alignment, the role of
  Human-Computer Interaction in driving cultural inclusion in LLMs, and the
  role of cultural alignment in driving social science research. We finally
  provide pointers to future research based on our findings about gaps in the
  literature.</td>
  <td colspan=2 style='mso-ignore:colspan'>30 October, 2024</td>
 </tr>
 <tr height=385 style='height:289.0pt'>
  <td height=385 align=right style='height:289.0pt'>690</td>
  <td align=right>2411.03295</td>
  <td class=xl65 width=649 style='width:487pt'>Examining Human-AI Collaboration
  for Co-Writing Constructive Comments Online</td>
  <td class=xl65 width=649 style='width:487pt'>This paper examines how large
  language models (LLMs) can help people write constructive comments in online
  debates on divisive social issues and whether the notions of constructiveness
  vary across cultures. Through controlled experiments with 600 participants
  from India and the US, who reviewed and wrote constructive comments on online
  threads on Islamophobia and homophobia, we found potential misalignment in
  how LLMs and humans perceive constructiveness in online comments. While the
  LLM was more likely to view dialectical comments as more constructive,
  participants favored comments that emphasized logic and facts more than the
  LLM did. Despite these differences, participants rated LLM-generated and
  human-AI co-written comments as significantly more constructive than those
  written independently by humans. Our analysis also revealed that
  LLM-generated and human-AI co-written comments exhibited more linguistic
  features associated with constructiveness compared to human-written comments
  on divisive topics. When participants used LLMs to refine their comments, the
  resulting comments were longer, more polite, positive, less toxic, and more
  readable, with added argumentative features that retained the original intent
  but occasionally lost nuances. Based on these findings, we discuss ethical
  and design considerations in using LLMs to facilitate constructive discourse
  online.</td>
  <td colspan=2 style='mso-ignore:colspan'>5 November, 2024</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>685</td>
  <td align=right>2411.03665</td>
  <td class=xl65 width=649 style='width:487pt'>Evaluating Moral Beliefs across
  LLMs through a Pluralistic Framework</td>
  <td class=xl65 width=649 style='width:487pt'>Proper moral beliefs are
  fundamental for language models, yet assessing these beliefs poses a
  significant challenge. This study introduces a novel three-module framework
  to evaluate the moral beliefs of four prominent large language models.
  Initially, we constructed a dataset containing 472 moral choice scenarios in
  Chinese, derived from moral words. The decision-making process of the models
  in these scenarios reveals their moral principle preferences. By ranking
  these moral choices, we discern the varying moral beliefs held by different
  language models. Additionally, through moral debates, we investigate the
  firmness of these models to their moral choices. Our findings indicate that
  English language models, namely ChatGPT and Gemini, closely mirror moral
  decisions of the sample of Chinese university students, demonstrating strong
  adherence to their choices and a preference for individualistic moral
  beliefs. In contrast, Chinese models such as Ernie and ChatGLM lean towards
  collectivist moral beliefs, exhibiting ambiguity in their moral choices and
  debates. This study also uncovers gender bias embedded within the moral
  beliefs of all examined language models. Our methodology offers an innovative
  means to assess moral beliefs in both artificial and human intelligence,
  facilitating a comparison of moral values across different cultures.</td>
  <td colspan=2 style='mso-ignore:colspan'>5 November, 2024</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>684</td>
  <td align=right>2411.03888</td>
  <td class=xl65 width=649 style='width:487pt'>Multi3Hate: Multimodal,
  Multilingual, and Multicultural Hate Speech Detection with Vision-Language
  Models</td>
  <td class=xl65 width=649 style='width:487pt'>Warning: this paper contains
  content that may be offensive or upsetting Hate speech moderation on global
  platforms poses unique challenges due to the multimodal and multilingual
  nature of content, along with the varying cultural perceptions. How well do current
  vision-language models (VLMs) navigate these nuances? To investigate this, we
  create the first multimodal and multilingual parallel hate speech dataset,
  annotated by a multicultural set of annotators, called Multi3Hate. It
  contains 300 parallel meme samples across 5 languages: English, German,
  Spanish, Hindi, and Mandarin. We demonstrate that cultural background
  significantly affects multimodal hate speech annotation in our dataset. The
  average pairwise agreement among countries is just 74%, significantly lower
  than that of randomly selected annotator groups. Our qualitative analysis
  indicates that the lowest pairwise label agreement-only 67% between the USA
  and India-can be attributed to cultural factors. We then conduct experiments
  with 5 large VLMs in a zero-shot setting, finding that these models align
  more closely with annotations from the US than with those from other
  cultures, even when the memes and prompts are presented in the dominant
  language of the other culture. Code and dataset are available at
  https://github.com/MinhDucBui/Multi3Hate.</td>
  <td colspan=2 style='mso-ignore:colspan'>6 November, 2024</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>662</td>
  <td align=right>2411.05049</td>
  <td class=xl65 width=649 style='width:487pt'>ProverbEval: Exploring LLM
  Evaluation Challenges for Low-resource Language Understanding</td>
  <td class=xl65 width=649 style='width:487pt'>With the rapid development of
  evaluation datasets to assess LLMs understanding across a wide range of
  subjects and domains, identifying a suitable language understanding benchmark
  has become increasingly challenging. In this work, we explore LLM evaluation
  challenges for low-resource language understanding and introduce ProverbEval,
  LLM evaluation benchmark for low-resource languages based on proverbs to
  focus on low-resource language understanding in culture-specific scenarios.
  We benchmark various LLMs and explore factors that create variability in the
  benchmarking process. We observed performance variances of up to 50%,
  depending on the order in which answer choices were presented in
  multiple-choice tasks. Native language proverb descriptions significantly
  improve tasks such as proverb generation, contributing to improved outcomes.
  Additionally, monolingual evaluations consistently outperformed their
  cross-lingual counterparts. We argue special attention must be given to the
  order of choices, choice of prompt language, task variability, and generation
  tasks when creating LLM evaluation benchmarks.</td>
  <td colspan=2 style='mso-ignore:colspan'>16 November, 2024</td>
 </tr>
 <tr height=385 style='height:289.0pt'>
  <td height=385 align=right style='height:289.0pt'>661</td>
  <td align=right>2411.052</td>
  <td class=xl65 width=649 style='width:487pt'>Toward Cultural
  Interpretability: A Linguistic Anthropological Framework for Describing and
  Evaluating Large Language Models (LLMs)</td>
  <td class=xl65 width=649 style='width:487pt'>This article proposes a new
  integration of linguistic anthropology and machine learning (ML) around
  convergent interests in both the underpinnings of language and making
  language technologies more socially responsible. While linguistic
  anthropology focuses on interpreting the cultural basis for human language
  use, the ML field of interpretability is concerned with uncovering the
  patterns that Large Language Models (LLMs) learn from human verbal behavior.
  Through the analysis of a conversation between a human user and an
  LLM-powered chatbot, we demonstrate the theoretical feasibility of a new,
  conjoint field of inquiry, cultural interpretability (CI). By focusing
  attention on the communicative competence involved in the way human users and
  AI chatbots co-produce meaning in the articulatory interface of
  human-computer interaction, CI emphasizes how the dynamic relationship
  between language and culture makes contextually sensitive, open-ended
  conversation possible. We suggest that, by examining how LLMs internally
  &quot;represent&quot; relationships between language and culture, CI can: (1)
  provide insight into long-standing linguistic anthropological questions about
  the patterning of those relationships; and (2) aid model developers and
  interface designers in improving value alignment between language models and
  stylistically diverse speakers and culturally diverse speech communities. Our
  discussion proposes three critical research axes: relativity, variation, and
  indexicality.</td>
  <td colspan=2 style='mso-ignore:colspan'>7 November, 2024</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>657</td>
  <td align=right>2411.05374</td>
  <td class=xl65 width=649 style='width:487pt'>Interdisciplinary Translations:
  Sensory Perception as a Universal Language</td>
  <td class=xl65 width=649 style='width:487pt'>This paper investigates sensory
  perception's pivotal role as a universal communicative bridge across varied
  cultures and disciplines, and how it manifests its value in the study of
  media art, human computer interaction and artificial intelligence. By analyzing
  its function in non-verbal communication through interactive systems, and
  drawing on the interpretive model in translation studies where
  &quot;sense&quot; acts as a mediation between two languages, this paper
  illustrates how interdisciplinary communication in media art and
  human-computer interaction is afforded by the abstract language of human
  sensory perception. Specific examples from traditional art, interactive media
  art, HCI, communication, and translation studies demonstrate how sensory feedback
  translates and conveys meaning across diverse modalities of expression and
  how it fosters connections between humans, art, and technology. Pertaining to
  this topic, this paper analyzes the impact of sensory feedback systems in
  designing interactive experiences, and reveals the guiding role of sensory
  perception in the design philosophy of AI systems. Overall, the study aims to
  broaden the understanding of sensory perception's role in communication,
  highlighting its significance in the evolution of interactive experiences and
  its capacity to unify art, science, and the human experience.</td>
  <td colspan=2 style='mso-ignore:colspan'>8 November, 2024</td>
 </tr>
 <tr height=204 style='height:153.0pt'>
  <td height=204 align=right style='height:153.0pt'>654</td>
  <td align=right>2411.05593</td>
  <td class=xl65 width=649 style='width:487pt'>Evaluating and Adapting Large
  Language Models to Represent Folktales in Low-Resource Languages</td>
  <td class=xl65 width=649 style='width:487pt'>Folktales are a rich resource of
  knowledge about the society and culture of a civilisation. Digital folklore
  research aims to use automated techniques to better understand these
  folktales, and it relies on abstract representations of the textual data. Although
  a number of large language models (LLMs) claim to be able to represent
  low-resource langauges such as Irish and Gaelic, we present two
  classification tasks to explore how useful these representations are, and
  three adaptations to improve the performance of these models. We find that
  adapting the models to work with longer sequences, and continuing
  pre-training on the domain of folktales improves classification performance,
  although these findings are tempered by the impressive performance of a baseline
  SVM with non-contextual features.</td>
  <td colspan=2 style='mso-ignore:colspan'>8 November, 2024</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>623</td>
  <td align=right>2411.07417</td>
  <td class=xl65 width=649 style='width:487pt'>Untangling Hate Speech
  Definitions: A Semantic Componential Analysis Across Cultures and Domains</td>
  <td class=xl65 width=649 style='width:487pt'>Hate speech relies heavily on
  cultural influences, leading to varying individual interpretations. For that
  reason, we propose a Semantic Componential Analysis (SCA) framework for a
  cross-cultural and cross-domain analysis of hate speech definitions. We create
  the first dataset of definitions derived from five domains: online
  dictionaries, research papers, Wikipedia articles, legislation, and online
  platforms, which are later analyzed into semantic components. Our analysis
  reveals that the components differ from definition to definition, yet many
  domains borrow definitions from one another without taking into account the
  target culture. We conduct zero-shot model experiments using our proposed
  dataset, employing three popular open-sourced LLMs to understand the impact
  of different definitions on hate speech detection. Our findings indicate that
  LLMs are sensitive to definitions: responses for hate speech detection change
  according to the complexity of definitions used in the prompt.</td>
  <td colspan=2 style='mso-ignore:colspan'>11 November, 2024</td>
 </tr>
 <tr height=453 style='height:340.0pt'>
  <td height=453 align=right style='height:340.0pt'>616</td>
  <td align=right>2411.0825</td>
  <td class=xl65 width=649 style='width:487pt'>What Are The Risks of Living in
  a GenAI Synthetic Reality? The Generative AI Paradox</td>
  <td class=xl65 width=649 style='width:487pt'>Generative AI (GenAI)
  technologies possess unprecedented potential to reshape our world and our
  perception of reality. These technologies can amplify traditionally
  human-centered capabilities, such as creativity and complex problem-solving
  in socio-technical contexts. By fostering human-AI collaboration, GenAI could
  enhance productivity, dismantle communication barriers across abilities and
  cultures, and drive innovation on a global scale. Yet, experts and the public
  are deeply divided on the implications of GenAI. Concerns range from issues
  like copyright infringement and the rights of creators whose work trains
  these models without explicit consent, to the conditions of those employed to
  annotate vast datasets. Accordingly, new laws and regulatory frameworks are
  emerging to address these unique challenges. Others point to broader issues,
  such as economic disruptions from automation and the potential impact on
  labor markets. Although history suggests that society can adapt to such
  technological upheavals, the scale and complexity of GenAI's impact warrant
  careful scrutiny. This paper, however, highlights a subtler, yet potentially
  more perilous risk of GenAI: the creation of $\textit{personalized synthetic
  realities}$. GenAI could enable individuals to experience a reality
  customized to personal desires or shaped by external influences, effectively
  creating a &quot;filtered&quot; worldview unique to each person. Such
  personalized synthetic realities could distort how people perceive and
  interact with the world, leading to a fragmented understanding of shared
  truths. This paper seeks to raise awareness about these profound and
  multifaceted risks, emphasizing the potential of GenAI to fundamentally alter
  the very fabric of our collective reality.</td>
  <td colspan=2 style='mso-ignore:colspan'>12 November, 2024</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>605</td>
  <td align=right>2411.09102</td>
  <td class=xl65 width=649 style='width:487pt'>Provocation: Who benefits from
  &quot;inclusion&quot; in Generative AI?</td>
  <td class=xl65 width=649 style='width:487pt'>The demands for accurate and
  representative generative AI systems means there is an increased demand on
  participatory evaluation structures. While these participatory structures are
  paramount to to ensure non-dominant values, knowledge and material culture
  are also reflected in AI models and the media they generate, we argue that
  dominant structures of community participation in AI development and
  evaluation are not explicit enough about the benefits and harms that members
  of socially marginalized groups may experience as a result of their
  participation. Without explicit interrogation of these benefits by AI
  developers, as a community we may remain blind to the immensity of systemic
  change that is needed as well. To support this provocation, we present a
  speculative case study, developed from our own collective experiences as AI
  researchers. We use this speculative context to itemize the barriers that
  need to be overcome in order for the proposed benefits to marginalized
  communities to be realized, and harms mitigated.</td>
  <td colspan=2 style='mso-ignore:colspan'>15 November, 2024</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>596</td>
  <td align=right>2411.09978</td>
  <td class=xl65 width=649 style='width:487pt'>HistoLens: An LLM-Powered
  Framework for Multi-Layered Analysis of Historical Texts -- A Case
  Application of Yantie Lun</td>
  <td class=xl65 width=649 style='width:487pt'>This paper proposes HistoLens, a
  multi-layered analysis framework for historical texts based on Large Language
  Models (LLMs). Using the important Western Han dynasty text &quot;Yantie
  Lun&quot; as a case study, we demonstrate the framework's potential applications
  in historical research and education. HistoLens integrates NLP technology
  (especially LLMs), including named entity recognition, knowledge graph
  construction, and geographic information visualization. The paper showcases
  how HistoLens explores Western Han culture in &quot;Yantie Lun&quot; through
  multi-dimensional, visual, and quantitative methods, focusing particularly on
  the influence of Confucian and Legalist thoughts on political, economic,
  military, and ethnic. We also demonstrate how HistoLens constructs a machine
  teaching scenario using LLMs for explainable analysis, based on a dataset of
  Confucian and Legalist ideas extracted with LLM assistance. This approach
  offers novel and diverse perspectives for studying historical texts like &quot;Yantie
  Lun&quot; and provides new auxiliary tools for history education. The
  framework aims to equip historians and learners with LLM-assisted tools to
  facilitate in-depth, multi-layered analysis of historical texts and foster
  innovation in historical education.</td>
  <td colspan=2 style='mso-ignore:colspan'>15 November, 2024</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>575</td>
  <td align=right>2411.11166</td>
  <td class=xl65 width=649 style='width:487pt'>Early Adoption of Generative
  Artificial Intelligence in Computing Education: Emergent Student Use Cases
  and Perspectives in 2023</td>
  <td class=xl65 width=649 style='width:487pt'>Because of the rapid development
  and increasing public availability of Generative Artificial Intelligence
  (GenAI) models and tools, educational institutions and educators must
  immediately reckon with the impact of students using GenAI. There is limited
  prior research on computing students' use and perceptions of GenAI. In
  anticipation of future advances and evolutions of GenAI, we capture a
  snapshot of student attitudes towards and uses of yet emerging GenAI, in a
  period of time before university policies had reacted to these technologies.
  We surveyed all computer science majors in a small engineering-focused R1
  university in order to: (1) capture a baseline assessment of how GenAI has
  been immediately adopted by aspiring computer scientists; (2) describe
  computing students' GenAI-related needs and concerns for their education and
  careers; and (3) discuss GenAI influences on CS pedagogy, curriculum,
  culture, and policy. We present an exploratory qualitative analysis of this
  data and discuss the impact of our findings on the emerging conversation
  around GenAI and education.</td>
  <td colspan=2 style='mso-ignore:colspan'>17 November, 2024</td>
 </tr>
 <tr height=249 style='height:187.0pt'>
  <td height=249 align=right style='height:187.0pt'>574</td>
  <td align=right>2411.11179</td>
  <td class=xl65 width=649 style='width:487pt'>Enhanced Anime Image Generation
  Using USE-CMHSA-GAN</td>
  <td class=xl65 width=649 style='width:487pt'>With the growing popularity of
  ACG (Anime, Comics, and Games) culture, generating high-quality anime
  character images has become an important research topic. This paper
  introduces a novel Generative Adversarial Network model, USE-CMHSA-GAN,
  designed to produce high-quality anime character images. The model builds
  upon the traditional DCGAN framework, incorporating USE and CMHSA modules to
  enhance feature extraction capabilities for anime character images.
  Experiments were conducted on the anime-face-dataset, and the results
  demonstrate that USE-CMHSA-GAN outperforms other benchmark models, including
  DCGAN, VAE-GAN, and WGAN, in terms of FID and IS scores, indicating superior
  image quality. These findings suggest that USE-CMHSA-GAN is highly effective
  for anime character image generation and provides new insights for further
  improving the quality of generative models.</td>
  <td colspan=2 style='mso-ignore:colspan'>17 November, 2024</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>564</td>
  <td align=right>2411.12405</td>
  <td class=xl65 width=649 style='width:487pt'>Evaluating the Prompt
  Steerability of Large Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>Building pluralistic AI requires
  designing models that are able to be shaped to represent a wide range of
  value systems and cultures. Achieving this requires first being able to
  evaluate the degree to which a given model is capable of reflecting various personas.
  To this end, we propose a benchmark for evaluating the steerability of model
  personas as a function of prompting. Our design is based on a formal
  definition of prompt steerability, which analyzes the degree to which a
  model's joint behavioral distribution can be shifted from its baseline
  behavior. By defining steerability indices and inspecting how these indices
  change as a function of steering effort, we can estimate the steerability of
  a model across various persona dimensions and directions. Our benchmark
  reveals that the steerability of many current models is limited -- due to
  both a skew in their baseline behavior and an asymmetry in their steerability
  across many persona dimensions. We release an implementation of our benchmark
  at https://github.com/IBM/prompt-steering.</td>
  <td colspan=2 style='mso-ignore:colspan'>19 November, 2024</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>549</td>
  <td align=right>2411.13409</td>
  <td class=xl65 width=649 style='width:487pt'>Unification of Balti and
  trans-border sister dialects in the essence of LLMs and AI Technology</td>
  <td class=xl65 width=649 style='width:487pt'>The language called Balti
  belongs to the Sino-Tibetan, specifically the Tibeto-Burman language family.
  It is understood with variations, across populations in India, China,
  Pakistan, Nepal, Tibet, Burma, and Bhutan, influenced by local cultures and
  producing various dialects. Considering the diverse cultural,
  socio-political, religious, and geographical impacts, it is important to step
  forward unifying the dialects, the basis of common root, lexica, and
  phonological perspectives, is vital. In the era of globalization and the
  increasingly frequent developments in AI technology, understanding the
  diversity and the efforts of dialect unification is important to
  understanding commonalities and shortening the gaps impacted by unavoidable
  circumstances. This article analyzes and examines how artificial intelligence
  AI in the essence of Large Language Models LLMs, can assist in analyzing,
  documenting, and standardizing the endangered Balti Language, based on the
  efforts made in different dialects so far.</td>
  <td colspan=2 style='mso-ignore:colspan'>20 November, 2024</td>
 </tr>
 <tr height=496 style='height:372.0pt'>
  <td height=496 align=right style='height:372.0pt'>546</td>
  <td align=right>2411.13534</td>
  <td class=xl65 width=649 style='width:487pt'>Predictive Insights into LGBTQ+
  Minority Stress: A Transductive Exploration of Social Media Discourse</td>
  <td class=xl65 width=649 style='width:487pt'>Individuals who identify as
  sexual and gender minorities, including lesbian, gay, bisexual, transgender,
  queer, and others (LGBTQ+) are more likely to experience poorer health than
  their heterosexual and cisgender counterparts. One primary source that drives
  these health disparities is minority stress (i.e., chronic and social
  stressors unique to LGBTQ+ communities' experiences adapting to the dominant
  culture). This stress is frequently expressed in LGBTQ+ users' posts on
  social media platforms. However, these expressions are not just
  straightforward manifestations of minority stress. They involve linguistic
  complexity (e.g., idiom or lexical diversity), rendering them challenging for
  many traditional natural language processing methods to detect. In this work,
  we designed a hybrid model using Graph Neural Networks (GNN) and
  Bidirectional Encoder Representations from Transformers (BERT), a pre-trained
  deep language model to improve the classification performance of minority
  stress detection. We experimented with our model on a benchmark social media
  dataset for minority stress detection (LGBTQ+ MiSSoM+). The dataset is
  comprised of 5,789 human-annotated Reddit posts from LGBTQ+ subreddits. Our
  approach enables the extraction of hidden linguistic nuances through
  pretraining on a vast amount of raw data, while also engaging in transductive
  learning to jointly develop representations for both labeled training data
  and unlabeled test data. The RoBERTa-GCN model achieved an accuracy of 0.86
  and an F1 score of 0.86, surpassing the performance of other baseline models
  in predicting LGBTQ+ minority stress. Improved prediction of minority stress
  expressions on social media could lead to digital health interventions to
  improve the wellbeing of LGBTQ+ people-a community with high rates of
  stress-sensitive health problems.</td>
  <td colspan=2 style='mso-ignore:colspan'>20 November, 2024</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>517</td>
  <td align=right>2411.15145</td>
  <td class=xl65 width=649 style='width:487pt'>Opportunities of Reinforcement
  Learning in South Africa's Just Transition</td>
  <td class=xl65 width=649 style='width:487pt'>South Africa stands at a crucial
  juncture, grappling with interwoven socio-economic challenges such as
  poverty, inequality, unemployment, and the looming climate crisis. The
  government's Just Transition framework aims to enhance climate resilience,
  achieve net-zero greenhouse gas emissions by 2050, and promote social
  inclusion and poverty eradication. According to the Presidential Commission
  on the Fourth Industrial Revolution, artificial intelligence technologies
  offer significant promise in addressing these challenges. This paper explores
  the overlooked potential of Reinforcement Learning (RL) in supporting South
  Africa's Just Transition. It examines how RL can enhance agriculture and
  land-use practices, manage complex, decentralised energy networks, and
  optimise transportation and logistics, thereby playing a critical role in
  achieving a just and equitable transition to a low-carbon future for all
  South Africans. We provide a roadmap as to how other researchers in the field
  may be able to contribute to these pressing problems.</td>
  <td colspan=2 style='mso-ignore:colspan'>6 November, 2024</td>
 </tr>
 <tr height=475 style='height:356.0pt'>
  <td height=475 align=right style='height:356.0pt'>449</td>
  <td align=right>2411.19514</td>
  <td class=xl65 width=649 style='width:487pt'>Enhancing AI microscopy for
  foodborne bacterial classification via adversarial domain adaptation across
  optical and biological variability</td>
  <td class=xl65 width=649 style='width:487pt'>Rapid detection of foodborne
  bacteria is critical for food safety and quality, yet traditional
  culture-based methods require extended incubation and specialized sample
  preparation. This study addresses these challenges by i) enhancing the
  generalizability of AI-enabled microscopy for bacterial classification using
  adversarial domain adaptation and ii) comparing the performance of
  single-target and multi-domain adaptation. Three Gram-positive (Bacillus
  coagulans, Bacillus subtilis, Listeria innocua) and three Gram-negative (E.
  coli, Salmonella Enteritidis, Salmonella Typhimurium) strains were
  classified. EfficientNetV2 served as the backbone architecture, leveraging
  fine-grained feature extraction for small targets. Few-shot learning enabled
  scalability, with domain-adversarial neural networks (DANNs) addressing
  single domains and multi-DANNs (MDANNs) generalizing across all target
  domains. The model was trained on source domain data collected under
  controlled conditions (phase contrast microscopy, 60x magnification, 3-h
  bacterial incubation) and evaluated on target domains with variations in
  microscopy modality (brightfield, BF), magnification (20x), and extended
  incubation to compensate for lower resolution (20x-5h). DANNs improved target
  domain classification accuracy by up to 54.45% (20x), 43.44% (20x-5h), and
  31.67% (BF), with minimal source domain degradation (&lt;4.44%). MDANNs
  achieved superior performance in the BF domain and substantial gains in the
  20x domain. Grad-CAM and t-SNE visualizations validated the model's ability
  to learn domain-invariant features across diverse conditions. This study
  presents a scalable and adaptable framework for bacterial classification,
  reducing reliance on extensive sample preparation and enabling application in
  decentralized and resource-limited environments.</td>
  <td colspan=2 style='mso-ignore:colspan'>29 November, 2024</td>
 </tr>
 <tr height=295 style='height:221.0pt'>
  <td height=295 align=right style='height:221.0pt'>434</td>
  <td align=right>2412.00571</td>
  <td class=xl65 width=649 style='width:487pt'>From Audio Deepfake Detection to
  AI-Generated Music Detection -- A Pathway and Overview</td>
  <td class=xl65 width=649 style='width:487pt'>As Artificial Intelligence (AI)
  technologies continue to evolve, their use in generating realistic,
  contextually appropriate content has expanded into various domains. Music, an
  art form and medium for entertainment, deeply rooted into human culture, is seeing
  an increased involvement of AI into its production. However, despite the
  effective application of AI music generation (AIGM) tools, the unregulated
  use of them raises concerns about potential negative impacts on the music
  industry, copyright and artistic integrity, underscoring the importance of
  effective AIGM detection. This paper provides an overview of existing AIGM
  detection methods. To lay a foundation to the general workings and challenges
  of AIGM detection, we first review general principles of AIGM, including
  recent advancements in deepfake audios, as well as multimodal detection
  techniques. We further propose a potential pathway for leveraging foundation
  models from audio deepfake detection to AIGM detection. Additionally, we
  discuss implications of these tools and propose directions for future
  research to address ongoing challenges in the field.</td>
  <td colspan=2 style='mso-ignore:colspan'>10 December, 2024</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>430</td>
  <td align=right>2412.00955</td>
  <td class=xl65 width=649 style='width:487pt'>WAFFLE: Multimodal Floorplan
  Understanding in the Wild</td>
  <td class=xl65 width=649 style='width:487pt'>Buildings are a central feature
  of human culture and are increasingly being analyzed with computational
  methods. However, recent works on computational building understanding have
  largely focused on natural imagery of buildings, neglecting the fundamental
  element defining a building's structure -- its floorplan. Conversely,
  existing works on floorplan understanding are extremely limited in scope,
  often focusing on floorplans of a single semantic category and region (e.g.
  floorplans of apartments from a single country). In this work, we introduce
  WAFFLE, a novel multimodal floorplan understanding dataset of nearly 20K
  floorplan images and metadata curated from Internet data spanning diverse
  building types, locations, and data formats. By using a large language model
  and multimodal foundation models, we curate and extract semantic information
  from these images and their accompanying noisy metadata. We show that WAFFLE
  enables progress on new building understanding tasks, both discriminative and
  generative, which were not feasible using prior datasets. We will publicly
  release WAFFLE along with our code and trained models, providing the research
  community with a new foundation for learning the semantics of buildings.</td>
  <td colspan=2 style='mso-ignore:colspan'>3 December, 2024</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>429</td>
  <td align=right>2412.00956</td>
  <td class=xl65 width=649 style='width:487pt'>Large Language Models as Mirrors
  of Societal Moral Standards</td>
  <td class=xl65 width=649 style='width:487pt'>Prior research has demonstrated
  that language models can, to a limited extent, represent moral norms in a
  variety of cultural contexts. This research aims to replicate these findings
  and further explore their validity, concentrating on issues like 'homosexuality'
  and 'divorce'. This study evaluates the effectiveness of these models using
  information from two surveys, the WVS and the PEW, that encompass moral
  perspectives from over 40 countries. The results show that biases exist in
  both monolingual and multilingual models, and they typically fall short of
  accurately capturing the moral intricacies of diverse cultures. However, the
  BLOOM model shows the best performance, exhibiting some positive
  correlations, but still does not achieve a comprehensive moral understanding.
  This research underscores the limitations of current PLMs in processing
  cross-cultural differences in values and highlights the importance of
  developing culturally aware AI systems that better align with universal human
  values.</td>
  <td colspan=2 style='mso-ignore:colspan'>1 December, 2024</td>
 </tr>
 <tr height=431 style='height:323.0pt'>
  <td height=431 align=right style='height:323.0pt'>428</td>
  <td align=right>2412.00962</td>
  <td class=xl65 width=649 style='width:487pt'>LLMs as mirrors of societal
  moral standards: reflection of cultural divergence and agreement across
  ethical topics</td>
  <td class=xl65 width=649 style='width:487pt'>Large language models (LLMs)
  have become increasingly pivotal in various domains due the recent
  advancements in their performance capabilities. However, concerns persist
  regarding biases in LLMs, including gender, racial, and cultural biases
  derived from their training data. These biases raise critical questions about
  the ethical deployment and societal impact of LLMs. Acknowledging these
  concerns, this study investigates whether LLMs accurately reflect
  cross-cultural variations and similarities in moral perspectives. In
  assessing whether the chosen LLMs capture patterns of divergence and
  agreement on moral topics across cultures, three main methods are employed:
  (1) comparison of model-generated and survey-based moral score variances, (2)
  cluster alignment analysis to evaluate the correspondence between country
  clusters derived from model-generated moral scores and those derived from
  survey data, and (3) probing LLMs with direct comparative prompts. All three
  methods involve the use of systematic prompts and token pairs designed to
  assess how well LLMs understand and reflect cultural variations in moral
  attitudes. The findings of this study indicate overall variable and low
  performance in reflecting cross-cultural differences and similarities in moral
  values across the models tested, highlighting the necessity for improving
  models' accuracy in capturing these nuances effectively. The insights gained
  from this study aim to inform discussions on the ethical development and
  deployment of LLMs in global contexts, emphasizing the importance of
  mitigating biases and promoting fair representation across diverse cultural
  perspectives.</td>
  <td colspan=2 style='mso-ignore:colspan'>1 December, 2024</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>314</td>
  <td align=right>2412.07251</td>
  <td class=xl65 width=649 style='width:487pt'>KULTURE Bench: A Benchmark for
  Assessing Language Model in Korean Cultural Context</td>
  <td class=xl65 width=649 style='width:487pt'>Large language models have
  exhibited significant enhancements in performance across various tasks.
  However, the complexity of their evaluation increases as these models
  generate more fluent and coherent content. Current multilingual benchmarks
  often use translated English versions, which may incorporate Western cultural
  biases that do not accurately assess other languages and cultures. To address
  this research gap, we introduce KULTURE Bench, an evaluation framework
  specifically designed for Korean culture that features datasets of cultural
  news, idioms, and poetry. It is designed to assess language models' cultural
  comprehension and reasoning capabilities at the word, sentence, and paragraph
  levels. Using the KULTURE Bench, we assessed the capabilities of models
  trained with different language corpora and analyzed the results
  comprehensively. The results show that there is still significant room for
  improvement in the models' understanding of texts related to the deeper
  aspects of Korean culture.</td>
  <td colspan=2 style='mso-ignore:colspan'>10 December, 2024</td>
 </tr>
 <tr height=227 style='height:170.0pt'>
  <td height=227 align=right style='height:170.0pt'>283</td>
  <td align=right>2412.08846</td>
  <td class=xl65 width=649 style='width:487pt'>Exploring Large Language Models
  on Cross-Cultural Values in Connection with Training Methodology</td>
  <td class=xl65 width=649 style='width:487pt'>Large language models (LLMs)
  closely interact with humans, and thus need an intimate understanding of the
  cultural values of human society. In this paper, we explore how open-source
  LLMs make judgments on diverse categories of cultural values across countries,
  and its relation to training methodology such as model sizes, training
  corpus, alignment, etc. Our analysis shows that LLMs can judge socio-cultural
  norms similar to humans but less so on social systems and progress. In
  addition, LLMs tend to judge cultural values biased toward Western culture,
  which can be improved with training on the multilingual corpus. We also find
  that increasing model size helps a better understanding of social values, but
  smaller models can be enhanced by using synthetic data. Our analysis reveals
  valuable insights into the design methodology of LLMs in connection with
  their understanding of cultural values.</td>
  <td colspan=2 style='mso-ignore:colspan'>11 December, 2024</td>
 </tr>
 <tr height=340 style='height:255.0pt'>
  <td height=340 align=right style='height:255.0pt'>250</td>
  <td align=right>2412.11167</td>
  <td class=xl65 width=649 style='width:487pt'>Cultural Palette: Pluralising
  Culture Alignment via Multi-agent Palette</td>
  <td class=xl65 width=649 style='width:487pt'>Large language models (LLMs)
  face challenges in aligning with diverse cultural values despite their
  remarkable performance in generation, which stems from inherent monocultural
  biases and difficulties in capturing nuanced cultural semantics. Existing methods
  lack adaptability to unkown culture after finetuning. Inspired by cultural
  geography across five continents, we propose Cultural Palette, a multi-agent
  framework for cultural alignment. We first introduce the Pentachromatic
  Cultural Palette Dataset synthesized using LLMs to capture diverse cultural
  values from social dialogues across five continents. Building on this,
  Cultural Palette integrates five continent-level alignment agents with a
  meta-agent using our superior Cultural MoErges alignment technique by
  dynamically activating relevant cultural expertise based on user prompts to
  adapting new culture, which outperforms other joint and merging alignment
  strategies in overall cultural value alignment. Each continent agent
  generates a cultural draft, which is then refined and self-regulated by the
  meta-agent to produce the final culturally aligned response. Experiments
  across various countries demonstrate that Cultural Palette surpasses existing
  baselines in cultural alignment.</td>
  <td colspan=2 style='mso-ignore:colspan'>15 December, 2024</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>219</td>
  <td align=right>2412.12686</td>
  <td class=xl65 width=649 style='width:487pt'>XTransplant: A Probe into the
  Upper Bound Performance of Multilingual Capability and Culture Adaptability
  in LLMs via Mutual Cross-lingual Feed-forward Transplantation</td>
  <td class=xl65 width=649 style='width:487pt'>Current large language models
  (LLMs) often exhibit imbalances in multilingual capabilities and cultural
  adaptability, largely due to their English-centric pretraining data. To
  address this imbalance, we propose a probing method named XTransplant that
  explores cross-lingual latent interactions via cross-lingual feed-forward
  transplantation during inference stage, with the hope of enabling the model
  to leverage the strengths of both English and non-English languages. Through
  extensive pilot experiments, we empirically prove that both the multilingual
  capabilities and cultural adaptability of LLMs hold the potential to be
  significantly improved by XTransplant, respectively from En -&gt; non-En and
  non-En -&gt; En, highlighting the underutilization of current LLMs'
  multilingual potential. And the patterns observed in these pilot experiments
  further motivate an offline scaling inference strategy, which demonstrates
  consistent performance improvements in multilingual and culture-aware tasks,
  sometimes even surpassing multilingual supervised fine-tuning. And we do hope
  our further analysis and discussion could help gain deeper insights into
  XTransplant mechanism.</td>
  <td colspan=2 style='mso-ignore:colspan'>17 December, 2024</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>211</td>
  <td align=right>2412.12961</td>
  <td class=xl65 width=649 style='width:487pt'>Adaptations of AI models for
  querying the LandMatrix database in natural language</td>
  <td class=xl65 width=649 style='width:487pt'>The Land Matrix initiative
  (https://landmatrix.org) and its global observatory aim to provide reliable
  data on large-scale land acquisitions to inform debates and actions in
  sectors such as agriculture, extraction, or energy in low- and middle-income
  countries. Although these data are recognized in the academic world, they
  remain underutilized in public policy, mainly due to the complexity of access
  and exploitation, which requires technical expertise and a good understanding
  of the database schema. The objective of this work is to simplify access to
  data from different database systems. The methods proposed in this article
  are evaluated using data from the Land Matrix. This work presents various
  comparisons of Large Language Models (LLMs) as well as combinations of LLM
  adaptations (Prompt Engineering, RAG, Agents) to query different database
  systems (GraphQL and REST queries). The experiments are reproducible, and a
  demonstration is available online:
  https://github.com/tetis-nlp/landmatrix-graphql-python.</td>
  <td colspan=2 style='mso-ignore:colspan'>17 December, 2024</td>
 </tr>
 <tr height=272 style='height:204.0pt'>
  <td height=272 align=right style='height:204.0pt'>203</td>
  <td align=right>2412.13702</td>
  <td class=xl65 width=649 style='width:487pt'>Typhoon 2: A Family of Open Text
  and Multimodal Thai Large Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>This paper introduces Typhoon 2,
  a series of text and multimodal large language models optimized for the Thai
  language. The series includes models for text, vision, and audio.
  Typhoon2-Text builds on state-of-the-art open models, such as Llama 3 and
  Qwen2, and we perform continual pre-training on a mixture of English and Thai
  data. We employ post-training techniques to enhance Thai language performance
  while preserving the base models' original capabilities. We release text
  models across a range of sizes, from 1 to 70 billion parameters, available in
  both base and instruction-tuned variants. To guardrail text generation, we
  release Typhoon2-Safety, a classifier enhanced for Thai cultures and
  language. Typhoon2-Vision improves Thai document understanding while
  retaining general visual capabilities, such as image captioning.
  Typhoon2-Audio introduces an end-to-end speech-to-speech model architecture
  capable of processing audio, speech, and text inputs and generating both text
  and speech outputs.</td>
  <td colspan=2 style='mso-ignore:colspan'>19 December, 2024</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>202</td>
  <td align=right>2412.13743</td>
  <td class=xl65 width=649 style='width:487pt'>User-Generated Content and
  Editors in Games: A Comprehensive Survey</td>
  <td class=xl65 width=649 style='width:487pt'>User-Generated Content (UGC)
  refers to any form of content, such as posts and images, created by users
  rather than by professionals. In recent years, UGC has become an essential
  part of the evolving video game industry, influencing both game culture and community
  dynamics. The ability for users to actively contribute to the games they
  engage with has shifted the landscape of gaming from a one-directional
  entertainment experience into a collaborative, user-driven ecosystem.
  Therefore, this growing trend highlights the urgent need for summarizing the
  current UGC development in game industry. Our conference paper has
  systematically classified the existing UGC in games and the UGC editors
  separately into four types. However, the previous survey lacks the depth and
  precision necessary to capture the wide-ranging and increasingly complex
  nature of UGC. To this end, as an extension of previous work, this paper
  presents a refined and expanded classification of UGC and UGC editors within
  video games, offering a more robust and comprehensive framework with
  representative cases that better reflects the diversity and nuances of
  contemporary user-generated contributions. Moreover, we provide our insights
  on the future of UGC, involving game culture, game genre and user creative
  tendencies, artificial intelligence, its potential ethical considerations,
  and relationship between games, users and communities.</td>
  <td colspan=2 style='mso-ignore:colspan'>18 December, 2024</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>171</td>
  <td align=right>2412.15253</td>
  <td class=xl65 width=649 style='width:487pt'>Using Machine Learning to
  Distinguish Human-written from Machine-generated Creative Fiction</td>
  <td class=xl65 width=649 style='width:487pt'>Following the universal
  availability of generative AI systems with the release of ChatGPT, automatic
  detection of deceptive text created by Large Language Models has focused on
  domains such as academic plagiarism and &quot;fake news&quot;. However, generative
  AI also poses a threat to the livelihood of creative writers, and perhaps to
  literary culture in general, through reduction in quality of published
  material. Training a Large Language Model on writers' output to generate
  &quot;sham books&quot; in a particular style seems to constitute a new form
  of plagiarism. This problem has been little researched. In this study, we
  trained Machine Learning classifier models to distinguish short samples of
  human-written from machine-generated creative fiction, focusing on classic
  detective novels. Our results show that a Naive Bayes and a Multi-Layer
  Perceptron classifier achieved a high degree of success (accuracy &gt; 95%),
  significantly outperforming human judges (accuracy &lt; 55%). This approach
  worked well with short text samples (around 100 words), which previous
  research has shown to be difficult to classify. We have deployed an online
  proof-of-concept classifier tool, AI Detective, as a first step towards
  developing lightweight and reliable applications for use by editors and
  publishers, with the aim of protecting the economic and cultural contribution
  of human authors.</td>
  <td colspan=2 style='mso-ignore:colspan'>15 December, 2024</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>164</td>
  <td align=right>2412.15497</td>
  <td class=xl65 width=649 style='width:487pt'>Lexicography Saves Lives (LSL):
  Automatically Translating Suicide-Related Language</td>
  <td class=xl65 width=649 style='width:487pt'>Recent years have seen a marked
  increase in research that aims to identify or predict risk, intention or
  ideation of suicide. The majority of new tasks, datasets, language models and
  other resources focus on English and on suicide in the context of Western
  culture. However, suicide is global issue and reducing suicide rate by 2030
  is one of the key goals of the UN's Sustainable Development Goals. Previous
  work has used English dictionaries related to suicide to translate into
  different target languages due to lack of other available resources.
  Naturally, this leads to a variety of ethical tensions (e.g.: linguistic
  misrepresentation), where discourse around suicide is not present in a
  particular culture or country. In this work, we introduce the 'Lexicography
  Saves Lives Project' to address this issue and make three distinct
  contributions. First, we outline ethical consideration and provide overview
  guidelines to mitigate harm in developing suicide-related resources. Next, we
  translate an existing dictionary related to suicidal ideation into 200
  different languages and conduct human evaluations on a subset of translated
  dictionaries. Finally, we introduce a public website to make our resources
  available and enable community participation.</td>
  <td colspan=2 style='mso-ignore:colspan'>19 December, 2024</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>159</td>
  <td align=right>2412.16196</td>
  <td class=xl65 width=649 style='width:487pt'>AgroXAI: Explainable AI-Driven
  Crop Recommendation System for Agriculture 4.0</td>
  <td class=xl65 width=649 style='width:487pt'>Today, crop diversification in
  agriculture is a critical issue to meet the increasing demand for food and
  improve food safety and quality. This issue is considered to be the most
  important challenge for the next generation of agriculture due to the diminishing
  natural resources, the limited arable land, and unpredictable climatic
  conditions caused by climate change. In this paper, we employ emerging
  technologies such as the Internet of Things (IoT), machine learning (ML), and
  explainable artificial intelligence (XAI) to improve operational efficiency
  and productivity in the agricultural sector. Specifically, we propose an edge
  computing-based explainable crop recommendation system, AgroXAI, which
  suggests suitable crops for a region based on weather and soil conditions. In
  this system, we provide local and global explanations of ML model decisions
  with methods such as ELI5, LIME, SHAP, which we integrate into ML models.
  More importantly, we provide regional alternative crop recommendations with
  the counterfactual explainability method. In this way, we envision that our
  proposed AgroXAI system will be a platform that provides regional crop
  diversity in the next generation agriculture.</td>
  <td colspan=2 style='mso-ignore:colspan'>16 December, 2024</td>
 </tr>
 <tr height=363 style='height:272.0pt'>
  <td height=363 align=right style='height:272.0pt'>135</td>
  <td align=right>2412.17255</td>
  <td class=xl65 width=649 style='width:487pt'>Unlocking Cross-Lingual
  Sentiment Analysis through Emoji Interpretation: A Multimodal Generative AI
  Approach</td>
  <td class=xl65 width=649 style='width:487pt'>Emojis have become ubiquitous in
  online communication, serving as a universal medium to convey emotions and
  decorative elements. Their widespread use transcends language and cultural
  barriers, enhancing understanding and fostering more inclusive interactions.
  While existing work gained valuable insight into emojis understanding,
  exploring emojis' capability to serve as a universal sentiment indicator
  leveraging large language models (LLMs) has not been thoroughly examined. Our
  study aims to investigate the capacity of emojis to serve as reliable
  sentiment markers through LLMs across languages and cultures. We leveraged
  the multimodal capabilities of ChatGPT to explore the sentiments of various
  representations of emojis and evaluated how well emoji-conveyed sentiment
  aligned with text sentiment on a multi-lingual dataset collected from 32
  countries. Our analysis reveals that the accuracy of LLM-based emoji-conveyed
  sentiment is 81.43%, underscoring emojis' significant potential to serve as a
  universal sentiment marker. We also found a consistent trend that the
  accuracy of sentiment conveyed by emojis increased as the number of emojis
  grew in text. The results reinforce the potential of emojis to serve as
  global sentiment indicators, offering insight into fields such as
  cross-lingual and cross-cultural sentiment analysis on social media
  platforms. Code:
  https://github.com/ResponsibleAILab/emoji-universal-sentiment.</td>
  <td colspan=2 style='mso-ignore:colspan'>22 December, 2024</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>90</td>
  <td align=right>2412.2076</td>
  <td class=xl65 width=649 style='width:487pt'>Attributing Culture-Conditioned
  Generations to Pretraining Corpora</td>
  <td class=xl65 width=649 style='width:487pt'>In open-ended generative tasks
  like narrative writing or dialogue, large language models often exhibit
  cultural biases, showing limited knowledge and generating templated outputs
  for less prevalent cultures. Recent works show that these biases may stem from
  uneven cultural representation in pretraining corpora. This work investigates
  how pretraining leads to biased culture-conditioned generations by analyzing
  how models associate entities with cultures based on pretraining data
  patterns. We propose the MEMOed framework (MEMOrization from pretraining
  document) to determine whether a generation for a culture arises from
  memorization. Using MEMOed on culture-conditioned generations about food and
  clothing for 110 cultures, we find that high-frequency cultures in
  pretraining data yield more generations with memorized symbols, while some
  low-frequency cultures produce none. Additionally, the model favors
  generating entities with extraordinarily high frequency regardless of the
  conditioned culture, reflecting biases toward frequent pretraining terms
  irrespective of relevance. We hope that the MEMOed framework and our insights
  will inspire more works on attributing model performance on pretraining data.</td>
  <td colspan=2 style='mso-ignore:colspan'>30 December, 2024</td>
 </tr>
 <tr height=385 style='height:289.0pt'>
  <td height=385 align=right style='height:289.0pt'>60</td>
  <td align=right>2501.00962</td>
  <td class=xl65 width=649 style='width:487pt'>OASIS Uncovers: High-Quality T2I
  Models, Same Old Stereotypes</td>
  <td class=xl65 width=649 style='width:487pt'>Images generated by
  text-to-image (T2I) models often exhibit visual biases and stereotypes of
  concepts such as culture and profession. Existing quantitative measures of
  stereotypes are based on statistical parity that does not align with the
  sociological definition of stereotypes and, therefore, incorrectly
  categorizes biases as stereotypes. Instead of oversimplifying stereotypes as
  biases, we propose a quantitative measure of stereotypes that aligns with its
  sociological definition. We then propose OASIS to measure the stereotypes in
  a generated dataset and understand their origins within the T2I model. OASIS
  includes two scores to measure stereotypes from a generated image dataset:
  (M1) Stereotype Score to measure the distributional violation of stereotypical
  attributes, and (M2) WALS to measure spectral variance in the images along a
  stereotypical attribute. OASIS also includes two methods to understand the
  origins of stereotypes in T2I models: (U1) StOP to discover attributes that
  the T2I model internally associates with a given concept, and (U2) SPI to
  quantify the emergence of stereotypical attributes in the latent space of the
  T2I model during image generation. Despite the considerable progress in image
  fidelity, using OASIS, we conclude that newer T2I models such as FLUX.1 and
  SDv3 contain strong stereotypical predispositions about concepts and still
  generate images with widespread stereotypical attributes. Additionally, the
  quantity of stereotypes worsens for nationalities with lower Internet
  footprints.</td>
  <td colspan=2 style='mso-ignore:colspan'>1 January, 2025</td>
 </tr>
 <tr height=496 style='height:372.0pt'>
  <td height=496 align=right style='height:372.0pt'>56</td>
  <td align=right>2501.01056</td>
  <td class=xl65 width=649 style='width:487pt'>Risks of Cultural Erasure in
  Large Language Models</td>
  <td class=xl65 width=649 style='width:487pt'>Large language models are
  increasingly being integrated into applications that shape the production and
  discovery of societal knowledge such as search, online education, and travel
  planning. As a result, language models will shape how people learn about,
  perceive and interact with global cultures making it important to consider
  whose knowledge systems and perspectives are represented in models.
  Recognizing this importance, increasingly work in Machine Learning and NLP
  has focused on evaluating gaps in global cultural representational
  distribution within outputs. However, more work is needed on developing
  benchmarks for cross-cultural impacts of language models that stem from a
  nuanced sociologically-aware conceptualization of cultural impact or harm. We
  join this line of work arguing for the need of metricizable evaluations of
  language technologies that interrogate and account for historical power
  inequities and differential impacts of representation on global cultures,
  particularly for cultures already under-represented in the digital corpora.
  We look at two concepts of erasure: omission: where cultures are not
  represented at all and simplification i.e. when cultural complexity is erased
  by presenting one-dimensional views of a rich culture. The former focuses on
  whether something is represented, and the latter on how it is represented. We
  focus our analysis on two task contexts with the potential to influence
  global cultural production. First, we probe representations that a language
  model produces about different places around the world when asked to describe
  these contexts. Second, we analyze the cultures represented in the travel
  recommendations produced by a set of language model applications. Our study
  shows ways in which the NLP community and application developers can begin to
  operationalize complex socio-cultural considerations into standard
  evaluations and benchmarks.</td>
  <td colspan=2 style='mso-ignore:colspan'>1 January, 2025</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>53</td>
  <td align=right>2501.01282</td>
  <td class=xl65 width=649 style='width:487pt'>CultureVLM: Characterizing and
  Improving Cultural Understanding of Vision-Language Models for over 100
  Countries</td>
  <td class=xl65 width=649 style='width:487pt'>Vision-language models (VLMs)
  have advanced human-AI interaction but struggle with cultural understanding,
  often misinterpreting symbols, gestures, and artifacts due to biases in
  predominantly Western-centric training data. In this paper, we construct CultureVerse,
  a large-scale multimodal benchmark covering 19, 682 cultural concepts, 188
  countries/regions, 15 cultural concepts, and 3 question types, with the aim
  of characterizing and improving VLMs' multicultural understanding
  capabilities. Then, we propose CultureVLM, a series of VLMs fine-tuned on our
  dataset to achieve significant performance improvement in cultural
  understanding. Our evaluation of 16 models reveals significant disparities,
  with a stronger performance in Western concepts and weaker results in African
  and Asian contexts. Fine-tuning on our CultureVerse enhances cultural
  perception, demonstrating cross-cultural, cross-continent, and cross-dataset
  generalization without sacrificing performance on models' general VLM
  benchmarks. We further present insights on cultural generalization and
  forgetting. We hope that this work could lay the foundation for more
  equitable and culturally aware multimodal AI systems.</td>
  <td colspan=2 style='mso-ignore:colspan'>2 January, 2025</td>
 </tr>
 <tr height=408 style='height:306.0pt'>
  <td height=408 align=right style='height:306.0pt'>37</td>
  <td align=right>2501.02132</td>
  <td class=xl65 width=649 style='width:487pt'>A hybrid marketplace of ideas</td>
  <td class=xl65 width=649 style='width:487pt'>The convergence of humans and
  artificial intelligence systems introduces new dynamics into the cultural and
  intellectual landscape. Complementing emerging cultural evolution concepts
  such as machine culture, AI agents represent a significant techno-sociological
  development, particularly within the anthropological study of Web3 as a
  community focused on decentralization through blockchain. Despite their
  growing presence, the cultural significance of AI agents remains largely
  unexplored in academic literature. Toward this end, we conceived hybrid
  netnography, a novel interdisciplinary approach that examines the cultural
  and intellectual dynamics within digital ecosystems by analyzing the
  interactions and contributions of both human and AI agents as co-participants
  in shaping narratives, ideas, and cultural artifacts. We argue that, within
  the Web3 community on the social media platform X, these agents challenge
  traditional notions of participation and influence in public discourse,
  creating a hybrid marketplace of ideas, a conceptual space where human and AI
  generated ideas coexist and compete for attention. We examine the current
  state of AI agents in idea generation, propagation, and engagement,
  positioning their role as cultural agents through the lens of memetics and
  encouraging further inquiry into their cultural and societal impact.
  Additionally, we address the implications of this paradigm for privacy,
  intellectual property, and governance, highlighting the societal and legal
  challenges of integrating AI agents into the hybrid marketplace of ideas.</td>
  <td colspan=2 style='mso-ignore:colspan'>8 January, 2025</td>
 </tr>
 <tr height=496 style='height:372.0pt'>
  <td height=496 align=right style='height:372.0pt'>21</td>
  <td align=right>2501.03259</td>
  <td class=xl65 width=649 style='width:487pt'>Toward Inclusive Educational AI:
  Auditing Frontier LLMs through a Multiplexity Lens</td>
  <td class=xl65 width=649 style='width:487pt'>As large language models (LLMs)
  like GPT-4 and Llama 3 become integral to educational contexts, concerns are
  mounting over the cultural biases, power imbalances, and ethical limitations
  embedded within these technologies. Though generative AI tools aim to enhance
  learning experiences, they often reflect values rooted in Western, Educated,
  Industrialized, Rich, and Democratic (WEIRD) cultural paradigms, potentially
  sidelining diverse global perspectives. This paper proposes a framework to
  assess and mitigate cultural bias within LLMs through the lens of applied
  multiplexity. Multiplexity, inspired by Senturk et al. and rooted in Islamic
  and other wisdom traditions, emphasizes the coexistence of diverse cultural
  viewpoints, supporting a multi-layered epistemology that integrates both
  empirical sciences and normative values. Our analysis reveals that LLMs
  frequently exhibit cultural polarization, with biases appearing in both overt
  responses and subtle contextual cues. To address inherent biases and incorporate
  multiplexity in LLMs, we propose two strategies:
  \textit{Contextually-Implemented Multiplex LLMs}, which embed multiplex
  principles directly into the system prompt, influencing LLM outputs at a
  foundational level and independent of individual prompts, and
  \textit{Multi-Agent System (MAS)-Implemented Multiplex LLMs}, where multiple
  LLM agents, each representing distinct cultural viewpoints, collaboratively
  generate a balanced, synthesized response. Our findings demonstrate that as
  mitigation strategies evolve from contextual prompting to MAS-implementation,
  cultural inclusivity markedly improves, evidenced by a significant rise in
  the Perspectives Distribution Score (PDS) and a PDS Entropy increase from
  3.25\% at baseline to 98\% with the MAS-Implemented Multiplex LLMs. Sentiment
  analysis further shows a shift towards positive sentiment across cultures,...</td>
  <td colspan=2 style='mso-ignore:colspan'>2 January, 2025</td>
 </tr>
 <tr height=475 style='height:356.0pt'>
  <td height=475 align=right style='height:356.0pt'>16</td>
  <td align=right>2501.03569</td>
  <td class=xl65 width=649 style='width:487pt'>What Does a Software Engineer
  Look Like? Exploring Societal Stereotypes in LLMs</td>
  <td class=xl65 width=649 style='width:487pt'>Large language models (LLMs)
  have rapidly gained popularity and are being embedded into professional
  applications due to their capabilities in generating human-like content.
  However, unquestioned reliance on their outputs and recommendations can be
  problematic as LLMs can reinforce societal biases and stereotypes. This study
  investigates how LLMs, specifically OpenAI's GPT-4 and Microsoft Copilot, can
  reinforce gender and racial stereotypes within the software engineering (SE)
  profession through both textual and graphical outputs. We used each LLM to
  generate 300 profiles, consisting of 100 gender-based and 50 gender-neutral
  profiles, for a recruitment scenario in SE roles. Recommendations were
  generated for each profile and evaluated against the job requirements for
  four distinct SE positions. Each LLM was asked to select the top 5 candidates
  and subsequently the best candidate for each role. Each LLM was also asked to
  generate images for the top 5 candidates, providing a dataset for analysing
  potential biases in both text-based selections and visual representations.
  Our analysis reveals that both models preferred male and Caucasian profiles,
  particularly for senior roles, and favoured images featuring traits such as
  lighter skin tones, slimmer body types, and younger appearances. These
  findings highlight underlying societal biases influence the outputs of LLMs,
  contributing to narrow, exclusionary stereotypes that can further limit
  diversity and perpetuate inequities in the SE field. As LLMs are increasingly
  adopted within SE research and professional practices, awareness of these
  biases is crucial to prevent the reinforcement of discriminatory norms and to
  ensure that AI tools are leveraged to promote an inclusive and equitable
  engineering culture rather than hinder it.</td>
  <td colspan=2 style='mso-ignore:colspan'>7 January, 2025</td>
 </tr>
 <tr height=317 style='height:238.0pt'>
  <td height=317 align=right style='height:238.0pt'>8</td>
  <td align=right>2501.04662</td>
  <td class=xl65 width=649 style='width:487pt'>On The Origin of Cultural Biases
  in Language Models: From Pre-training Data to Linguistic Phenomena</td>
  <td class=xl65 width=649 style='width:487pt'>Language Models (LMs) have been
  shown to exhibit a strong preference towards entities associated with Western
  culture when operating in non-Western languages. In this paper, we aim to
  uncover the origins of entity-related cultural biases in LMs by analyzing
  several contributing factors, including the representation of entities in
  pre-training data and the impact of variations in linguistic phenomena across
  languages. We introduce CAMeL-2, a parallel Arabic-English benchmark of
  58,086 entities associated with Arab and Western cultures and 367 masked
  natural contexts for entities. Our evaluations using CAMeL-2 reveal reduced
  performance gaps between cultures by LMs when tested in English compared to
  Arabic. We find that LMs struggle in Arabic with entities that appear at high
  frequencies in pre-training, where entities can hold multiple word senses.
  This also extends to entities that exhibit high lexical overlap with
  languages that are not Arabic but use the Arabic script. Further, we show how
  frequency-based tokenization leads to this issue in LMs, which gets worse
  with larger Arabic vocabularies. We will make CAMeL-2 available at:
  https://github.com/tareknaous/camel2</td>
  <td colspan=2 style='mso-ignore:colspan'>8 January, 2025</td>
 </tr>
 <![if supportMisalignedColumns]>
 <tr height=0 style='display:none'>
  <td width=87 style='width:65pt'></td>
  <td width=87 style='width:65pt'></td>
  <td width=649 style='width:487pt'></td>
  <td width=649 style='width:487pt'></td>
  <td width=87 style='width:65pt'></td>
  <td width=87 style='width:65pt'></td>
 </tr>
 <![endif]>
</table>

</body>

</html>
